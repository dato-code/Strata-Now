{"big-data-conference-ny-2015/public/schedule/detail/46176": {"room": "3D 06/07", "title": "Launch New Financial Products with Confidence", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46176", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/218955", "/big-data-conference-ny-2015/public/schedule/speaker/188941"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "To launch new products, financial institutions are turning to a wide array of data sources such as streaming news feeds, filings and traditional proprietary financial information to find insight. For example, one company is taking filings data from 130 countries in 20 languages, and trade information from over 500,000 equity instruments to create new real-time visualization applications. Such applications require sophisticated techniques such as machine learning and text analytics on both streaming data and data stored in Hadoop or relational databases. Excellent data integration across these data sources is essential for information to be trusted. In this session, we will highlight an architecture designed to capture data at any point in time and ensure such information is trusted for confident decision making across the organization. We will explain how to build data integration and governance into all your applications in a reliable, scalable and secure manner. Finally, we will provide real-world use cases for how clients are being successful.\nThis session is sponsored by IBM"}, "big-data-conference-ny-2015/public/schedule/detail/42620": {"room": "3D 05/08", "title": "Hadoop in the cloud: An architectural how-to", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42620", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/144901"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "Apache Hadoop was designed when cloud models were in their infancy. Despite this fact, Hadoop has proven remarkably adept at migrating its architecture to work well in the context of the cloud, as production workloads migrate to a cloud environment.\nThis talk will have cover several topics on adapting Hadoop to the cloud, including:\n\nArchitectural models that work well in the cloud for different classes of use cases: ETL, Analytics, Machine Learning, Search, etc.\nCloud hardware architectures and implications on Hadoop capabilities\nSecuring Hadoop environments properly in the cloud\nOperations management in cloud environments and expense management\nLooking forward: coming changes to data analytics in the cloud."}, "big-data-conference-ny-2015/public/schedule/detail/45629": {"room": "Javits North", "title": "Unblock the pathways of intelligence from data to action", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45629", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/153159"], "timing": "9:30am\u20139:35am Wednesday, 09/30/2015", "abstract": "An enterprise is only as intelligent as the data it can analyze within the time it takes to make a useful decision. Business analysts at enterprises today are hampered in these efforts by the lack of domain-specific or customizable applications based on big data analytics.\u00a0App developers are slowed down by the complexity of the big data infrastructure and the lack of analytic functions for which they depend on data scientists. And\u00a0data scientists, in turn, are thwarted by\u00a0the\u00a0time it takes to build hand-crafted models, which are re-coded for production and rarely reused, slowing down their deployment and update. This problem is not for any one company to solve but for the entire community and industry to address. Here\u2019s where Intel starts. Join us.\nThis keynote is sponsored by Intel\n\u00a0"}, "big-data-conference-ny-2015/public/schedule/detail/42938": {"room": "1 E12/ 1 E13", "title": "Big data, small internet: How to circumnavigate your information", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42938", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/184559", "/big-data-conference-ny-2015/public/schedule/speaker/203092"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "The world has changed. Value is driven by data and the ability to analyze massive bits of information in central systems. But what do you do when you create data at a faster rate than it can be sent through the internet? In many parts of the world bandwidth is limited. There are still internet nodes that consist of an individual who bicycles USB flash drives between small villages and internet cafes.\nWhile working for a global manufacturing firm, Scott and Ray ran into issues with accessing and analyzing data from African, South American, and rural Chinese factories. Machines were producing data exponentially faster than the available bandwidth.\nScott and Ray will lead an interactive conversation discussing the various options they considered, and go through the technical details of their end solution. This talk is geared toward both technical and business audiences, as they show how you can solve business problems simply and elegantly."}, "big-data-conference-ny-2015/public/schedule/detail/42526": {"room": "1 E20 / 1 E21", "title": "Estimating financial risk with Apache Spark", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42526", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/156694"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Under reasonable circumstances, how much can you expect to lose? The financial statistic Value at Risk (VaR) seeks to answer this question. Since its development on Wall Street soon after the stock market crash of 1987, VaR has been widely adopted across the financial services industry. Some organizations report the statistic to satisfy regulations, some use it to better understand the risk characteristics of large portfolios, and others compute it before executing trades to help make informed and immediate decisions.\nEstimating VaR can be computationally intensive. As a flexible processing framework with the ability to both scale up to large amounts of data and leverage vast compute resources, Apache Spark is a compelling platform for undertaking financial risk calculations. At Cloudera, we\u2019ve assisted several organizations in using Spark to compute VaR and other financial statistics.\nIn this talk, we\u2019ll walk through a basic VaR calculation with Spark. The calculation employs the widely used Monte Carlo method, which is useful for modeling portfolios with non-normal distributions of returns. It simulates thousands or millions of random market scenarios, and uses a model to predict the response of the portfolio to each scenario. The talk, which will cover Spark design patterns in time series analysis, visualizing data, and Monte Carlo simulation, aims to give a feel for what it is like to approach financial modeling with Spark."}, "big-data-conference-ny-2015/public/schedule/detail/45395": {"room": "3D 02/11", "title": "An open source approach to gathering and analyzing device-sourced health data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45395", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/195272"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Capturing and integrating device-based and other health data for research is frustratingly difficult. We explain the open source technology frame\u200bwork for capturing and routing device-based health data for use by healthcare providers and for access, via a trusted analytic container, to \u200b\u200bresearchers we developed, working with O\u2019Reilly Media and with support from the Robert Wood Johnson Foundation.\u200b\n\u200bThe session will cover our work with the Switchboard application for routing data and the trusted analytic container\u200b (TAC) for consolidating data for analytics while addressing privacy issues"}, "big-data-conference-ny-2015/public/schedule/detail/45867": {"room": "1 E15", "title": "Catalog, secure, and govern your Hadoop data lake", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45867", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/128048", "/big-data-conference-ny-2015/public/schedule/speaker/108381", "/big-data-conference-ny-2015/public/schedule/speaker/218519", "/big-data-conference-ny-2015/public/schedule/speaker/189481"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Alex Gorelik will moderate a panel to discuss this topic. Panelists are:\nDavid Tabacco, IT Director at Merck\nJim Kaskade, VP & GM, Big Data & Analytics at CSC\nDavid Paige, Sr. Director, EDS Data Platforms at AutoTrader\nIt\u2019s easy to load data into a data lake, but it\u2019s not easy to know what is there, and find the right data to do exploratory analytics \u2013 and do so in compliance with security and data governance policies.\nEmerging data discovery, data prep, and data visualization tools have made it easy to do \u201cthe last mile\u201d of self-service data transformation and exploratory analytics by wrangling a few files. However, the tools presume that the data scientists and data analysts already know where to find the data right away, and that all the data has already been cataloged, so all they have to do is mash it up in the tool. But because the lake isn\u2019t inventoried, finding data in the lake is like finding \u201ca needle in a stack of needles.\u201d\nFurthermore, it\u2019s not enough to just put a search interface over the data. The metadata about the data first has to be defined, and access to the data must comply with security and data governance policies. While you want to promote data agility, you also don\u2019t want to violate your data governance, compliance, and security policies. For example, HIPAA (Health Insurance Portability and Accountability Act) regulations demand confidentiality safeguards on sensitive data such as personally identifiable information. This means that sensitive data going into Hadoop must be identified and protected before users can get to it.\nLearn about a best practice two-step approach to accelerate data discovery while complying with security, compliance, and data governance needs. First, implement an automated and governed inventory of your data assets. Second, deploy a secure self-service to the business to find and understand the right data quickly.\nIn this session you will learn about:\n\nAutomated technical, business, and compliance metadata discovery\nAutomated sensitive data discovery\nAutomated tagging and tag propagation\nData governance\nData profiling and data quality\nSecure data self-service and provisioning\nCertification with Hadoop Distributions and integration (e.g., Cloudera Navigator, Project Atlas, Drill)\n\nThis session is sponsored by Waterline Data"}, "big-data-conference-ny-2015/public/schedule/detail/42930": {"room": "1 E8 / 1 E9", "title": "Web-scale machine learning using the Spark stack", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42930", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/147597"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Web-scale machine learning plays a central role in today\u2019s internet applications and intelligent systems. These problem settings have pushed the field to address issues of scale that were almost inconceivable even a decade ago. Today, a typical industrial machine learning application may analyze up to trillions of training samples, using a correspondingly large model with up to tens of billions of unique features. This \u201cweb-scale\u201d machine learning is driving the need for scalable, distributed learning algorithms and systems that can handle big data. Unfortunately, existing open source big data systems (e.g., Hadoop/Mahout, Spark/MLlib, etc.) fail to readily support the rapid increase in the magnitude and complexity of these analysis tasks, especially the challenges associated with datasets of massive size and dimensionality.\nIn this talk, we will present our efforts and experience in building web-scale machine learning using the Apache Spark stack. In particular, we will talk about \u201cwar stories\u201d (online ads, in-game purchases, fraud detection, etc.) for many large cloud service providers, share the best practices to scale these learning algorithms on Spark (by exploiting data sparsity, location-aware parameter aggregation, etc.), and discuss tradeoffs in designing learning systems for Spark frameworks (e.g., fault tolerance through iterative optimizations, model parallelism and/or parameter server, etc.)."}, "big-data-conference-ny-2015/public/schedule/detail/42807": {"room": "3D 05/08", "title": "Failing fast and falling often is no way to run a cluster!", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42807", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/137919"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "In the beginning, there was only one single most important point of failure\u2026 the Name Node. Lose the Name Node, you lose your cluster.\nToday, there are now multiple points of failure, depending on which vendor\u2019s solution you choose to implement.\nHive, HCatalog, and Ranger among other components have now become critical components. The failure of any one of these can cripple a cluster and cause either significant downtime, data loss, or both.\nBy identifying where failures can occur, one can either mitigate the risk, or consider alternative designs."}, "big-data-conference-ny-2015/public/schedule/detail/43047": {"room": "1 E18 / 1 E19", "title": "Copycat: Fault tolerant streaming data ingestion powered by Apache Kafka", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43047", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/197609"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Stream processing is skyrocketing in popularity, but most organizations still face a serious challenge: getting their data into (and out of) their stream processing framework of choice. Even a small organization may have dozens of data sources at course granularity (one or more databases, app events, log files, metrics, and more), and many thousands if you consider each table, application log type, or metric individually. Loading all this data into your stream processing framework often means tracking down dozens of third party connectors that vary greatly in quality and robustness.\nOn the other hand, Apache Kafka has quickly grown into a de facto standard for data storage with stream processing frameworks. All the major frameworks, including Samza, Spark Streaming, and Storm, include robust and actively-maintained connectors. It has achieved this status because it is a natural fit for streaming data: high throughput, low latency, scales to very large workloads, and uses retention policies to explicitly limit the stored data, something that needs to be done manually with most data stores. And it\u2019s not only good for input and output; it\u2019s also ideal for intermediate storage of derived data streams that may feed back into additional downstream processing jobs.\nWhat if we leveraged Kafka to drastically simplify this ecosystem and improve the overall quality of connectors? This presentation introduces Copycat, a new framework for loading structured data into and out of Kafka, and therefore into and out of all the major stream processing frameworks. First we will describe the types of impedance mismatch that arise between data sources and sinks, that make connectors difficult to write well and lead to high variation in quality; then explain how introducing Kafka solves these problems. Next, we will dive into the details of Copycat and how it compares to some systems with similar goals, discussing key design decisions made that trade off between ease of use for writers of connectors, operational complexity, and reuse of existing connectors. Finally, we\u2019ll discuss how standardizing on Copycat and Kafka for ingestion of data into your stream processing framework can ultimately lead to simplifying your entire data pipeline, and make ETL into your data warehouse as simple as adding another Copycat job."}, "big-data-conference-ny-2015/public/schedule/detail/43208": {"room": "1 E18 / 1 E19", "title": "Paying the technical debt of machine learning: Managing ML models in production", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43208", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157232"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Machine learning has become the key component in building intelligence-infused applications.  However, as companies increase the number of such deployments, the number of machine learning models that need to be created, maintained, monitored, tracked, and improved grow at a tremendous pace.  This growth has lead to a huge (and well-documented) accumulation of technical debt.\nDeveloping a machine learning application is an iterative process that involves building multiple models over a dataset. The dataset itself evolves over time as new features and new data points are collected. Furthermore, once deployed, the models require updates over time. Changes in models and datasets become difficult to track over time, and one can quickly lose track of which version of the model used which data and why it was subsequently replaced.\nIn this talk, we outline some of the key challenges in large-scale deployments of many interacting machine learning models.  We then describe a methodology for management, monitoring, and optimization of such models in production, which helps mitigate the technical debt. In particular, we demonstrate how to:\n\nTrack models and versions, and visualize their quality over time\nTrack the provenance of models and datasets, and quantify how changes in data impact the models being served\nOptimize model ensembles in real time, based on changing data, and provide alerts when such ensembles no longer provide the desired accuracy."}, "big-data-conference-ny-2015/public/schedule/detail/42604": {"room": "3D 02/11", "title": "Hadoop application architectures: Fraud detection", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42604", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/126882", "/big-data-conference-ny-2015/public/schedule/speaker/134675", "/big-data-conference-ny-2015/public/schedule/speaker/169835", "/big-data-conference-ny-2015/public/schedule/speaker/147273"], "timing": "9:00am\u201312:30pm Tuesday, 09/29/2015", "abstract": "Description\nImplementing a scalable low-latency architecture requires understanding a broad range of frameworks, such as Kafka, HBase, HDFS, Flume, Spark, SparkStreaming, and Impala among many others. The good news is that there\u2019s an abundance of materials \u2013 books, web sites, conferences, etc. \u2013 for gaining a deep understanding of these related projects. The bad news is there\u2019s still a scarcity of information on how to integrate these components to implement complete solutions.\nIn this tutorial (Part 1 of \u201cArchitecture Day\u201d), we\u2019ll walk through the end-to-end case study of building a fraud detection system to provide a concrete example of how to architect and implement real-time systems. We\u2019ll use this example to illustrate important topics, such as:\n\nModeling data in Kafka, HBase, and Hadoop and selecting optimal formats for storing data\nIntegrating multiple data collection, processing, and storage systems\nCollecting and analyzing event-based data such as logs and machine-generated data and storing the data in Hadoop\nQuerying and reporting on data\n\nThroughout the example, best practices and considerations for architecting real-time applications will be covered. This tutorial will be valuable for developers, architects, or project leads who are already knowledgeable about Hadoop or similar distributed data processing systems, and are now looking for more insight into how it can be leveraged to implement real-world applications."}, "big-data-conference-ny-2015/public/schedule/detail/43538": {"room": "1 E18 / 1 E19", "title": "Calculating high-resolution, global-scale geospatial analytics with MapReduce Geospatial", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43538", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204117"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "The increasing amount and resolution of geospatial data from remote sensing platforms and traditional geospatial information systems (GIS), as well as the geospatial data from new data sources such as social media and Internet of Things data sets, provide great opportunities to answer new and bigger questions from a geospatial perspective.\nHowever, to extract new insights from these vastly expanding geospatial data streams requires new tools.  DigitalGlobe in collaboration with the National Geospatial Intelligence Agency, NGA, recently open sourced MrGeo on NGA\u2019s GitHub page as one tool to compute products to help in this area.\nMrGeo can ingest and store global data sets in the cloud in an application-ready format. freeing the user from all the heavy data logistics previously required in downloading and preprocessing the data on traditional desktop GIS systems.  This allows the user to ask bigger questions of the data in the cloud, and receive just the calculated answers for their areas of interest.\nMrGeo provides a general engine to do raster math on geospatially referenced rasters such as digital elevation models and LandSat imagery from NASA, and multi-spectral and aerial imagery from commercial providers such as DigitalGlobe.  It also provides a user-friendly command line syntax called Map Algebra, to chain basic operations into pipelines to create higher level analytic outputs.  An example of a basic operation is calculating the slope of terrain at high resolution for the entire globe.  An example of a more complex product is calculating and visualizing walking time for every location on the globe to a pub.  This would include applying an equation to that slope layer and utilizing all the locations of pubs across the globe from an open data set.  We will present performance benchmarks for such an example to show how the system scales with cluster size."}, "big-data-conference-ny-2015/public/schedule/detail/43205": {"room": "3D 03/10", "title": "Designing happiness with data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43205", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203893"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Happiness is part of the zeitgeist. Every month there seems to be a new book about happiness coming out, from the bestselling The Happiness Project to Happy City to Hardwiring Happiness. There is Pharrell\u2019s \u201cHappy,\u201d #100HappyDays, and apps like Happify and Happier.\nYet so much of the news about technology tells us that the internet makes us anxious, our smartphones take us out of the present moment, and social media ensnares us in a dopamine loop. If you give a cursory look at what makes people happy, rarely is an app or a website in the mix. Happiness, it seems, is not a screen.\nCan happiness and technology co-exist? The answer may just be in the data. Based on a combination of big data analysis and small-scale studies, this session will look at what we can learn about happiness online and the implications for designing technology."}, "big-data-conference-ny-2015/public/schedule/detail/43783": {"room": "3D 01/12", "title": "Spark Development Bootcamp (Day 3)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43783", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/215137"], "timing": "9:00am\u20135:00pm Thursday, 10/01/2015", "abstract": "Description\nOverview\nThis three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.\nSpark is a unified framework for big data analytics. Spark provides one integrated API for use by developers, data scientists, and analysts to perform diverse tasks that would have previously required separate processing engines such as batch analytics, stream processing and statistical modeling. Spark supports a wide range of popular languages including Python, R, Scala, SQL, and Java. Spark can read from diverse data sources and scale to thousands of of nodes.\nIn this class, you will learn how to build and manage Spark applications using Spark\u2019s core programming APIs and its standard Libraries. Hands-on exercises will be done in Scala. Course materials emphasize design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.\nThose who attend the training will have opportunities during the tutorial to meet and have discussions with members of the Spark development community, including Q&A sessions and discussions about real-world use cases. You will receive a free Databricks account for the duration of training.\nCourse Learning Objectives\nAfter taking this class you will be able to:\n\nBuild a data pipeline using Spark DataFrames and Spark SQL\nUnderstand Spark concepts, architecture, and applications\nExecute SQL queries on large scale data using Spark\nExplore and visualize your data by entering and running code in Notebooks\nTrain, and use an ML model on real data with Spark\u2019s Machine Learning library MLlib\nTune Spark job performance and troubleshoot errors using logs and administration UIs\nFind answers to common questions using Spark documentation and discussion forums\nWrite and monitor a Spark Streaming job to analyze data with sub-second latency\nUnderstand common use-cases and business applications of Spark\nRecognize all of the topics tested by the Spark Developer Certification and know what further work is required to prepare to take and pass the exam\n\nPrerequisites\nStudents, please arrive to class with:\n\nA basic understanding of software development\nSome experience coding in Python, Java, SQL, or Scala\nA modern operating system (Windows, OS X, Linux), browser (Internet Explorer not supported)\n\nOutline of topics covered in class\n\nHistory of Big Data & Apache Spark\n    \u2013 Introduction to the Spark Shell and the training environment\n    \u2013 Just enough Scala for Spark\n    \u2013 Introduction to Spark DataFrames and Spark SQL\n    \u2013 Introduction to RDDs\n       \u2013 Lazy Evaluation\n        \u2013 Transformations and Actions\n        \u2013 Caching\n        \u2013 Using the Spark UIs\n\n\nData Sources: reading from Parquet, S3, Cassandra, HDFS, and your local file system\nSpark\u2019s Architecture\nProgramming with Accumulators and Broadcast Variables\nDebugging and tuning Spark jobs using Spark\u2019s admin UIs\nMemory & Persistence\nAdvanced programming with RDDs (understanding the shuffle phase, partitioning, etc.)\nVisualization: matplotlib, gg_plot, dashboards, exploration and visualization in notebooks\nIntroduction to Spark Streaming\nIntroduction to MLlib and GraphX"}, "big-data-conference-ny-2015/public/schedule/detail/42774": {"room": "1 E8 / 1 E9", "title": "Apache Hadoop operations for production systems", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42774", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/190141", "/big-data-conference-ny-2015/public/schedule/speaker/163508", "/big-data-conference-ny-2015/public/schedule/speaker/107366", "/big-data-conference-ny-2015/public/schedule/speaker/151508"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\n(Full Day Tutorial)\nHadoop is emerging as the standard for big data processing and analytics. However, as usage of the Hadoop clusters grow, so do the demands of managing and monitoring these systems.\nIn this full-day tutorial, attendees will get an overview of all phases for successfully managing Hadoop clusters, with an emphasis on production systems \u2014 from installation, to configuration management, service monitoring, troubleshooting and support integration.\nWe will review tooling capabilities and highlight the ones that have been most helpful to users, and share some of the lessons learned and best practices from users who depend on Hadoop as a business-critical system.\nProposed Agenda Topics\n\nInstallation (hardware considerations, OS prerequisites, sanity testing, security considerations)\nConfiguration (mechanics, key configurations, resource management)\nTroubleshooting (managing, troubleshooting, and debugging Hadoop clusters and applications)\n\ufffc* Enterprise Considerations (scaling, logs, failure testing)"}, "big-data-conference-ny-2015/public/schedule/detail/43275": {"room": "3D 03/10", "title": "What have you done!? How to visualize methods and models for decision makers", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43275", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/180236"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "It\u2019s time to start visualizing methods, not just data.  The days of trusting obscure \u201cblack-box\u201d data processes are numbered.  Dismissing the data manipulation and modeling process as too technical or unnecessary prevents decision makers from effectively leveraging their data.  Communicating about data science provides a richer and more participatory understanding of data, driving better-informed decision-making.  In this tutorial, we\u2019ll begin by detailing what exactly we\u2019re doing to data.  These actions include:\n\nAggregation\nFiltering\nCleaning (whatever that means)\nStatistical modeling\nMachine learning\n\nAfter describing the landscape of data modeling and manipulation, we\u2019ll introduce a process for isolating which aspects of data processing are key to communicate.   This requires a nuanced understanding of both your user and your data.  Based on user-centered design, participants will be exposed to methods of honing in on the most relevant aspects of their data processing.  Through articulating the goals and limitations of data users, we are able to identify assumptions in our data cleaning and modeling that are crucial to communicate.\nOnce participants have the tools to determine what to communicate, we\u2019ll explore examples of sharing complex ideas visually.  This link provides an excellent example of how a dense method (Markov Chains) can be expressed through simple visual representations.  This type of visual representation is a prerequisite to engagement with data.\nWhile there has been an explosive focus on data visualization, it\u2019s time to start demanding methods visualization.  Until we fully understand what we\u2019re doing to data, the impact of visualizing it will be limited."}, "big-data-conference-ny-2015/public/schedule/detail/45877": {"room": "1 E15", "title": "Hydrate a data lake in days with CDAP", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45877", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/194760"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Data lakes represent a powerful new data architecture, providing enterprises with the scale and flexibility required for big data: unbounded storage for unbounded questions. Hadoop is the de facto standard for implementing data lakes, but significant expertise, time, and effort are still required for organizations to deliver one. Today, enterprises building their own data lakes on Hadoop are effectively implementing their own internal platforms from a collection of individual open source technologies.\nThe many projects provided by open source and commercial Hadoop distributions must be integrated with each other, integrated with the existing environment, and operationalized into new and existing processes. With no established best practices or standards, each organization is left to find their own way and rely on expensive, external experts. Data lake proof of concepts can take months.\nThis talk introduces Cask Hydrator, a new open source data lake framework included in the latest release of the Cask Data App Platform (CDAP). Hydrator is a self-service data ingestion and ETL framework with a drag-and-drop user interface and JSON-based pipeline configurations. Enforcing best practices and providing out-of-the-box functionality, Hydrator enables enterprises to build data lakes in a matter of days. Integrations are included with open source and traditional data sources, from Kafka and Flume to Oracle and Teradata. Completely open source, Cask Hydrator is highly extensible and can be easily integrated with new data sources and sinks, and extended with custom transformations and validations.\nAttendees will learn about data lakes, the different approaches and architectures enterprises are utilizing, the benefits and challenges associated with them, and how Cask Hydrator can enable the rapid creation of data lakes and dramatically decrease the complexity in operationalizing them.\nThis session is sponsored by Cask"}, "big-data-conference-ny-2015/public/schedule/detail/43169": {"room": "1 E10 / 1 E11", "title": "Your data is screaming at you. Learn to listen through customer choice modeling", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43169", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203887"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Understanding the mechanisms that drive choice is an age-old challenge. Consider the fundamental problem a retailer must solve in stocking a store: a large assortment will likely draw a larger set of customers, but will be substantially more expensive to carry and will also likely introduce cannibalization between products. A smaller assortment might not carry these risks, but may fail to attract a sufficiently large group of customers. One can begin to address this tradeoff by successfully modeling how customer purchase behavior varies as a function of the assortment offered to the customer. Building such models have been as much an art as a science, limiting scope to situations with a small universe of well-understood products. In stark contrast, retailers buy millions of products every season, most of them new to the market.\nCelect is able to build predictive models of choice for the gigantic, poorly-understood product universes. We achieve this goal through the development of a new, easily distributed, non-parametric approach to modeling choice that utilizes large quantities of data and distributed computational resources. The key insight to our approach is to treat all transactional data as indicative of an implicit comparison between products. For instance, if you purchased a red jacket when I showed you jackets in red, white, and blue, you\u2019re telling me that you prefer red to blue or white. More generally, such comparisons are ubiquitous in transactional and behavioral data both online and in the store, and we use large volumes of comparisons to build accurate models of choice. Coming full circle, we can now use these models to understand the precise set of products to place at a given store, with a view to maximizing revenue while minimizing assortment breadth, SKU count, and risk to the retailer.\nIn this talk, we will walk through an innovative new approach to machine learning that seeks to model and learn customer choice patterns and preferences from sparse transactional data.  We will then discuss how this approach helps retailers build hyper-local product assortments that are personalized to the foot traffic at each store, while simultaneously reducing assortment complexity and discovering new, surprising opportunities for growth."}, "big-data-conference-ny-2015/public/schedule/detail/45950": {"room": "1 E14", "title": "Pentaho featuring Forrester: Delivering Governed Data for Analytics at Scale", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45950", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/213309", "/big-data-conference-ny-2015/public/schedule/speaker/157499"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Top-performing organizations are now using data for competitive advantage, digging deeper into internal and external data. The way organizations provision and manage data has to change in order to keep up with empowered customers and nimble competitors. Listen to guest speaker Michele Goetz, internationally recognized Forrester Research principal analyst, discuss findings from Delivering Governed Data for Analytics at Scale, a June 2015 commissioned study conducted by Forrester on behalf of Pentaho on the topic of data governance and delivery.\nIn this session, Michele will discuss the results of the study, best practices of leading organizations in terms of delivering governed data, and recommendations on how organizations can effectively and confidently blend the right data and distribute it to the right users.\nThis session is sponsored by Pentaho"}, "big-data-conference-ny-2015/public/schedule/detail/46000": {"room": "1 E18 / 1 E19", "title": "Amazon Kinesis deep dive: Real-time streaming on Amazon Web Services", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46000", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/153678"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "Amazon Kinesis is a fully managed service for real-time streaming big data ingestion and processing. This talk explores Kinesis concepts in detail, including best practices for scaling your core streaming data ingestion pipeline. We then discuss building, and deploying Kinesis processing applications using capabilities like Kinesis Client Libraries, AWS Lambda, and Amazon EMR (via Spark)."}, "big-data-conference-ny-2015/public/schedule/detail/45814": {"room": "3D 06/07", "title": "How Autodesk is using Tableau to visualize its Kafka-Splunk-Hadoop pipeline", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45814", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/188781"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Autodesk is making the transition from a traditional boxed software company to the cloud. A key part of this transition is the development of a scalable analytics pipeline that Autodesk recently built on Kafka, Splunk, and Hadoop. Tableau and Tableau Server are key tools for helping rapidly optimize and deliver value from this pipeline. Whether its validating data coming out of the pipeline, performing ad-hoc analysis and data mash-ups, all the way to delivering easy to consume reporting to non-technical stakeholders \u2013 Tableau plays a key role at every point.\nThis session is sponsored by Tableau"}, "big-data-conference-ny-2015/public/schedule/detail/46003": {"room": "1 E14", "title": "Hadoop II: The SQL", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46003", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/194522"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Enormous estates of BI and Analytic workloads running on traditional large-scale Teradata, Oracle, IBM, and Microsoft systems are crashing into the price/performance wall of big data.   These systems can\u2019t cope with the ever-increasing volumes of data.  And even if they could cope, your wallet sure can\u2019t.  It\u2019s time for a change! Is Hadoop the answer?  Can Hadoop now handle your enterprise analytic workloads?\nActian SVP of Engineering Emma McGrattan will describe the various solutions that comprise the SQL on Hadoop landscape, identify the features that are important for those modernizing their enterprise analytic workloads on Hadoop, and describe some of the successes that Actian customers have had in moving their BI and Analytic workloads to Hadoop.\nThis session is sponsored by Actian"}, "big-data-conference-ny-2015/public/schedule/detail/45812": {"room": "1 E6 / 1 E7", "title": "Meeting the needs of the business: Real-time and historical big data for comprehensive security analytics and operational insights", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45812", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/216724"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Security teams study many months and years of data for baselining and incident forensics, but IT operations may only want to store weeks or months of data to analyze for operational insights. And the two different needs can be difficult to reconcile. Learn how TELUS\u2019s security analysts provide value to both teams.\nTELUS implemented a powerful tool for exploratory analytics of long-term historical data in Hadoop, along with high-volume, low-value data sources not required for real-time monitoring but valuable for forensics and compliance. With this wealth of detailed historical data, the security team can also provide operational insights into platform and service usage to TELUS business and operational units as a value-added service. The security group is no longer perceived as the bad guy enforcing security rules, but as the hero supporting business growth.\nThis session is sponsored by Splunk"}, "big-data-conference-ny-2015/public/schedule/detail/45813": {"room": "Javits North", "title": "Improving decisions", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45813", "abstract": "Katherine will discuss recent behavioral science research suggesting how a number of simple, inexpensive tools can be used to encourage improved decisions.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/214577"], "timing": "10:05am\u201310:25am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/45871": {"room": "1 E6 / 1 E7", "title": "Cognitive computing: From theory to ubiquity", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45871", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/1771"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "In this session, Tim Estes, CEO and founder of Digital Reasoning, will walk through key advances in artificial intelligence (AI) and natural language processing (NLP), which have enabled the emerging space of cognitive computing. We will first explore how many of Digital Reasoning\u2019s customers, both in the public and private sectors, are rapidly adopting cognitive computing technologies in order to uncover intelligence, fight human trafficking, and discover financial and medical insights from reservoirs of big data. Next, session attendees will get a look at some of Digital Reasoning\u2019s current research that is pushing the boundaries of language understanding with deep learning. We will take a look at what the future of machine cognition looks like, what new solutions are on the horizon, and which capabilities will be imperative for companies to survive and thrive in a data-driven future where cognitive computing will be ubiquitous. Be sure to visit the Digital Reasoning team at booth #665 in the Exhibit Hall.\nThis session is sponsored by Digital Reasoning"}, "big-data-conference-ny-2015/public/schedule/detail/44764": {"room": "Javits North", "title": "Thursday keynote welcome", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44764", "abstract": "Strata + Hadoop World Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/5107", "/big-data-conference-ny-2015/public/schedule/speaker/103766", "/big-data-conference-ny-2015/public/schedule/speaker/17816"], "timing": "8:45am\u20138:50am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/45894": {"room": "1 E15", "title": "How Riot Games uses Platfora to improve League of Legends' performance", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45894", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/144700", "/big-data-conference-ny-2015/public/schedule/speaker/134932"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "Riot Games is publisher of the mega-hit gaming phenomenon League of Legends. Since its introduction in 2009, League of Legends has grown a massive and loyal following, with more than 67 million playing every month. With this incredible growth, the company needed an analytics solution that would work well with their push-model data pipeline, one that would put analysis and exploration capability directly into the hands of the game designers, and one that would provide fast and flexible visualization of all their data.\nPlatfora, the leading big data discovery platform, addresses all of these needs, while providing new and unexpected insights into the world of League of Legends. In this session, Chris will discuss how their game designers use Riot\u2019s data pipeline and Platfora to measure and validate player-focused changes like improvements to game servers and client performance.\nThis session is sponsored by Platfora"}, "big-data-conference-ny-2015/public/schedule/detail/42844": {"room": "1 E8 / 1 E9", "title": "Advanced Data Science with Spark Streaming", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42844", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203040", "/big-data-conference-ny-2015/public/schedule/speaker/203364"], "timing": "2:25pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "We present StreamDM, a new real-time analytics open source software library built on top of Spark Streaming, developed at Huawei Noah\u2019s Ark Lab.\nThe tools and algorithms in StreamDM are specifically designed for the data stream setting. Due to the large amount of data that is created \u2013 and needs to be processed \u2013 in real-time streams, such methods need to be extremely time-efficient while using very small amounts of time and memory. StreamDM is the first library to include advanced stream mining algorithms for Spark Streaming willing to be the gathering point of practical implementation and deployments for large-scale data streams.\nThis new library contains methods for classification, regression, clustering and frequent pattern mining. In this talk, we will show how these advanced methods work in practice, discuss some big data analytics applications in telecommunication networks, compare them with the methods available in MLLib and spark.ml, and show their ease of use and extensibility."}, "big-data-conference-ny-2015/public/schedule/detail/46222": {"room": "3D 06/07", "title": "Harnessing Data to Change Banking for Good", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46222", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/213758"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Capital One is on a mission to Change Banking for Good. In service of our mission, Capital One\u2019s Data Lab works to invent solutions that harness data and advanced analytics to more deeply understand over 60 million customers and translate that understanding into simpler, more intuitive, and intelligent products and experiences that help our customers succeed.\nJoin Capital One as we take you through the journey of the Data Lab. How did we get started? What have we learned about mingling disciplines such as human centered design, full stack engineering, and data science? And how are we taking an entrepreneurial approach to develop successful solutions that deliver real impact?\nThis session is sponsored by Capital One"}, "big-data-conference-ny-2015/public/schedule/detail/45891": {"room": "1 E15", "title": "Fast fish eat slow fish: How to move faster", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45891", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/193122"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Whether you\u2019re a large enterprise or a startup, successfully competing with modern, nimble, fast-moving companies like Uber or Airbnb can only be done with modern, model-driven development environments and big data solutions. Infrastructure shouldn\u2019t restrict the interactions between relational data and big data. Development shouldn\u2019t slow analytics. Third-party software integration should be native.\nYou need a fluid solution that allows you to map and adapt to all the changing relationships between your customers, your company, your data, and your analytics. Find out what Canonical is doing to make you a fast fish, regardless of your size.\nThis session is sponsored by Canonical"}, "big-data-conference-ny-2015/public/schedule/detail/45333": {"room": "1 E6 / 1 E7", "title": "Innovation + growth", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45333", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/5107", "/big-data-conference-ny-2015/public/schedule/speaker/171704", "/big-data-conference-ny-2015/public/schedule/speaker/104526", "/big-data-conference-ny-2015/public/schedule/speaker/62617", "/big-data-conference-ny-2015/public/schedule/speaker/111682"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "This is a day to learn about the data innovations that have the potential to blindside even the most careful organizations. Aimed at decision makers, the Innovation + Growth program focuses on how data-oriented startups, academics, and venture capitalists approach innovation and the potential to disrupt incumbent business models.\nSession topics include AI, Machine Learning, Internet of Things (IoT) and beyond, across a number of industry segments. Attendees can expect an expansive program providing lessons and insights into what\u2019s possible and how to embrace innovation."}, "big-data-conference-ny-2015/public/schedule/detail/45899": {"room": "1 E15", "title": "Patterns from the future", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45899", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/123141"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Imagine the possibilities of having all of your data in one place \u2013 at a reasonable cost \u2013 with the computing potential to learn from relationships between data in all domains. Now imagine being able to react in a nimble and agile fashion, and use newly discovered relationships for good as quickly as possible. Advanced analytics and Hadoop are changing the way organizations approach big data. \nDuring this talk, attendees will get tips from the future and learn about key patterns emerging from a wide cross section of corporate and institutional Hadoop journeys. Attendees will:\n\nDiscover how organizations are establishing the enterprise data hub (alongside or encompassing the former data warehouse).\nFind out how others are building large shared clusters and \u201cmoving the work to the data.\u201d\nHear how organizations are serving a broad cross section of users and use cases, from traditional data science to ad hoc end user discovery.\nLearn how they\u2019re using techniques that range from old-school batch, to interactive, to acting directly to the stream from the fire hose.\nKnow how others are creating the environment where experiments that succeed are put to production in record time, and there are no failed experiments \u2013 just lessons for the future.\n\nThis session is sponsored by SAS"}, "big-data-conference-ny-2015/public/schedule/detail/43278": {"room": "3D 03/10", "title": "Architecting a data platform", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43278", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/133624", "/big-data-conference-ny-2015/public/schedule/speaker/105756", "/big-data-conference-ny-2015/public/schedule/speaker/198879"], "timing": "1:30pm\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nWhat are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive, and real-time analytical workloads.\nBy tracing the flow of data from source to output, we\u2019ll explore the options and considerations for components, including:\n\nAcquisition: from internal and external data sources\nIngestion: offline and real-time processing\nStorage\nProviding data services: exposing data to applications\nAnalytics: batch and interactive\nData management: data security, lineage, metadata, and quality\n\nWe\u2019ll give also advice on:\n\nTool selection\nThe function of the major Hadoop components and other big data technologies such as Spark and Kafka\nHardware sizing and cloud provisioning\nIntegration with legacy systems"}, "big-data-conference-ny-2015/public/schedule/detail/43373": {"room": "1 E20 / 1 E21", "title": "Netflix: Integrating Spark at petabyte scale", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43373", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/178161"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "The Big Data Platform team at Netflix maintains a cloud-based data warehouse with over 10 petabytes of data stored predominantly in Parquet format.  Our platform has traditionally leveraged Pig for ETL processing, Hive for large analytic workloads, and Presto for interactive and exploratory use cases.  For a long time, Spark seemed attractive to complement our platform, but technical gaps prevented effective use at scale in our environment.  Recent improvements have allowed us to add Spark to our cloud data architecture and interoperate seamlessly with the other tools and services in our stack.\nWe will go into detail about our deployment configuration and what it takes to run Spark alongside traditional workloads on YARN.  We will share examples of a few of our largest workflows translated to Spark for comparison in terms of both performance and complexity.  We also identified cases where big data tools were used to solve problems clearly out of their respective domains.  This resulted in awkward implementations that were elegantly solved by Spark.  Finally, we will share our vision of how Spark will evolve our platform and push the state of big data processing at Netflix."}, "big-data-conference-ny-2015/public/schedule/detail/45511": {"room": "Javits North", "title": "Playing with, and for, data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45511", "abstract": "Unusual collaborations can often lead to new ways of taking, and analyzing data. This talk looks at lessons learned from working with chefs, circus performers, and preschoolers.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/215433"], "timing": "9:10am\u20139:20am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43376": {"room": "3D 04/09", "title": "Preventing a big data security breach", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43376", "topics": "Security & Governance", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203928", "/big-data-conference-ny-2015/public/schedule/speaker/190839", "/big-data-conference-ny-2015/public/schedule/speaker/147601"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Attend this session to:\n\nGain an understanding of how securing big data differs from traditional enterprise security\nReview the Hadoop Security Maturity Model developed by Cloudera, MasterCard, and Intel\nLearn about the latest tools and initiatives around Hadoop platform security\nUnderstand the four pillars of security for a comprehensively secured and governed solution\nReview MasterCard\u2019s production use case which presently has 10PB of data running on a fully PCI-compliant cluster using Cloudera\u2019s Enterprise Data Hub.\n\nDon\u2019t miss this opportunity to hear from the market leaders in big data management systems, security, and silicon architectures."}, "big-data-conference-ny-2015/public/schedule/detail/44824": {"room": "Hall B", "title": "Cultivate: Leading Through Culture", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44824", "topics": "Cultivate", "speaker_urls": [], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "\u201cCommand and control\u201d no longer works. Systems, speed, and culture do.\nWe\u2019re at the cusp of a new network age. The companies defining it are fast, flat, and flexible. They devour data and focus obsessively on their customers. \u201cAnalyze and adapt\u201d is their Standing Operating Procedure. At Cultivate, experts from these leading companies will tell you how they do it\u2014and how you can, too.\nJoin us for Cultivate, a two-day event taking place during Strata + Hadoop World New York.\nYour business is facing perpetual change\u2014every business is. And that means everyone responsible for its success\u2014from team leaders up to CXOs\u2014must shift to a new way of managing. Here\u2019s what you\u2019re up against in today\u2019s business environment:\n\nProduct development cycles\u2013for both online and physical products\u2013are continually contracting, as durable goods with software at the core can be modified nearly as easily as a website.\n\n\nEvery company needs to navigate a deluge of data\u2014not to confirm the opinion of the highest-paid person in the room, but to spark intelligent discussion of strategies and options among employees, regardless of their position.\n\n\nYour customers no longer simply want things to work; they want products that please and delight them. Creating a great user experience is no longer just \u201cnice;\u201d it\u2019s essential.\n\nSimply put, organizations need to embrace continual change, leverage copious data, and adapt to a marketplace in which customers\u2019 needs and desires truly come first. Those that can will survive; those that can\u2019t face failure.\nThe kind of leadership needed for these nimble, responsive organizations isn\u2019t taught in traditional management programs. It\u2019s been worked out in practice by the engineers, designers, programmers, and data scientists who are living it. They have created companies that activate leadership at every level, from small, task-driven working groups to the board room.\nThis is core of Cultivate: growing talent for leadership within organizations, not from managers assigned to lead engineers or designers, but by identifying and encouraging designers and engineers with the ability to lead.\nA crucial element of this new business model is an organization\u2019s culture. At Cultivate, we\u2019ll explore the values, practices, and structures that enable organizations to respond with agility to changes in their products and the marketplace.\nIf you need to learn about growing teams within your organization; training the leadership it needs now and in the future; and creating processes that work (rather than work against you), you need to be at Cultivate. Learn from the leaders who have been through this before; meet others who are also struggling to build their businesses; and work with us to cultivate the leadership and organizational understanding we\u2019ll need for the next century."}, "big-data-conference-ny-2015/public/schedule/detail/42925": {"room": "1 E8 / 1 E9", "title": "Tackling machine learning complexity for data curation", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42925", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/187774"], "timing": "2:05pm\u20132:25pm Thursday, 10/01/2015", "abstract": "In this talk, we focus on entity consolidation, which is arguably the most difficult data curation challenge, because it is notoriously complex and hard to scale. The problem statement sounds deceptively simple: \u201cFind all the records from a collection of multiple data sources that refer to the same real-world entity.\u201d The problem has been traditionally modeled as a record clustering problem, followed by a mechanism to merge these resulting clusters into a unified representation.\nThere\u2019s a large body of work on this problem by both academia and industry. Techniques have included:\n\nHuman curation\nAutomatic discovery of clusters using predefined thresholds on record similarity\nMachine learning techniques to discover classifiers able to label pairs of records as matches\n\nUnfortunately, none of these techniques alone has been able to provide sufficient accuracy and scalability. This talk will provide deeper insight into the entity consolidation problem, and discuss how machine learning, human expertise, and problem semantics collectively can deliver a scalable, high-accuracy solution."}, "big-data-conference-ny-2015/public/schedule/detail/42924": {"room": "1 E8 / 1 E9", "title": "Probabilistic programming in data science", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42924", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/163777"], "timing": "1:35pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Machine learning is the driving force behind many recent revolutions in data science. Comprehensive libraries provide the data scientist with many turnkey algorithms that have very weak assumptions on the actual distribution of the data being modeled. While this blackbox property makes machine learning algorithms applicable to a wide range of problems, it also limits the amount of insight that can be gained by applying them.\nThe field of statistics on the other hand often approaches problems individually and hand-tailors statistical models to specific problems. To perform inference on these models, however, is often mathematically very challenging, and thus requires time-deriving equations as well as simplifying assumptions (like the normality assumption) to make inference mathematically tractable.\nProbabilistic programming is a new programming paradigm that provides the best of both worlds and revolutionizes the field of machine learning. Recent methodological advances in sampling algorithms like Markov Chain Monte Carlo (MCMC), as well as huge increases in processing power, allow for almost complete automation of the inference process. Probabilistic programming thus greatly increases the number of people who can successfully build statistical models and machine learning algorithms, and makes experts radically more effective. Data scientists can create complex generative Bayesian models tailored to the structure of the data and specific problem at hand, but without the burden of mathematical tractability or limitations due to mathematical simplifications.\nThis talk will provide an overview of the past, present, and future of probabilistic programming with many real-world applications from various domains. Various probabilistic programming languages like STAN or PyMC will be covered, and more advanced topics like Approximate Bayesian Computation (ABC) that allow inference even when the probability distribution of the data is intractable; as well as recent advances that enable probabilistic programming to work with big data."}, "big-data-conference-ny-2015/public/schedule/detail/43587": {"room": "1 E8 / 1 E9", "title": "Queering quant: How having all the data isn\u2019t enough to represent a complex social phenomena", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43587", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/149953"], "timing": "1:35pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "Big data quantitative methods of inquiry (\u201cquant\u201d) help us to see complex patterns and process large data sets in an effort to gain a better understanding of the world around us.  But our understanding of the world as we view it through data is only as diverse as the repositories we build and the variability we allow in our data sets.  Applying queer theory to data science can expand the kinds of questions we can ask of data, and the ways in which we derive meaning from data.\nQueer theory looks beyond binary either/or categorizations.  It looks for patterns that are contradictory and that are unconventional and unusual.  We can have all the data we want but if we don\u2019t explicitly address non-normative cases, we are just going to reinvent a simplistic representation of the world.  This presentation identifies some of the areas in data creation and analytics where we perpetuate the simplistic representation of the world. It demonstrates alternative ways of creating and analyzing data to take non-normative cases into consideration.\nIn data creation, applying queer theory can be as simple as framing a question about gender as more than a binary choice. Instead of asking people to select male or female, the question can be presented as a multi-select option of male, female, transgender, and other with the ability to specify what the user means by other.  Traditional question creation also typically presumes a specific normal way of being. For example, asking women how many children they have presumes that women want children.  Queering quant would ask people in general if they want children and then ask them how many they have.\nIn data analytics, queering quant can include looking for patterns that might not be coherent pictures.  Queer theory focuses on the ways our lives are contradictory, and suggests we need to look for realistic and sometimes contradictory patterns.  Queering quant in data analytics also means being explicit about who is being represented in the data and who is absent from the data, and seeks to capture the perspectives of those absent. For example, looking at income differentials between women and men and seeking hidden values in that metric can reveal absent perspectives that tell a more complete story.\nReframing questions provides a more complete data set that helps minimize the limitations we currently have with modeling social scenarios. Most importantly, queering quant embodies what we are looking for from big data: learn the complex social phenomena we live in so we can make better models to achieve whatever our end goals may be: social justice, market share, or something more complicated than that simple either/or."}, "big-data-conference-ny-2015/public/schedule/detail/45869": {"room": "3D 06/07", "title": "Combining open source software and cloud-native data processing services on Google Cloud Platform", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45869", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/216106"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "On one side are cloud-native data processing and analytics services, fully managed and delivered as a consumption-based utility, with proven scale, reliability, and cost-efficiency. On the other side, a bustling ecosystem of open source engines and tools which provide a panoply of capabilities and support a large set of deployment options. Which do you choose? Trick question, you don\u2019t have to choose. By running Spark, Hadoop, IPython and other open source software in a way which takes advantage of the capabilities of the Cloud you can mix and match the operational experience of Cloud-native services with the versatility and familiarity of the Hadoop toolset.\nIn this talk, we will describe a Cloud-optimized deployment model for Spark and Hadoop, and explore how these tools and Cloud-native services complement each other to form the most productive and efficient data processing platform.\nThis session is sponsored by Google"}, "big-data-conference-ny-2015/public/schedule/detail/43506": {"room": "1 E18 / 1 E19", "title": "Big data at a crossroads: Time to go meta (on use)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43506", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/102993"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "In its early days, Hadoop was notable as a vibrant open source community, which rallied around a common architectural hypothesis and an open software ecosystem.  More recently, Hadoop has evolved into a marketplace, supporting a growing variety of goods and services.  As the Hadoop market scales, incentives shift toward increased differentiation, with various parties seeking to introduce unique value in the ecosystem.  These incentives are generally positive forces, driving innovation both within the open source core and in value-added proprietary solutions.  But the pressures of differentiation also bring risks of fragmentation, with the potential for isolated and incompatible competing components that could eventually stymie the growth of innovative uses of data.\nTo maintain a degree of cohesion and continue to incentivize innovation, the Hadoop ecosystem requires an agreed-upon medium for interoperation among software components, and collaboration among users. In a data-rich environment, this medium is provided by metadata services.  Over the past six months, we have witnessed rapidly-growing community interest\u2014from both non-profit and commercial parties\u2014in a new generation of open metadata services for the Hadoop ecosystem.\nMetadata services need to be significantly revisited in the evolving big data context, which is so often focused on agile analytics and structure-on-use.  In 20th century enterprises, metadata was typically a deeply engineered artifact\u2014the blueprint for a enterprise-wise data edifice, painstakingly designed in advance of construction to provide a \u201cgolden master\u201d of enterprise truth. 20th century metadata management software was designed for this engineering philosophy, and its associated waterfall engineering processes.\nIn modern big data systems, the lion\u2019s share of metadata arises through agile work processes, and needs to be managed in new ways.  It is true that some metadata will continue to be produced by design: this includes metadata for core system functionality including security metadata about users, permissions, and access control. However, the bulk of valuable metadata in the modern context will be generated as metadata on use: emergent, contextual information that arises naturally when data is assessed and analyzed for use.\nTo get a sense of this distinction, consider a typical scenario of how metadata can be generated in the lifecycle of modern data-driven business; similar scenarios arise in scientific applications.  An innovative consumer electronics company decides to leverage usage logs from their devices to understand customer behavior, improve product usability, and offer new differentiated features to their customers.  This effort begins with aggressive data wrangling: gathering raw logs across a variety of devices, teasing apart their structure and content, assessing and remediating data quality, and blending the multiple logs to enable analyses across users, devices, time and geography.   The details that are uncovered need to be logged, and will not have the shape and style of traditional data.  For example, some metadata may describe rich detail about the data content. (\u201cThe clock for product XYZ gets reset on hardware restart, so timestamps should not be trusted until GPS turns on and sets the clock\u201d.)  Some metadata may describe usage and expertise (\u201cDJ Patil worked on this data at timestamp T; see file foo.ipynb in github for ways to chart it\u201d.)  Some may describe data lineage, a topic of great interest to scientists interested in reproducibility of data-centric experiments. (\u201cThis data set was generated via an Oozie job described in file bar.xml, using data sources Q, R and S\u201d).  These are just a few examples of the breadth of rich metadata that emerges organically through usage.  Note that while the examples were described above in prose, 21st century metadata will inevitably be a mixture of human-generated annotations and information generated by an expanding ecosystem of software tools.\nMost importantly, note that some of the key content and structure of this metadata cannot be anticipated in advance: it is a product of the exploratory process of data analysis, and is inherently metadata-on-use.  This is a very good thing: data is best understood by an organization when the data is actively being worked.  Analysts know their data best when they are hip-deep in it, doing aggressive data wrangling and interactive, experimental analytics. Without breaking flow, the analyst\u2014with assistance and automation from the software they work with\u2014should be able to fluidly capture and store information about both the data, and the manner and context in which it is being used.\nTo support this fluid, unanticipated generation of knowledge, metadata services must be able to continuously support new users, new data sources, new types of metadata and new software components.  At the same time, they have to provide an environment in which people\u2014and software\u2014can add value over time: mining, culling and organizing metadata in accordance with its utility, measured both in grass-roots terms (e.g., via frequency of use) and strategic measure (e.g., stated value to the organization).\nIn order to succeed in the Hadoop environment, a new metadata service needs to meet basic criteria of interoperability and openness suited to metadata on use.  The most important of these criteria can be derived from previously successful systems like HDFS:\n\nIt needs to be an open-source, vendor-neutral project.\nIt needs to provide a minimum of functionality and a maximum of flexibility, to leave opportunities for a broad range of unanticipated uses and value-added services.\nIt needs to scale out arbitrarily, both in volume and in workload; experience shows that metadata services can be big data problems in their own right.\n\nIn this talk, we will describe the need for metadata-on-use services in the big data context, and the reason why various constituencies in the community benefit from \u201cgoing meta\u201d in an open way.   We will explain the need for metadata services with use cases in both big science and enterprise deployments.  Finally, we will lay out the design challenges and opportunities endemic to systems supporting metadata-on-use."}, "big-data-conference-ny-2015/public/schedule/detail/46092": {"room": "1 E6 / 1 E7", "title": "How Pepsi wrangles the diverse data of consumer packaged goods", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46092", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/219430", "/big-data-conference-ny-2015/public/schedule/speaker/220824"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "The success of consumer packaged goods (CPG) companies relies on the seamless communication between a large interconnected network of suppliers, production facilities, logistics partners, retailers, etc\u2026 Despite this heavy reliance on coordination, each member in this network generates and shares information in a wide variety of data volumes and formats \u2014 making it difficult for CPG organizations to analyze, compare, and share date between members of their network.\nIn this talk, Pepsi analyst Matt Derda and Trifacta Director Customer Success Doug Stradley discuss how data wrangling can empower CPG companies to analyze these disparate data sources holistically, and dramatically improve their company\u2019s bottom line.\nThis session is sponsored by Trifacta"}, "big-data-conference-ny-2015/public/schedule/detail/45835": {"room": "Javits North", "title": "Sponsored keynote", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45835", "abstract": "Details to come\u2026", "speaker_urls": [], "timing": "9:10am\u20139:20am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43471": {"room": "1 E8 / 1 E9", "title": "How Airbnb uses machine learning to detect host preferences", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43471", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204016"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "At Airbnb we seek to match people who are looking for accommodation \u2013 guests \u2013 with those looking to rent out their place \u2013 hosts. Guests reach out to hosts whose listings they wish to stay in; however a match succeeds only if the host also wants to accommodate the guest.\nI first heard about Airbnb in 2012 from a friend. He offered his nice apartment when he traveled to see his family during our vacations from grad school. His main goal was to fit as many booked nights as possible into the 1-2 weeks when he was away. My friend would accept or reject requests depending on whether or not the request would help him to maximize his occupancy.\nAbout two years later, I joined Airbnb as a data scientist. I remembered my friend\u2019s behavior and was curious to discover what affects hosts\u2019 decisions to accept accommodation requests, and how Airbnb could increase acceptances and matches on the platform.\nWhat started as a small research project resulted in the development of a machine learning model that learns our hosts\u2019 preferences for accommodation requests based on their past behavior. For each search query that a guest enters on Airbnb\u2019s search engine, our model computes the likelihood that relevant hosts will want to accommodate the guest\u2019s request. Then, we surface likely matches more prominently in the search results. In our A/B testing the model showed about a 3.75% increase in booking conversion, resulting in many more matches on Airbnb. In this blog post I outline the process that brought us to this model."}, "big-data-conference-ny-2015/public/schedule/detail/42619": {"room": "1 E8 / 1 E9", "title": "Learning to love Bayesian statistics", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42619", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/202122"], "timing": "2:25pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Bayesian statistical methods provide powerful tools for answering questions and making decisions. \u00a0For example, the result of Bayesian analysis is a set of values and probabilities that can be fed directly into a cost-benefit or risk analysis, which is not possible with conventional statistics.\nBut there are several barriers to widespread adoption: people with the knowledge and ability to apply these methods to practical problems are rare, and there are few accessible resources for developing these skills.\nIn this presentation, I will explain the advantages of Bayesian methods over classical approaches using concrete examples like A/B testing. \u00a0I will recommend resources and suggest steps data science teams can take to develop skills and begin applying Bayesian methods to real-world problems.\nTopics include:\n\nWhat\u2019s wrong with classical statistics?\nHow do Bayesian methods address these problems?\nWhat are the challenges?\nHow do we get started?\nHow do these methods scale up in production?"}, "big-data-conference-ny-2015/public/schedule/detail/44872": {"room": "Javits North", "title": "Data Science for Mission", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44872", "abstract": "The mission statement of the Central Intelligence Agency (CIA) \u2013 We are the nation\u2019s first line of defense. We accomplish what others cannot accomplish and go where others cannot go. \u2013 underscores the unique nature of the work being done by intelligence officers, including data scientists. Data scientists at CIA have extensive problem sets, interesting restrictions, and a laser focus on the discovery of critical pieces of information that have far-reaching implications. In his ten minute keynote, CIA Chief Information Officer Douglas Wolfe discusses how data science is a true team sport and how the rapid evolution of this field continually improves the impact of the CIA mission.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/210559"], "timing": "8:50am\u20139:00am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/45805": {"room": "Javits North", "title": "Unleashing the power of big data today", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45805", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/189554"], "timing": "9:40am\u20139:45am Wednesday, 09/30/2015", "abstract": "IoE, IoT, and big data \u2013 three topics you hear and read about often in our various industries. Let\u2019s quickly look at these market and technology dynamics, and see how they are each in their own way \u2019democratizing\u2019 data access and analysis, resulting in new businesses, technologies, and improved community solutions throughout the world.\nThis keynote is sponsored by Cisco"}, "big-data-conference-ny-2015/public/schedule/detail/43178": {"room": "1 E8 / 1 E9", "title": "We need better maps: Smarter spatial clustering at the city level", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43178", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/137714"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Research in urban science and geospatial analytics is often hampered by arbitrary choices of the spatial unit of analysis. At best, the choice of an appropriate unit of analysis forces further parameterization of an already complex problem; at worst, using arbitrarily or historically defined units like ZIP codes or Census tracts allows local variation to dilute regional variation, making it difficult or impossible to discern patterns at a macro level.\nWith the increasing availability of spatial data (especially geocoded point-level data reflecting individual observations), and advances in spatial analytics, researchers can perform their own aggregations to obviate arbitrary divisions in the data as originally collected. However, because statistically significant effects require data at a sufficiently high level of aggregation, the same tradeoff persists: either the researcher resorts to arbitrary units that obscure or magnify underlying effects, or they create their own units in an attempt to isolate regions that behave differently. This latter solution, a form of clustering, requires ever more parameters: how many clusters to create, how large to make each cluster (and how to define size), which variables on which to create the clusters, how to weight the variables relative to each other, how to evaluate the best solution, and so on. We do not believe this problem has been adequately solved for spatial analysis, especially in the context of large quantities of open municipal data and streaming data.\nBrett Goldstein, a senior fellow at the University of Chicago Harris School of Public Policy and one of the leaders of the Plenar.io open data portal, will describe his team\u2019s work on a \u201csmart clustering\u201d approach that builds units of analysis on the fly based on the variables of interest. The algorithm, built on the max-p regions implementation by researchers at Arizona State University, creates spatial clusters using only one input parameter, the minimum size of each cluster. This nonparametric approach creates clusters that fit the data as closely as possible, fully isolating regions based only on the variables of interest.  Using the example of 311 calls, he will describe how city managers currently collect data within units like administrative wards and ZIP codes, show how this dilutes true variation in the data, and demonstrate how smart clusters allow researchers to ask and answer better questions.\nGoldstein\u2019s work on smart clustering is part of an overall approach to start seeing the urban environment as fluid and driven by people, not streets, highways, and arbitrary lines drawn by city planners. In other words, the map changes depending on what the user cares about, with neighborhood boundaries that shift depending on only the variables of interest. This paves the way to prediction on a larger scale: we can ask not only how patterns of traffic or crime change, but how the de facto boundaries of neighborhoods and cities shift over time."}, "big-data-conference-ny-2015/public/schedule/detail/43177": {"room": "3D 04/09", "title": "Transparent encryption in HDFS: The missing piece in big data security", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43177", "topics": "Security & Governance", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157428"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Data encryption is a requirement for many business sectors dealing with confidential information, such as finance, healthcare, and government. For example, HIPAA, FISMA, and DCI all require that data is encrypted while it is in-flight (being transferred over the network), and when it is at-rest (stored durably on disk). There can also be additional restrictions surrounding access, management, and storage of encryption keys.\nTo meet these requirements, transparent, end-to-end encryption was added to HDFS. Once configured, data read from and written to certain HDFS directories is transparently encrypted and decrypted without requiring any changes to user application code. This encryption is also end-to-end, meaning that data is protected both in-flight and at-rest, and can only be encrypted and decrypted by the client. This improves security since HDFS itself never handles unencrypted data or data encryption keys. Furthermore, through the use of a new cluster service, the Hadoop Key Management Server (KMS), the responsibilities of key administration and HDFS administration can be separated, further enhancing security.\nDuring this talk, we will cover the design, implementation, and usage of transparent encryption in HDFS."}, "big-data-conference-ny-2015/public/schedule/detail/44906": {"room": "1 E8 / 1 E9", "title": "Mapping big data: A data driven market report", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44906", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/76630"], "timing": "1:15pm\u20131:35pm Wednesday, 09/30/2015", "abstract": "Description\nRelato has created a big data market report in collaboration with O\u2019Reilly Media. We mapped partnerships between companies, and ran various social network metrics on the resulting data. These results were then interpreted by market experts, and compiled into a report.\nThis talk will cover the making of the report, as well as the insights from the report, and graph analytics in general."}, "big-data-conference-ny-2015/public/schedule/detail/46017": {"room": "Javits North", "title": "What does it take to apply data science for social good?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46017", "abstract": "More than good intentions or a good knowledge of R, using data science to tackle social issues depends on close collaboration between data science and issue area experts. But getting everyone to the table is only the first step. Jake Porway, founder and executive director of DataKind, unveils five keys for successful data science for good projects, based on the organization\u2019s three years of work rallying thousands of volunteers worldwide to give back.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/148636"], "timing": "9:50am\u201310:00am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/42852": {"room": "3D 02/11", "title": "Oulu Smart City pilot", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42852", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/202927"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Oulu Smart City in Finland has a lively living lab tradition. We continuously collect data from transportation, infrastructure, and people; expand our ecosystem of data providing and consuming companies, research institutes, city officials, and citizens; and we develop services on top of the ecosystem.\nTo exploit our data lake, we need different technologies and algorithms to handle the streams of mobility-related data, aiming to reach capability for distributed real-time analysis and low latency ad hoc queries. In this talk, a run-through on real-world use cases will be presented, and for the use cases, we discuss the required components.\nOur services include situational pictures for city dwellers, a real-time speeding alert system, interactive dashboards to visualize statistics about the traffic flow, a recommendation system for individual drivers, realization of distributed reasoning on traffic events, and our plans to experiment on the edges of the network with mobile code.\nWe discuss the pros and cons of different technologies we have tested in our use cases:\n\nLambda architecture with data collection and ingestion with Apache Kafka, and implementation of the batch layer using Hadoop File-system (HDFS), Apache Hive, and Apache Spark. The speed layer consists of Kafka combined with the Apache Storm stream processing engine. The serving layer in our platform consists of OpenTSDB and Apache Hbase.\nTraffic data analytics platform implementing heterogeneous data aggregation from different sources: moving object data, weather, etc. to HDFS using Flume, and performing data filtering and real-time analysis with Spark Streaming alongside with Flume. The system periodically executes Spark (or MapReduce) jobs to analyze the data in HDFS.\nPython and R for data analysis."}, "big-data-conference-ny-2015/public/schedule/detail/44163": {"room": "1B 03", "title": "Practical data science on Hadoop (Day 2)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44163", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/190721", "/big-data-conference-ny-2015/public/schedule/speaker/155534", "/big-data-conference-ny-2015/public/schedule/speaker/198568", "/big-data-conference-ny-2015/public/schedule/speaker/45559", "/big-data-conference-ny-2015/public/schedule/speaker/220734"], "timing": "9:00am\u20135:00pm Wednesday, 09/30/2015", "abstract": "Description\nIn this three-day course, you will:\n\nLearn how to use machine learning, text analysis, and real-time analytics to solve frequently \nencountered, high-value business problems.\nUnderstand data science methodology and end-to-end work flow of problem solution including \ndata preparation, model building and validation, and model deployment.\nUse Apache Spark and other tools for analytics.\n\nAgenda\nDay 1\n\nFundamental data science methodology\nOverview of selected machine learning methods\nHands-on labs with Spark MLlib and SystemML libraries\nDescriptive statistics\nFeature transformations\nSupervised and unsupervised methods\nDiagnostics\n\nDay 2\n\nText analytics concepts\nText analytics development, testing, and deployment\nContinuous analytics (streaming)\nHands-on labs on text analytics and streaming\n\nDay 3\n\nRecommendation engines with hands-on lab\nUsing Apache Spark with IBM SPSS Modeler\nWhat\u2019s coming in data science\nSpark and hardware accelerators\nMachine learning pipelines with hands-on lab\nProductization with Spark\n\nTarget Audience\nData scientists, business analysts.\nSome knowledge of R and/or Python is preferable but not required.\nAdditional Information\nHands-on lab environment will be provided by IBM."}, "big-data-conference-ny-2015/public/schedule/detail/44164": {"room": "1B 03", "title": "Practical data science on Hadoop (Day 3)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44164", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/190721", "/big-data-conference-ny-2015/public/schedule/speaker/155534", "/big-data-conference-ny-2015/public/schedule/speaker/198568", "/big-data-conference-ny-2015/public/schedule/speaker/45559", "/big-data-conference-ny-2015/public/schedule/speaker/220734"], "timing": "9:00am\u20135:00pm Thursday, 10/01/2015", "abstract": "Description\nAgenda\nIn this three-day course, you will:\n\nLearn how to use machine learning, text analysis, and real-time analytics to solve frequently \nencountered, high-value business problems.\nUnderstand data science methodology and end-to-end work flow of problem solution including \ndata preparation, model building and validation, and model deployment.\nUse Apache Spark and other tools for analytics.\n\nDay 1\n\nFundamental data science methodology\nOverview of selected machine learning methods\nHands-on labs with Spark MLlib and SystemML libraries\nDescriptive statistics\nFeature transformations\nSupervised and unsupervised methods\nDiagnostics\n\nDay 2\n\nText analytics concepts\nText analytics development, testing, and deployment\nContinuous analytics (streaming)\nHands-on labs on text analytics and streaming\n\nDay 3\n\nRecommendation engines with hands-on lab\nUsing Apache Spark with IBM SPSS Modeler\nWhat\u2019s coming in data science\nSpark and hardware accelerators\nMachine learning pipelines with hands-on lab\nProductization with Spark\n\nTarget Audience\nData scientists, business analysts.\nSome knowledge of R and/or Python is preferable but not required.\nAdditional Information\nHands-on lab environment will be provided by IBM."}, "big-data-conference-ny-2015/public/schedule/detail/44167": {"room": "1B 04", "title": "Designing and Building Big Data Applications", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44167", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/160550"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Take your knowledge to the next level and solve real-world problems with training for Hadoop and the enterprise data hub\nCloudera University\u2019s three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).\nYou will work through the entire process of designing and building solutions, including ingesting data, determining the appropriate file format for storage, processing the stored data, and presenting the results to the end-user in an easy-to-digest form. Go beyond MapReduce to use additional elements of the EDH and develop converged applications that are highly relevant to the business.\nHands-on Hadoop \nThrough instructor-led discussion and interactive, hands-on exercises, participants will navigate the Hadoop ecosystem, learning topics such as:\n\nCreating a data set with Kite SDK\nWriting user-defined functions for Hive and Impala\nTransforming data with Morphlines\nIndexing data with Cloudera Search\nBuilding a Search UI with Hue\n\nAudience and prerequisites\nThis course is best suited to developers, engineers, and architects who want to use Hadoop and related tools to solve real-world problems. Participants should have already attended Cloudera Developer Training for Apache Hadoop or have equivalent practical experience.\nGood knowledge of Java and basic familiarity with Linux are required. Experience with SQL is helpful.\nDeveloper certification\nUpon completion of the course, attendees are encouraged to continue their study and register for a Cloudera certification exam. Certification is a great differentiator; it helps establish you as a leader in the field, providing employers and customers with tangible evidence of your skills and expertise."}, "big-data-conference-ny-2015/public/schedule/detail/43699": {"room": "1 E10 / 1 E11", "title": "Unboxing data startups", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43699", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157962"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Most people are familiar with the basic principles driving today\u2019s hottest big data and enterprise companies. But what\u2019s really going on underneath the hood? In this session, Kleiner Perkins Caufield & Byers General Partner Michael Abbott unboxes a variety of startups in the space to examine the technology, architecture, and innovations they\u2019ve harnessed to deliver superior products and services.\nNOTE: The specific companies to be featured are still under consideration, but to give you a sense of the companies, last year\u2019s session included Eugene Mandel, data science lead at Jawbone; Christopher Pouliot, the VP of data science at Lyft; and Mike Polcari, chief architect at 23andMe (Source: http://strataconf.com/big-data-conference-ca-2015/public/schedule/detail/38507). Other prior companies have included Gilt Groupe, Airbnb, and Lookout, among others (http://strataconf.com/stratany2014/public/schedule/detail/36129)."}, "big-data-conference-ny-2015/public/schedule/detail/42992": {"room": "3D 05/08", "title": "Apache Drill bootcamp", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42992", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/103127", "/big-data-conference-ny-2015/public/schedule/speaker/152293"], "timing": "1:30pm\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nIn this tutorial you\u2019ll learn how to use Apache Drill, the open source, distributed schema-free, SQL engine. At the end of this tutorial, you\u2019ll be able to explore and analyze your data in place with standard SQL queries or BI tools, whether the data is sitting in files on your laptop, or in HDFS, HBase, MongoDB, or even a relational database.\nAgenda:\n\nApache Drill overview\nHello World!\nData model and data types\nData sources: storage plugin architecture; using storage plugins; navigating the namespace; HDFS; Hive; HBase; MongoDB\nMetadata in Drill: decentralized metadata; optional schemas; information catalog\nExploring, analyzing, and transforming data: exploration (SELECT * LIMIT 10 and Drill Explorer); analysis (SELECT); transformation (CREATE TABLE AS)\nUsing virtual datasets (Views): why virtual datasets?; creating virtual datasets (CLI and Drill Explorer); virtual dataset internals (.drill); how virtual datasets are exposed\nAPIs: ODBC; JDBC; REST; C; Java\nClients: CLI; BI (Excel, Tableau, etc.); Python (PyData, Pandas); R\nQuerying complex and/or schemaless data: handling schemaless data; traditional BI on complex data\n\nNote that this is a hands-on tutorial, so bring your laptop and you\u2019ll be able to run all the examples (as well as some of your own queries) throughout the tutorial."}, "big-data-conference-ny-2015/public/schedule/detail/42993": {"room": "1 E20 / 1 E21", "title": "Effective testing of Spark programs and jobs", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42993", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/128567"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "This session explores best practices of creating both unit and integration tests for Spark programs as well as acceptance tests for the data produced by our Spark jobs. We will explore the difficulties with testing streaming programs, options for setting up integration testing with Spark, and also examine best practices for acceptance tests.\nUnit testing of Spark programs is deceptively simple. The talk will look at how unit testing of Spark itself is accomplished, as well as factor out a number of best practices into traits we can use. This includes dealing with local mode cluster creation and teardown during test suites, factoring our functions to increase testability, mock data for RDDs, and mock data for Spark SQL.\nTesting Spark Streaming programs has a number of interesting problems. These include handling of starting and stopping the Streaming context, and providing mock data and collecting results. As with the unit testing of Spark programs, we will factor out the common components of the tests that are useful into a trait that people can use.\nWhile acceptance tests are not always part of testing, they share a number of similarities. We will look at which counters Spark programs generate that we can use for creating acceptance tests, best practices for storing historic values, and some common counters we can easily use to track the success of our job.\nRelevant Spark Packages & Code:\nhttps://github.com/holdenk/spark-testing-base / http://spark-packages.org/package/holdenk/spark-testing-base \nhttps://github.com/holdenk/spark-validator"}, "big-data-conference-ny-2015/public/schedule/detail/44896": {"room": "3D 06/07", "title": "Introduction to visualizations using D3", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44896", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/25170"], "timing": "9:00am\u201312:30pm Tuesday, 09/29/2015", "abstract": "Description\nIn this tutorial we\u2019ll walk through the basic types of charts and graphs and how we can create them using D3. When we understand the basics, we can begin to build more complex visualizations.\nYou need to take into consideration more than just the data. For instance, we need to consider issues with color, size, resolution, and audience. There are several tools to help and we\u2019ll look at some of the gotchas to make sure your designs are the best possible.\nEven if you have the data, selected the right chart, and managed to make it suit your needs, you still need to figure out exactly what story you want to tell. By putting a laser focus on just a few aspects of what you are trying to convey and telling that story as best as possible, you can make the best impact. We\u2019ll go through some of those storytelling techniques.\nUsing D3 and other tools, we\u2019ll go through some examples and show you how you can be a hero at your job. The goal is for you to go back afterward and impress your co-workers with your new data design skills."}, "big-data-conference-ny-2015/public/schedule/detail/42994": {"room": "1 E18 / 1 E19", "title": "Google Cloud Dataflow - two worlds become a much better one", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42994", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/198748"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Big Data processing is challenged by four conflicting desires: reducing the time to answer questions (minimizing latency), increasing the accuracy of answers to said questions (correctness), finding the easiest implementation path (simplicity), and effectively managing expenses for systems used to process said questions (minimizing cost).  These desires are then mapped across traditional batch processing and current stream processing models, resulting in a tempest of challenges.\nThe batch processing world (e.g. MapReduce, SQL) has evolved over time to become a highly accurate and consistent processing model for massive datasets; however, the underlying systems to produce these results create inherent delay. Moreover, they require constant tuning to deal with variable data size and data structure, varying runtime complexity, and inflexible boundaries like finite CPU resources.\nThe stream processing world is delivering on the promise of true low-latency processing; however, existing implementations (e.g. Apache Spark, Apache Storm) pull forward much of the operational overhead of provisioning, management, and tuning akin to MapReduce, plus the additional complexity of managing in-flight data.  These systems also lack sufficient correctness constructs, thereby limiting the types of questions that can be processed.\nGoogle has encountered all of these problems and much more at internet scale.  Over the past decade we solved them with a variety of programming models (focused on developer agility and portability) and highly scalable distributed services.  Technologies like MapReduce, FlumeJava, and Millwheel were born from a need to drive down latency, while increasing simplicity and attempting to reduce cost.  These technologies, combined with the Google Cloud Platform, have been synthesized into a single solution for big data processing \u2013 Google Cloud Dataflow.\nGoogle Cloud Dataflow intelligently merges the worlds of batch and stream processing into a unified programming model.  This model \u2013 and more specifically the underlying execution environment \u2013 aims to remove the polarity between correctness and latency, AND strike the right balance between simplicity and operational robustness.  Cloud Dataflow is also a fully managed, highly scalable, strongly consistent processing service for both batch and stream-based processing.  Finally, Google has committed itself to support an open source ecosystem for its native model implementations (e.g.  Cloud Dataflow SDK for Java 7).  This is spurring alternate language implementations like Scala, community-driven extensions, and alternate execution environments for Dataflow like Spark and Flink.\nThis session will:\n\nDrill into the Cloud Dataflow programming model primitives (i.e. building a pipeline) and compare and contrast common development patterns found in MapReduce and Apache Spark\nTeach you how NOT to manage a cluster or a tune a job. Cloud Dataflow pushes new boundaries for the meaning of a \u201cfully managed service\u201d including: automated cluster management, dynamic work rebalancing, and throughput based auto-scaling\nDemonstrate how to frictionlessly move between batch- and stream-based processing with flexible and simple correctness controls\nReview techniques for monitoring and debugging pipelines\nDemonstrate how to execute Cloud Dataflow pipelines on alternate runners like Apache Spark and Apache Flink"}, "big-data-conference-ny-2015/public/schedule/detail/45889": {"room": "1 E14", "title": "Think like a data scientist: Build your big data blueprint", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45889", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/108271"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Bill Schmarzo, EMC CTO of Global Services, and author of Big Data: Understanding How Data Powers Big Business, will utilize a workshop approach to help you identify where and how to integrate data and analytics into your business strategies. During the session, you\u2019ll use a workbook to apply data science techniques to identify how you can efficiently extract business value through insights gained from new and existing data sources.  You will also be led through an interactive exercise to identify and prioritize big data opportunities for one of Bill\u2019s favorite businesses.\nFinally, you\u2019ll gain an understanding of how EMC Federation Business Data Lake provides the foundation for your organization\u2019s big data and data science initiatives, accelerating time-to-value and mitigating implementation and production risks with a pre-engineered data lake platform.\nAll attendees will receive a coupon code for a free copy of Bill\u2019s forthcoming book Big Data MBA: Driving Business Strategies with Data Science.\nThis session is sponsored by EMC"}, "big-data-conference-ny-2015/public/schedule/detail/43366": {"room": "3D 04/09", "title": "Ethical big data - what's legal and what's right", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43366", "topics": "Law, Ethics, & Open Data", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/156867", "/big-data-conference-ny-2015/public/schedule/speaker/203928", "/big-data-conference-ny-2015/public/schedule/speaker/190839"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "While technologies like Hadoop offer amazing opportunities to use data, as a Gartner Analyst commented: it\u2019s important to stop big data \u201ccrossing the creepy line.\u201d Governance and security experts from Cloudera and MasterCard will discuss the legal and ethical usage of big data. This will include what\u2019s legal and some of the specific legal restrictions that vary dramatically between countries and states, as well as what\u2019s right \u2013 how to make sure your big data usage never crosses the creepy line.\nFundamentally, ethical behavior drives trust. They are inseparably linked. If we want customers to trust us, and to continue to do business with us, and not to ask their representatives to shut us down, then we must design and execute an ethical framework on data usage for all data (not just big). Companies will also need to consider creating an Ethical Chief Data Officer role to enable and support this framework."}, "big-data-conference-ny-2015/public/schedule/detail/45443": {"room": "1 E16 / 1 E17", "title": "Great debate: Big data will live in the cloud", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45443", "topics": "Business & Innovation", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/17816", "/big-data-conference-ny-2015/public/schedule/speaker/87848", "/big-data-conference-ny-2015/public/schedule/speaker/146119", "/big-data-conference-ny-2015/public/schedule/speaker/28960"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Data has gravity. Jim Gray once said that, \u201ccompared to the cost of moving bytes around, everything else is free,\u201d and because of what this means for the economics of computing, the more data you have, the more it wants to be near other data. That means all big data systems, eventually, will live in centralized cloud environments.\nOn the other hand, different data is processed in different ways, and much of it is private, governed by organizational policies and national borders. So data lives in islands, carefully insulated from attack or misuse. Rather than pay recurring service costs, companies want to own and depreciate their hardware. And mainframes aren\u2019t going away. So big data systems will live on-premises.\nConfused? Hopefully this session will help you sort it out. In this Oxford-style debate, two teams try to convince you they\u2019re right, and you vote for the winner. The no-holds-barred argument promises to be lively and controversial, so come pick a side."}, "big-data-conference-ny-2015/public/schedule/detail/43268": {"room": "3D 04/09", "title": "Personal information out of context: Building a consumer subject review board", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43268", "topics": "Law, Ethics, & Open Data", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203951", "/big-data-conference-ny-2015/public/schedule/speaker/100367"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "In our data-driven economy, new uses of data continuously emerge in a wide variety of contexts. Many of these new uses of personal information are natural extensions of current practices, well within the expectations of consumers and the boundaries of traditional Fair Information Practice Principles. In other cases, data uses may be outside consumer expectations, and traditional mechanisms for providing users with notice and choice unavailable.\nIn some cases, organizations should not proceed due to heightened risks to the affected individuals or broader ethical concerns. In other cases, new data uses more offer tremendous benefits to individuals, communities, or society at large. An ethical analysis would allow such uses to proceed, but how can organizations \u2014 and for-profit ventures in particular \u2014 approach ethical questions around data use?\nAttendees will learn about recent calls for the creation of consumer subject review boards to evaluate new data uses, which was embraced by the White House in its recent Consumer Privacy Bill of Rights proposal under the title of \u201cPrivacy Review Boards.\u201d\nThey will also learn about some of the existing conceptual and practical challenges to addressing ethics in data use. The speakers will recommend that organizations build upon existing ethical frameworks, namely the seminal Belmont Report on ethical principles for human subject research in the biomedical and behavioral sciences, and the more recent Menlo Report for computer and information security research. Specifically, this presentation proposes:\n\nTo discuss how the foundational principles of (1) beneficence, (2) justice, and (3) respect for persons can be applied in the context of big data\nTo explain how these principles can be adapted to the fast-paced data economy, where speed and scale is of the utmost importance.\n\nPhilosopher Evan Selinger will discuss some of the chief ethical issues around non-contextual uses of personal information, and how these issues are already posing challenges for privacy and other compliance officers within organizations. Joseph Jerome, policy counsel at the Future of Privacy Forum, will discuss how these challenges could be channeled through the creation of formalized review boards. He will offer a possible framework for developing a consumer subject review board, including organizational structure and basic substantive rules, based on existing corporate practices and lessons taken from institutional review boards."}, "big-data-conference-ny-2015/public/schedule/detail/43302": {"room": "3D 02/11", "title": "Elastic stream processing without tears", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43302", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/122725"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Walmart handles more than 1 million customer transactions every hour. The average Boeing 737 engine generates 10 terabytes of data every 30 minutes in flight. By 2020, researchers estimate there will be 100 million internet connected devices. To process this data in real time\u2014whether from mobile phones or jet engines\u2014will be the new normal. How are companies today adapting to this new real-time stream of data, using open source projects that allow them to do this kind of stream processing at scale, including Apache Kafka, Apache Storm, Apache Samza, Apache Spark, and so on?\nAt the same time, how are organizations adapting the compute power depending on business needs or to accommodate the relentless growth in inbound traffic? True elastic stream processing can be achieved by combining a highly-scalable platform like Apache Mesos, with stream-processing frameworks built on top of it such as Marathon, Spark, Kafka, and new emerging solutions. In this talk we will discuss the use cases and requirements, and demonstrate a Mesos-based solution for elastically processing data streams."}, "big-data-conference-ny-2015/public/schedule/detail/43263": {"room": "1 E16 / 1 E17", "title": "Simplifying Hadoop: A secure and unified data access path for compute frameworks", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43263", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/143642", "/big-data-conference-ny-2015/public/schedule/speaker/143657"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "One of the key values of the Hadoop ecosystem is its flexibility. There is a myriad of components that make up this ecosystem, allowing Hadoop to tackle otherwise intractable problems. However, having so many components provides a significant integration, implementation, and usability burden. Features that ought to work in all the components often require sizable per-component effort to ensure correctness across the stack.\nIn this talk, we introduce a new service to address this problem. The service provides an API to read data from Hadoop storage managers and return them as canonical records. This eliminates the need for components to support individual file formats, handle security, perform auditing, and implement sophisticated IO scheduling and other common processing that is at the bottom of any computation.\nWe discuss the architecture of the service and the integration work done for MapReduce and Spark. Many existing applications on those frameworks can take advantage of the service with little to no modification. We demonstrate how this provides fine grain (column level and row level) security, through Sentry integration, and improves performance for existing MapReduce and Spark applications by up to 5\u00d7. We conclude by discussing how this architecture can enable significant future improvements to the Hadoop ecosystem."}, "big-data-conference-ny-2015/public/schedule/detail/45880": {"room": "1 E6 / 1 E7", "title": "Building predictive applications with real-time data pipelines", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45880", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/131490"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "What happens when trillions of sensors go online? By 2020, this could be a reality. Real-time mobile applications will become integral to capturing, processing, analyzing, and serving massive amounts of data from these sensors to millions of users. Predictive applications will become the new design paradigm, and users will expect the ultimate personalization from each of their devices. Join Eric Frenkiel, CEO and co-founder of MemSQL, as he illustrates compelling use cases for bringing billions of data points into the light of day, with Apache Spark, Apache Kafka, and MemSQL as a real-time data pipeline for advanced processing and data analytics. Eric will dig deeper into the key attributes of this data pipeline, including persistence and performance, in order to take your business from insight to impact.\nThis session is sponsored by MemSQL"}, "big-data-conference-ny-2015/public/schedule/detail/45883": {"room": "1 E14", "title": "Real data, real implementations: What actual customers are doing", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45883", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217427"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "By now, we\u2019re all used to the big data rhetoric: data is a huge business asset; those that leverage their data will earn significantly more revenue than those who don\u2019t; wrangling data is hard; and getting \u201cactionable insights\u201d from data is the Holy Grail.\nBut beyond the euphoria of what big data can do, and the stress that comes from feeling that you\u2019re not doing enough, how can you really get started?  What are some concrete things you can do and some reasonable results you can expect?\nThis panel, moderated by Datameer and ZDNet\u2019s Andrew Brust, and featuring real customers who are technology implementation leaders, will help you answer these questions. Cut through the hype, gather advice gleaned from sober experience and level-set your expectations, realistically. Data has always been the lifeblood of business technology. This panel will help you understand how newer data and analytics technology can continue that tradition while transcending past limitations.\nThis session is sponsored by Datameer, Inc."}, "big-data-conference-ny-2015/public/schedule/detail/45378": {"room": "1 E10 / 1 E11", "title": "Where\u2019s the puck headed?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45378", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/118939", "/big-data-conference-ny-2015/public/schedule/speaker/173887", "/big-data-conference-ny-2015/public/schedule/speaker/145786", "/big-data-conference-ny-2015/public/schedule/speaker/155324", "/big-data-conference-ny-2015/public/schedule/speaker/188542"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In this panel discussion, we\u2019ll consider the big trends in big data, asking top-tier VCs to look over the horizon and discuss the visions they have two or more years in the future."}, "big-data-conference-ny-2015/public/schedule/detail/43266": {"room": "1 E8 / 1 E9", "title": "Running experiments with logged-out users: Solving the mixed group problem", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43266", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203706", "/big-data-conference-ny-2015/public/schedule/speaker/203949"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Airbnb runs A/B tests to better understand user behavior and make data-informed product decisions. As a marketplace, it\u2019s important to us to understand the behavior of users who aren\u2019t logged in, which means that we can\u2019t always assign users to treatment groups using their userid. But this means that users who log in from multiple devices will sometimes see multiple treatments\u2014which makes it impossible to attribute their behavior to either one!\nWe call this the Mixed Group Problem. Using observations of these users in the final experimental analysis would create ambiguous results; leaving them out entirely introduces problems as well. If your company runs experiments on users who aren\u2019t logged in, it\u2019s likely that you\u2019re encountering the same problem.\nIn this talk, we\u2019ll discuss:\n\nHow we quantified the impact of the Mixed Group Problem at Airbnb (it\u2019s higher than you might think)\nAll of the insidious ways that the Mixed Group Problem could be biasing your experiments\nDifferent approaches to mitigating the mixed group problem\nThe conceptual framework we created to understand all the different circumstances that can create the Mixed Group Problem, and which can be prevented\nThe engineering solution that we built to solve it, as well as all the freaky implementation challenges that you\u2019re likely to run into along the way."}, "big-data-conference-ny-2015/public/schedule/detail/45887": {"room": "3D 03/10", "title": "Virtual reality: From immersive visualization to data-driven narrative", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45887", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217108"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Data is all science, no art. Think of a film that inspired or moved you. Now imagine the filmmaker decided that instead of making the film, they would present the material to you in the form of a graph or a chart. That\u2019s where we are with data. Data collection and analysis needs scientists. Data visualization needs artists. The communication of insights from data needs storytellers. It\u2019s all about mindset, perspective and point-of-view.\nVirtual Reality headsets turn a smartphone into a fully immersive cinematic display. The technology is improving steadily while the costs are declining. A VR display allows a user to experience the world of data from the inside, haptics (or other inputs) enable fluid interaction and navigation, and the phenomenon of presence can be used to examine complex human experiences such as empathy.\nIf we want to unlock the full potential of data we need to welcome different perspectives. Innovation happens when things that are separate get mixed."}, "big-data-conference-ny-2015/public/schedule/detail/45886": {"room": "Javits North", "title": "Keynote with David Hyman", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45886", "abstract": "David Hyman, CEO & Co-Founder at Chosen", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/1033"], "timing": "9:45am\u20139:55am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/42508": {"room": "3D 03/10", "title": "LIVE from New York: An introduction to Linked Immersive Visualization Environments", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42508", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/200945"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Financial markets are a highly adversarial data environment, in which the ability to quickly analyze and understand data provides a decisive advantage. In such an environment, flexibility, speed, and most importantly, human intuition are key to gaining insight.  Data visualizations are a core tool for enhancing analyst effectiveness.\nLIVE is a collection of technologies and visualizations with the overarching goal of improving situational awareness of real-time data, by utilizing low-latency, linked visualizations to create an immersive data exploration environment.  Best-in-breed visualizations can be combined to create a highly responsive system where near-instant feedback from the machine allows the operator to visually explore and query their data in real time.\nIn this talk, we will address the technological and design challenges that arise from building linked data visualizations that can function at the speed and scale necessary for visualizing financial markets data, including:\n\nHow to use multiple visualizations to view complex relationships among various data dimensions and data sources\nEmploying a common visual grammar across many different visualizations\nUtilizing multiple visualization technologies in a single system, including d3, WebGL, and WebVR\nLatency and state, how to increase immersion by decreasing reaction time for interactive visualizations\nUI, UX, and user testing to design intuitive workflows for the financial domain."}, "big-data-conference-ny-2015/public/schedule/detail/43510": {"room": "1 E10 / 1 E11", "title": "The science behind #TheDress: Measuring virality at BuzzFeed", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43510", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204027"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "How did #TheDress gain international attention in just a few hours? In this session, we analyze how shared BuzzFeed content \u201cgoes viral.\u201d We begin with a presentation of previously unexplored graph data, then share how we define and compute virality on large-scale data. We conclude with the application of such analysis, demonstrating BuzzFeed\u2019s ability to rapidly disseminate critical information to a global audience."}, "big-data-conference-ny-2015/public/schedule/detail/45391": {"room": "3D 05/08", "title": "Ask me anything: Hadoop's storage gap - resolving transactional access/analytic performance tradeoffs", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45391", "topics": "Ask Me Anything", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/75982", "/big-data-conference-ny-2015/public/schedule/speaker/214065", "/big-data-conference-ny-2015/public/schedule/speaker/214066", "/big-data-conference-ny-2015/public/schedule/speaker/137795", "/big-data-conference-ny-2015/public/schedule/speaker/214067", "/big-data-conference-ny-2015/public/schedule/speaker/190064"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Ask the panel questions about the tradeoffs between real-time transactional access, and fast analytic performance, from the perspective of storage engine internals."}, "big-data-conference-ny-2015/public/schedule/detail/43466": {"room": "3D 05/08", "title": "Real-world NoSQL schema design", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43466", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/109262"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "There are lots of claims about the benefits of NoSQL databases, but few realistic demonstrations of the impact that such a database can have on anything more than toy-sized data.  In this talk, I will deconstruct a real-world database schema into the corresponding NoSQL design.\nThe database that I will use is the Musicbrainz database, which exhibits many important idioms found in real databases, such as factoring relations into multiple tables to implement column families, linkage tables, and many-to-one relationships. The transformations that I will highlight show how almost all of the auxiliary tables in the original design are reduced to a format that is much simpler to understand \u2013 nested data structures. As a result, the number of tables drops by nearly 5x and the ease of understanding the design increases by a similar degree.\nIn spite of such radical structural changes, the resulting denormalized and nested data can still be queried with SQL by using Apache Drill, and the queries are often noticeably simpler than the queries used against the original data structures. The methods presented in this talk are practical and easy to apply, and can sometimes even be largely automated.\nI will also show how a percolator pattern can be used to allow the resulting NoSQL database to be automatically maintained in multiple NoSQL technologies simultaneously, so that full text search, recommendations, and the HBase API can all be used to access the same data."}, "big-data-conference-ny-2015/public/schedule/detail/43143": {"room": "3D 02/11", "title": "Building a Hadoop data application", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43143", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/43459", "/big-data-conference-ny-2015/public/schedule/speaker/183010"], "timing": "1:30pm\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nIn the second (afternoon) half of the Architecture Day tutorial, attendees will build a data application from the ground up. As a part of the tutorial, we will demonstrate how Kite codifies the best practices from the Hadoop Architecture Day morning session."}, "big-data-conference-ny-2015/public/schedule/detail/46226": {"room": "1 E14", "title": "Enter the Snake Pit for Fast and Easy Spark & Cassandra", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46226", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/182647"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Everyone knows that Python isn\u2019t suitable for massive scale analytics, right? Wrong. Spark 1.3 introduced data frames, which allow for high performance Spark batch jobs, streaming, and machine learning over massive datasets. In this talk you\u2019ll learn how to combine Cassandra, a highly scalable, always on OLTP data store, with PySpark, a framework for distributed computation.  You\u2019ll learn how PySpark matches the performance that you\u2019d see from either Scala or Java, something only a year ago was impossible.  I\u2019ll show you how to unleash the full power of SQL with Cassandra data \u2013 that means joins, aggregations, and complex where clauses. Lastly I\u2019ll show you how to wrap up your queries with rich visualizations. So let\u2019s get started and put the death wrap on your data.\nThis session is sponsored by DataStax"}, "big-data-conference-ny-2015/public/schedule/detail/42894": {"room": "1 E20 / 1 E21", "title": "What's coming for the Spark community", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42894", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/152591"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "In the last year Spark has seen substantial growth in adoption as well as the pace and scope of development. This talk will look forward and discuss both technical initiatives and the evolution of the Spark community.\nOn the technical side, I\u2019ll discuss two key initiatives ahead for Spark. The first is a tighter integration of Spark\u2019s libraries through shared primitives such as the data frame API. The second is across-the-board performance optimizations that exploit schema information embedded in Spark\u2019s newer APIs. These initiatives are both designed to make Spark applications easier to write and faster to run.\nOn the community side, this talk will focus on the growing ecosystem of extensions, tools, and integrations evolving around Spark. I\u2019ll survey popular language bindings, data sources, notebooks, visualization libraries, statistics libraries, and other community projects. Extensions will be a major point of growth in the future, and this talk will discuss how we can position the upstream project to help encourage and foster this growth."}, "big-data-conference-ny-2015/public/schedule/detail/44763": {"room": "Javits North", "title": "Wednesday keynote welcome", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44763", "abstract": "Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/5107", "/big-data-conference-ny-2015/public/schedule/speaker/103766", "/big-data-conference-ny-2015/public/schedule/speaker/17816"], "timing": "8:45am\u20138:55am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43389": {"room": "3D 02/11", "title": "Building a real-time analytics stack with Kafka, Samza, and Druid", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43389", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/153565", "/big-data-conference-ny-2015/public/schedule/speaker/153566"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "The maturation and development of open source technologies has made it easier than ever for companies to derive insights from vast quantities of data. In this session, we will cover how to build a real-time analytics stack using Kafka, Samza, and Druid.\nAnalytics pipelines running purely on Hadoop can suffer from hours of data lag. Initial attempts to solve this problem often lead to inflexible solutions, where the queries must be known ahead of time; or fragile solutions where the integrity of the data cannot be assured. Combining Hadoop with Kafka, Samza, and Druid can guarantee system availability, maintain data integrity, and support fast and flexible queries.\nIn the described system, Kafka provides a fast message bus and is the delivery point for machine-generated event streams. Samza and Hadoop work together to load data into Druid. Samza handles near-real-time data, and Hadoop handles historical data and data corrections. Druid provides flexible, highly available, low-latency queries."}, "big-data-conference-ny-2015/public/schedule/detail/43144": {"room": "1 E20 / 1 E21", "title": "How Spark is working out at Comcast scale", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43144", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/188348", "/big-data-conference-ny-2015/public/schedule/speaker/191562"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Spark is redefining the big Data ecosystem and opening doors to capabilities not available before. Comcast is moving in the direction of adopting Spark for several projects, ranging from real-time processing, data science, and large scale analytics, to the Xfinity personalization platform.\nThis talk will showcase some of the use cases and how we use Spark to solve many of the tough problems at Comcast scale.\n1. Real time processing is a challenging topic, and historically we had to use a mixture of home-grown and off-the-shelf technologies to get a working model. We will talk about how we use Kafka to get events from over 40 million STBs, and hand over to Spark streaming which processes the events.\n2. Data science is a very challenging field and more requirements pop up all the time. We are looking at Spark to build a reliable flexible framework that integrates with Hadoop, HBase, and Solr as well as provide ease of migration for data scientists who work in R and SAS.\n3. Comcast provides personalized recommendations to its customers on the X1 platform. Our initial implementation was built on the Hadoop map-reduce framework using a batch computation model. When we wanted to explore how we can offer real-time recommendations, we looked to Spark because of its increased computational efficiency, and its ease in developing both streaming and batch processing solutions using the same code base.\nIn short, there are many fields that can profit from the plethora of capabilities provided by Spark."}, "big-data-conference-ny-2015/public/schedule/detail/44600": {"room": "3D 03/10", "title": "Design and prototyping approaches to data experiences", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44600", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/206808", "/big-data-conference-ny-2015/public/schedule/speaker/206807"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Data shapes the experiences of the people who interact with it.  It informs the way businesses and other organizations communicate internally, interact with customers and clients, and form perceptions of themselves.  Data has also increasingly become a critical challenge for organizations across a wide range of sectors \u2013 from governments to businesses to philanthropies.\nIDEO uses a human-centered design approach to help influential companies and organizations with challenging data problems.  Our approach looks at data in several different ways:\n\nData as a medium to be designed\nData as a tool for informing and structuring design solutions\nData as a material for prototyping and building\n\nThis talk will focus on how IDEO uses these lenses and looks at the holistic journey of users who interact with data, to work with clients and design experiences based on data. In this talk we will present case studies around creative process, design, prototyping with data, and data solutions."}, "big-data-conference-ny-2015/public/schedule/detail/43380": {"room": "1 E12/ 1 E13", "title": "Use case examples of building applications on Hadoop with CDAP", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43380", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/194760"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Hadoop is often used as a cost-effective augmentation of an enterprise data warehouse, or a new business intelligence tool. A greater opportunity exists if a broader range of developers could harness the power of Hadoop analytics not just for BI, but also for operational applications. The developer challenges to implement solutions on Hadoop are significant.\nIn this presentation, we will provide a brief background on the Cask Data Application Platform (CDAP), and then delve into three customer case studies.  The case studies will show how organizations can accelerate Hadoop solution development and deployment across a range of use cases and vertical markets.  In all of the examples, we will show how a data application platform can help organizations more effectively use existing talent, minimizing the cost of specialized Hadoop developers or consultants.\nEach of these customers dealt with a common set of challenges driven by the evolution of Hadoop.  Tremendous innovation around Hadoop and NoSQL technologies has resulted in a proliferation of data storage and processing options that require unique skills. The tight coupling of data, logic, and infrastructure makes reuse difficult. Finally, any operational systems require life cycle management, which is largely absent from the current Hadoop ecosystem.\nMuch like application servers such as Weblogic opened the door for application innovation on relational databases, the Hadoop ecosystem will progress faster with a data application platform. It has been about a year since the CDAP was contributed to open source to address the needs of developers.  Since then, thousands of developers have downloaded the platform or cloned the GitHub repo, highlighting the project\u2019s role as the leading platform for developers on Hadoop."}, "big-data-conference-ny-2015/public/schedule/detail/45879": {"room": "1 E14", "title": "Commercializing IOT: What do you need to know?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45879", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/151135"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Discover the internet of everything (IoT); smart, connected products that are transforming businesses, and how Deloitte is innovating and commercializing an idea \u201cwhose time has come.\"\nThe IoT continues to give rise to new business models in Retail, Industrial Manufacturing, Healthcare, Insurance, Medical device manufacturers, Telecommunications, and Technology. Learn what those efforts are and how to capitalize on these opportunities for your clients.\nOur conversation will include the following:\n\nThe IoT ecosystem\nImpact of IoT on industries\nHow to unlock value for your clients\nSelected use cases\nImplementing IoT as telematics, digital home, and smart grids\n\nThis session is sponsored by Deloitte Consulting"}, "big-data-conference-ny-2015/public/schedule/detail/43383": {"room": "1 E12/ 1 E13", "title": "Hive + Amazon EMR + S3 = Elastic big data SQL analytics processing in the cloud, a real-world case study", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43383", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/202739"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "The Financial Industry Regulatory Authority (FINRA) acquires approximately 30 billion market events per day that are ingested, aggregated, and analyzed for the purpose of performing surveillance of the US markets. FINRA uses cloud computing and leverages big data technology to process an ever-increasing volume of financial data.\nFINRA\u2019s Market Regulation Surveillance Program runs hundreds of surveillance algorithms and patterns daily against hundreds of terabytes of market data to detect market manipulation, compliance breeches, and other potentially illegal activities. FINRA is leveraging a Hadoop architecture running on Amazon Web Services (AWS) cloud infrastructure.  This architecture provides elastic capacity so that FINRA can easily handle dynamic workloads while benefiting from operational economies of scale.\nThis talk will focus on how FINRA is successfully migrating a large portfolio of SQL-oriented batch analytics jobs from an in-house proprietary MPP database appliance platform to a cloud based SQL over the Hadoop platform, consisting of Hive, Amazon EMR, and S3. This presentation covers the following:\n\nCurrent MPP-based architecture behind FINRA\u2019s surveillance patterns\nPain points with the MPP platform that are related to data silos, cost, and non-elasticity\nNew architecture solution that\u2019s designed based on SQL on Hadoop and IaaS using Hive, AWS EMR, and S3\nSQL over Hadoop Vs Java map reduces considerations for implementation\nHIVE performance tuning for large-scale analytics\nLeveraging elasticity for real-world problems\nUtilizing SQL combined with Hive UDFs and UDTFs \u2013 a way to get the best of both worlds!\nLessons learned, risks, and mitigation strategies applied for migration\nFuture evolution of SQL over Hadoop\nFuture plans and the role of computation platforms like SPARK"}, "big-data-conference-ny-2015/public/schedule/detail/44117": {"room": "3D 05/08", "title": "Data 101", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44117", "topics": "Business & Innovation", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/184273", "/big-data-conference-ny-2015/public/schedule/speaker/146540", "/big-data-conference-ny-2015/public/schedule/speaker/137697", "/big-data-conference-ny-2015/public/schedule/speaker/1", "/big-data-conference-ny-2015/public/schedule/speaker/166455", "/big-data-conference-ny-2015/public/schedule/speaker/133627"], "timing": "9:00am\u201312:30pm Tuesday, 09/29/2015", "abstract": "The data ecosystem has matured to the point where the pluses and minuses of distributed systems are clear, and best practices for building and managing both the human and the technical side of a big data program have emerged. Now is the time to step back and take a fresh look at the options. Whether starting a data science program, reaching the breaking point with your current data technology, or figuring out what the competition is up to, these sessions will give you a bird\u2019s-eye view of data technologies, techniques, and data-driven organizations."}, "big-data-conference-ny-2015/public/schedule/detail/45829": {"room": "Javits North", "title": "Big data, context  and sensemaking systems", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45829", "abstract": "Jeff Jonas, IBM Fellow; Chief Scientist, Context Computing", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/4055"], "timing": "10:25am\u201310:45am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/44118": {"room": "1 E14 / 1 E115", "title": "Data-driven business day", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44118", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/17816", "/big-data-conference-ny-2015/public/schedule/speaker/169393", "/big-data-conference-ny-2015/public/schedule/speaker/1305", "/big-data-conference-ny-2015/public/schedule/speaker/213347", "/big-data-conference-ny-2015/public/schedule/speaker/167430", "/big-data-conference-ny-2015/public/schedule/speaker/146451", "/big-data-conference-ny-2015/public/schedule/speaker/202897", "/big-data-conference-ny-2015/public/schedule/speaker/181476", "/big-data-conference-ny-2015/public/schedule/speaker/189027", "/big-data-conference-ny-2015/public/schedule/speaker/204038", "/big-data-conference-ny-2015/public/schedule/speaker/203394", "/big-data-conference-ny-2015/public/schedule/speaker/204185"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "For business strategists, marketers, product managers, and entrepreneurs, Data-Driven Business looks at how to use data to make better business decisions faster. Packed with case studies, panels, and eye-opening presentations, this fast-paced day focuses on how to solve today\u2019s thorniest business problems with big data. It\u2019s the missing MBA for a data-driven, always-on business world."}, "big-data-conference-ny-2015/public/schedule/detail/44119": {"room": "1 E10 / 1 E11", "title": "Hardcore data science", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44119", "topics": "Hardcore Data Science", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/9477", "/big-data-conference-ny-2015/public/schedule/speaker/187782", "/big-data-conference-ny-2015/public/schedule/speaker/209645", "/big-data-conference-ny-2015/public/schedule/speaker/192508", "/big-data-conference-ny-2015/public/schedule/speaker/187749", "/big-data-conference-ny-2015/public/schedule/speaker/211273", "/big-data-conference-ny-2015/public/schedule/speaker/180940", "/big-data-conference-ny-2015/public/schedule/speaker/211685", "/big-data-conference-ny-2015/public/schedule/speaker/211683", "/big-data-conference-ny-2015/public/schedule/speaker/164026", "/big-data-conference-ny-2015/public/schedule/speaker/211101", "/big-data-conference-ny-2015/public/schedule/speaker/204077", "/big-data-conference-ny-2015/public/schedule/speaker/147848"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Data science is a hot topic, but much of it is simply business intelligence in a new mantle. In this track, we push the envelope of data science, exploring emerging topics and new areas of study, made possible by vast troves of raw data and cutting-edge architectures for analyzing and exploring information. We\u2019ll cover topics such as data management, machine learning, natural language processing, crowdsourcing, and algorithm design.\nWho should attend: Data scientists, data engineers, statisticians, data modellers, and analysts with a strong understanding of data science fundamentals will find themselves at home in this tutorial, as will CTOs, chief scientists, and academic researchers."}, "big-data-conference-ny-2015/public/schedule/detail/45970": {"room": "TBD", "title": "Mobile App Test", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45970", "abstract": "Mobile App Test", "speaker_urls": [], "timing": "6:15pm\u20136:30pm Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43317": {"room": "1 E20 / 1 E21", "title": "Lifelogging for insights", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43317", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/138285"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Lifelog is the product that pushed Sony Mobile to become a data-driven company. In this talk we will show how we use large scale analytics on Spark to generate insights to Lifelog users about themselves and the population, through some specific examples. We also use analytics of usage data to understand our users and to improve our applications with data-driven design. Furthermore we have developed a user lifecycle model that allows us to formulate and test hypotheses for actions toward increased user engagement and retention. We will present the model and discuss some results.\nExample applications will also be demonstrated, showing how third party application developers can access and integrate Lifelog data and insights into their applications with permissions from the user, through the open Lifelog API."}, "big-data-conference-ny-2015/public/schedule/detail/43419": {"room": "3D 05/08", "title": "Building a production-ready data lake in the cloud", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43419", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204029"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Hadoop\u2019s ability to handle large amounts of varied data has been a critical enabler of the big data revolution. Many organizations\u2019 ambitions to become more data-driven, however, are held back by a shortage of resources as well as the time and expense needed to purchase and set up Hadoop\u2019s hardware and software underpinnings. The cloud offers a natural alternative that overcomes these barriers and also promises superior economics with elastic scaling.\nIn this session, attendees will learn the key steps and best practices for implementing Hadoop in the cloud, to support a production-ready data lake use case. The process will be broken down into clear steps, including:\n\nPlanning: understanding which workloads are a good match for Hadoop, selecting a distribution, performance benchmarking, scoping memory, and compute resource requirements\nProvisioning: setting up public cloud resources and installing Hadoop\nMoving data: connecting to data sources, encrypting data in movement, optimizing WAN transfer\nConnecting and using analytical tools: ensuring a seamless transition for end-users for all workloads, monitoring and guaranteeing workload SLAs are met\nManaging infrastructure: monitoring health, ensuring availability, disaster recovery\nEnsuring security: data encryption, managing users/roles/groups, key management, VPN\n\nThe session will conclude by sharing additional lessons learned, best practices and business outcomes."}, "big-data-conference-ny-2015/public/schedule/detail/43090": {"room": "1 E10 / 1 E11", "title": "Science fiction to product: Data-driven development", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43090", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157181"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Many organizations have built strong technical and cultural data processes, and yet still struggle with moving their use of data beyond analytics. This talk is about thinking through the new opportunities your data offers to optimize and create new products and businesses; and finding a better answer to the \u201cwhat do we do now?\u201d question.\nData products are products or features that are only possible because of the existence a use of a data resource that may itself be collected as part of the product. Standout examples are Dark Sky, Foursquare, OPower, and others. This talk will give specific examples of data product development, the processes and research that went into them, and a technical perspective on current undervalued algorithm/product opportunities. The goal is to offer attendees a process for creative and technical development that will be useful tomorrow."}, "big-data-conference-ny-2015/public/schedule/detail/43312": {"room": "1 E18 / 1 E19", "title": "How companies are using Tachyon, a memory-centric distributed storage", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43312", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157596"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "Memory is the key to fast big data processing. This has been realized by many, and frameworks such as Spark and Shark already leverage memory performance. As data sets continue to grow, storage is increasingly becoming a critical bottleneck in many workloads.\nTo address this need, we have developed Tachyon, a memory-centric fault-tolerant distributed storage system, which enables reliable file sharing at memory-speed across cluster frameworks such as Spark and MapReduce. The result of over three years of research and development, Tachyon achieves both memory-speed and fault tolerance.\nTachyon is Hadoop compatible. Existing Spark and MapReduce programs can run on top of it without any code changes. Tachyon is the default off-heap option in Spark, which means that RDDs can automatically be stored inside Tachyon to make Spark more resilient and avoid GC overheads. The project is open source and is already deployed at multiple companies. In addition, Tachyon has more than 80 contributors from over 30 institutions, including Yahoo, Tachyon Nexus, Redhat, Nokia, Intel, and Databricks. The project is the storage layer of the Berkeley Data Analytics Stack (BDAS) and also part of the Fedora distribution.\nIn this talk, we introduce Tachyon. We will present its architecture, performance evaluation, as well as several use cases we have seen in the real world."}, "big-data-conference-ny-2015/public/schedule/detail/43311": {"room": "3D 02/11", "title": "High performance results using Spark to analyze mining equipment sensor data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43311", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/159966"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Remote health monitoring (RHM) systems collect data from sensors on mining equipment in use across the globe to analyze productivity, availability, utilization, and status. Issues with legacy RHM systems include low performance, low scalability in meeting growing equipment implementations and data volumes, availability and manageability issues (up to 10 hours of down time), and escalating costs to maintain the system. Using an open source technology stack and distributed processing, we implemented a solution that delivers high performance, low latency results \u2013 achieving over 120,000 writes per second sustained.\nWe will share the technical architecture and tools we used to implement the solution:\n\nKafka provisions for data ingestion from collector servers and event queuing to Spark\nSpark performs real-time complex event processing and trend level analysis\nSpark provisions for remote analysis and storage in Cassandra distributed database\nVisualization\n\nSome of the challenges encountered in the project included collecting data across different geographies, processing the data in real-time, applying business rules, and providing remote monitoring. We will also cover technical benchmarks achieved for streaming, distribution, in-memory computation, data syncing into Cassandra, and visualization as well as some tips for easy integration."}, "big-data-conference-ny-2015/public/schedule/detail/43093": {"room": "1 E8 / 1 E9", "title": "How data science helps prevent churn at Avira, a 100-million user company", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43093", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203524", "/big-data-conference-ny-2015/public/schedule/speaker/203825"], "timing": "1:15pm\u20131:35pm Thursday, 10/01/2015", "abstract": "Customer churn rate, also known as customer attrition, customer turnover, or customer defection, is defined as the loss of customers, in percentage. While for startups it is most important to gather new users, one of the greatest challenges for a mature company is to keep customers happy and therefore to keep the churn rate as low as possible.\nIt is very simple to ask your former customers why they decided to leave, but once you have this data what can you do? What if one feature of your product or business strategy makes some users happy while others find it as a reason to switch to the competition? How can one service be adjusted to every single user?\nIn this session Iulia Pasov, machine learning engineer, and Calin Burloiu, big data engineer, will focus mostly on details about how to predict user churn. Topics such as data science, big data, machine learning, natural language processing, sentiment analysis, Hadoop, Spark, etc. will not be neglected. The presentation will touch the most important steps in the user churn prevention process:\n\nDiagnosis \u2013 identifying users who leave, and obtaining details about their discontent by sentiment analysis and text mining. This is where the main reasons for dissatisfaction are stated.\n\n\nUnderstanding \u2013 comprehension into why the same product might make some users happy while others end up frustrated or distressed. The data scientist\u2019s job becomes a detective\u2019s job: tracing each user\u2019s path from the first installation.\n\n\nTreatment \u2013 choosing customized behaviour for handling each case.\n\n\nPrevention \u2013 recognizing users who might be afflicted in the future and giving them the ideal treatment.  This is where machine learning comes to the rescue to predict which users might fall into frustration before they actually do."}, "big-data-conference-ny-2015/public/schedule/detail/43568": {"room": "1 E8 / 1 E9", "title": "Data modeling for data science: Simplify your workload with complex types", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43568", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/143615", "/big-data-conference-ny-2015/public/schedule/speaker/134623", "/big-data-conference-ny-2015/public/schedule/speaker/204253"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Complex types (structs, arrays, and maps) and the resulting nested schemas initially gained prominence with XML as a niche solution for document-based data. However, over the past few years they have become mainstream in Hadoop-based data modeling and storage: virtually all modern serialization and storage formats (JSON, Protocol Buffers, Avro, Thrift, Parquet, ORC) now support complex types, and most Hadoop-based analytic frameworks allow the user to interact with nested schemas.\nIn this talk, we will explain how data scientists use nested data structures in order to increase their analytic productivity. We will use two well-known relational schemas \u2013 TPC-H and Twitter \u2013 to demonstrate how to simplify data science workloads with nested schemas. As part of that, we will outline best practices for converting flat relational schemas into nested schemas, and give examples of data science-style analysis utilizing Impala\u2019s recently added support for complex types in SQL. Aside from their expressive power, nested schemas, when married to modern columnar formats such as Parquet, also enhance productivity through performance gains, which we will again demonstrate by comparing and contrasting the nested and flat relational versions of the TPC-H and Twitter schemas."}, "big-data-conference-ny-2015/public/schedule/detail/43569": {"room": "1 E16 / 1 E17", "title": "Modern query processing with columnar formats: The best is yet to come", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43569", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/128388", "/big-data-conference-ny-2015/public/schedule/speaker/204255", "/big-data-conference-ny-2015/public/schedule/speaker/204256"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Apache Parquet is an open-source file-format which arranges all of its data into columns \u2013 this is distinct from the traditional row-oriented layout, which stores entire rows consecutively. Columnar data offers lots of advantages to modern data engines \u2013 like Impala, Apache Spark, and Apache Flink \u2013 in terms of IO efficiency, but the full benefits of the format are yet to be realized.\nWe have been working with Intel to apply modern CPU instruction sets to the common programming tasks associated with querying data in Parquet format: decompression, predicate evaluation, and row-reconstruction. Our work has yielded significant speedups in standard query benchmarks running on Cloudera\u2019s Impala SQL query engine, and very high speedups in targeted microbenchmarks.\nIn this talk we\u2019ll describe the symbiosis between modern CPU architectures and the requirements of columnar data processing. We\u2019ll show how vectorization \u2013 processing many items with a single instruction \u2013 is a widely applicable technique that can provide real performance benefits to all application frameworks that use columnar formats. We\u2019ll present the changes that we have made to Impala\u2019s \u2018scanner,\u2019 which reads Parquet data, and map out even more future enhancements.\nThis talk will be of interest to audiences interested in the internals of big data processing engines, or the impact of recent advances in modern CPU architectures."}, "big-data-conference-ny-2015/public/schedule/detail/45881": {"room": "1 E6 / 1 E7", "title": "Business impact from IoT? Just add data science", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45881", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/162633"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "The Internet of Things (IoT) will forever change the way businesses interact with each other and their customers. In 2020, 25 billion connected \u201cthings\u201d will be in use, reports Gartner. IDC predictions are even higher as analysts estimate IoT will grow from 15 billion devices in 2015 to 30 billion devices in 2020.\nWith the adoption of these devices, effective leveraging of the torrent of data will be critical to driving the transformation of industries. Central to the fundamental shift will be the ability to pool data, and build models that drive real and significant actions.\nFrom smart sensors to connected hospitals, Sarah will use illustrative use cases to demonstrate the fundamental concepts required to drive true impact from these connected devices. She will cover which models are most appropriate for a variety of actions and outcomes, what considerations around data access and processing are critical, and which tools are available to accomplish your task at hand.\nThis session is sponsored by Pivotal"}, "big-data-conference-ny-2015/public/schedule/detail/43413": {"room": "3D 03/10", "title": "Data inclusion for all", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43413", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203744"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "The goal for this session is for everyone to walk away understanding how data inclusion is something we all must work to fix, as it\u2019s not an issue isolated to people with disabilities. I will show how the lack of proper insights hinder everyone (regardless of disability status) from taking quick, appropriate, and intuitive action on any data-driven insight. This session will specifically cover the following topics:\n\nBeing inclusive with data: We will dive head-first by showing some  of the current issues with our data visualizations and tools. I will demonstrate how a person with various disabilities would use the tool, and highlight the biggest blockers. I will then prove that these issues are not something that is isolated to people with disabilities, and motivate everyone to take action to resolve these issues.\n\n\nProper insight creation: Humans lie, a lot. When there\u2019s personal bias put into a data visualization, very little can be trusted.  This portion of the talk will show how we need to restructure our thinking on proper insight generation so that we can react more quickly to all issues."}, "big-data-conference-ny-2015/public/schedule/detail/44262": {"room": "3A & 3B", "title": "Lunch / Thursday BoF Tables", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44262", "topics": "Events", "speaker_urls": [], "timing": "12:00pm\u20131:15pm Thursday, 10/01/2015", "abstract": "Note: Expo Plus pass holders do not have access to Lunch or Lunchtime BoFs.\nBirds of a Feather (BoF) discussions are a great way to informally network with people in similar industries or interested in the same topics.\nBoFs will happen during lunch on Wednesday, September 30 and Thursday, October 1.\n\n\n\n\n\nThis year\u2019s Industry Birds of a Feather discussion topics include:\n\nAdvertising & Marketing\nEnergy\nFinance\nGovernment & Policy\nHealthcare\nMedia & Entertainment\nRetail & eCommerce\nTelecommunications"}, "big-data-conference-ny-2015/public/schedule/detail/45310": {"room": "1 E16 / 1 E17", "title": "Should you trust your money to a robot?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45310", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/213294"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Computers are making more and more decisions for us, and increasingly so in areas that require human judgment.  Google tells us what to search, Amazon recommends what we should buy, and in the not so distant future, driverless cars will take us where we want to go.  Has the time come to trust our money to a robot?\nIn an era of big data and machines to make sense of it all, machines can have an inherent advantage over humans in making financial and investment decisions.   A robot should be considered seriously in situations where there is sufficient data from which it can learn, and the frequency of decisions favors unemotional decision-making over human judgment.\nThis session will contrast humans and machines in several parts of the investment landscape and describe problem areas where machines have already made significant inroads and where they are heading."}, "big-data-conference-ny-2015/public/schedule/detail/43493": {"room": "3D 04/09", "title": "Big data governance", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43493", "topics": "Security & Governance", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/156867", "/big-data-conference-ny-2015/public/schedule/speaker/177995", "/big-data-conference-ny-2015/public/schedule/speaker/204065", "/big-data-conference-ny-2015/public/schedule/speaker/186053"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Moderator Steve Totman of Cloudera and panelists Kristi Cunningham, responsible for big data governance at Capital One; Susan Meyer, business lead, fraud management solutions at Mastercard; Ben Harden, managing director for big data at Captech; and Mark Donsky, navigator product manager at Cloudera, will discuss practicalities and realities of big data governance for business and IT.\nAttend if you have challenges governing big data or Hadoop and want to learn from real world practitioners."}, "big-data-conference-ny-2015/public/schedule/detail/43045": {"room": "3D 04/09", "title": "Process, store, and analyze like a boss with Team Apache: Kafka, Spark, and Cassandra", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43045", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203763"], "timing": "1:30pm\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nWe as an industry are collecting more data every year. IOT, web, and mobile applications send torrents of bits at our data centers that have to be processed and stored. In addition, users expect an always-on experience, with little room for error. Numerous successful companies are doing this every day, and I can show you how.\nIn this tutorial session, we will cover the powerful Team Apache: Apache Kafka, Spark, and Cassandra. You\u2019ll learn how to organize a stream of data into an efficient queue using Apache Kafka. Process the data in flight using Apache Spark Streaming. Store the data in a highly scaling and fault-tolerant database using Apache Cassandra. Transform and find insights in volumes of stored data using Apache Spark. Topics we will discuss:\n\nUnderstanding the right use case\nConsiderations when deploying Apache Kafka\nProcessing streams with Apache Spark Streaming\nDeep dive into how Apache Cassandra stores data\nIntegration between Cassandra and Spark\nData models for Time Series\nPost processing without ETL using Apache Spark on Cassandra\n\nThere is a lot to cover in three hours so get ready and bring your laptop."}, "big-data-conference-ny-2015/public/schedule/detail/45535": {"room": "Javits North", "title": "Haunted by data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45535", "abstract": "Big data is a bit like nuclear energy: while full of promise, it generates residue that is difficult to dispose of, poses risks for those who store it, and leaves the industry one major incident away from scaring the public off the technology entirely.\nThe rise of pervasive commercial surveillance has grown so rapidly that we\u2019ve simply ignored the problem of data disposal.  I hope to persuade you that adopting enforceable limits on what data we\u2019re allowed to keep is not just necessary, but benefits us all.  Users who don\u2019t feel like they are making an irrevocable commitment will be open to sharing much more personal information, and that will create opportunities that would not be possible given the status quo.\nThe alternative to tackling the disposal problem ourselves is seeing regulation imposed by legislative fiat in the wake of a major breach.  Join me in imagining a brighter, more forgetful future.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/213522"], "timing": "10:00am\u201310:20am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/45872": {"room": "1 E6 / 1 E7", "title": "Requirements for secure, multi-tenant Hadoop: It\u2019s much more than YARN", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45872", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217393"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Most Hadoop deployments to date have met the needs of multiple user groups (aka tenants) through a \u201cshared nothing\u201d model, with siloed physical infrastructure that often results in low utilization and high management overhead.\nHowever, as enterprises build out and expand their Hadoop clusters, there is an increasing imperative to provide secure multi-tenant environments with shared infrastructure \u2013 to improve business agility, reduce overhead, and improve utilization.\nThere is a Hadoop reference architecture for multi-tenancy on a single physical cluster based on YARN. But in real-world scenarios, where multiple versions and/or different distributions of Hadoop need to co-exist with strict isolation, this model is a non-starter.  Another challenge with this approach is the fact that complex, time-consuming configurations are required to maintain compute and data isolation between tenants.\nTo meet today\u2019s enterprise requirements, a secure multi-tenant Hadoop architecture must accommodate multiple user groups, multiple concurrent Hadoop jobs, multiple applications, multiple versions and/or distributions of Hadoop, security isolation, and service level guarantees while utilizing shared infrastructure.\nThis session will discuss these requirements and provide recommendations on how to deploy a secure multi-tenant, multi-cluster Hadoop environment.\nThis session is sponsored by BlueData"}, "big-data-conference-ny-2015/public/schedule/detail/45863": {"room": "1 E15", "title": "Apache Spark as a code-free data science workbench", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45863", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217360", "/big-data-conference-ny-2015/public/schedule/speaker/217411"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Few users need convincing that Spark is becoming one of the hottest technologies around in the field of distributed computing. However, great performance and rapidly growing machine-learning libraries do not seem to be enough to attract the majority of the data science community to use it as their big data analytics tool of choice.\nAt DeepSense.io we are trying to close this gap, and complement the computing power of Spark with great UX and usability. By introducing an intuitive graphical user interface carefully designed to visualize various aspects of pipeline-based processing, Spark becomes more user-friendly and appealing to individuals familiar with conventional data science platforms. When accompanied by such features as a shareable model and dataset library, advanced mechanism for statically validating your pipelines, and a method to quickly deploy your models to production, Spark takes a huge step toward being a complete data science toolkit.\n*This session is sponsored by CodiLime Inc. (DeepSense.io)"}, "big-data-conference-ny-2015/public/schedule/detail/45862": {"room": "3D 06/07", "title": "The 10 millisecond rule: Getting to 'Yes' with fast data and Hadoop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45862", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/199134"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "You have 10 milliseconds.  This means at the rate and volume at which data is streaming into your system, you have a small window to collect, explore, analyze, and act before it loses value.\nWhy is this important? The value of understanding data in real time provides a competitive edge in terms of what you can know, how fast, and how quickly you can react to it before it hits Hadoop for historical analytics.\nBut there\u2019s a next step in the process \u2014 combine streaming real-time analytics with operational interaction. This requires an operational element combined with real-time analytics in one system. And all of this still needs to happen in less than 10 milliseconds.\nWhat\u2019s a real-world example? A mobile operator managing user experience with millions of simultaneous users, and doing it on a per-person, per-event basis. Emagine International delivers such a platform; a real-time analytics and campaign management platform that is context-aware and uses predictive analytics to determine the right action, while managing multiple channels of communication based on preference and analytics \u2013 all without breaking the 10 millisecond rule (they do it in 3 milliseconds!).  If you\u2019re a mobile operator, can you afford not to compete by the same rules?\nMobile devices, sensor networks, social media. The physical and digital worlds are coming together creating the \u201cdata-fication of life,\u201d enabling the ability to capture aspects of our world and our daily lives that have never been quantifiable before. For many businesses it\u2019s an untapped opportunity, a potential differentiator, or a competitive threat. It requires smart, context-aware experiences with real-time interaction. Managing fast data is an important part of managing your data architecture if you want to keep relevant and not break the 10 millisecond rule.\nThis session is sponsored by VoltDB"}, "big-data-conference-ny-2015/public/schedule/detail/45902": {"room": "3D 06/07", "title": "Oozie or Easy: Managing Hadoop workflows the EASY way", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45902", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217607"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "According to Gartner Inc., about two-thirds of all organizations are investing in big data and Hadoop but only 8 percent have production applications. Given the universal acceptance that big data is a huge competitive differentiator, it\u2019s likely that the companies that have not yet deployed big data applications are struggling to get their arms around this new technology.\nIndustry experts agree that Hadoop and its ecosystem are extending the enterprise IT fabric, rather replacing any major portions of it. This is particularly evident with workflow management, where Hadoop jobs are growing in volume and complexity and are becoming more intertwined with many existing technologies. For IT organizations that are already overwhelmed by a proliferation of management tools, the ideal solution would be support for Hadoop\u2019s new workloads by the existing tools already in place; and more importantly, that already meet the required standards for governance, ease of use, reporting, planning, and service level management. Such an approach will allow IT to leverage existing staff and mature best practices rather than having to re-invent the wheel yet again specifically for Hadoop.\nThis session is sponsored by BMC Software"}, "big-data-conference-ny-2015/public/schedule/detail/43159": {"room": "3D 02/11", "title": "Modeling predictive maintenance applications in the IoT Era", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43159", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/189058"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "This talk is targeted to data scientists, students, researchers, and non-technical professionals who are interested in data-driven predictive maintenance applications in the IT industry. At the end of the talk, the audience will get a clear picture of the landscape and challenges of predictive maintenance applications, including problem coverage, various applicable predictive models based on data available, and knowledge about what data should be collected.\nPredictive maintenance is a technique to predict when an in-service machine will fail so that maintenance can be planned in advance. In a broader sense, it covers a variety of topics, including but not limited to: failure prediction, failure diagnosis, failure type classification, and recommendation of maintenance actions after failure.\nData-driven predictive maintenance, in particular, is gaining increased attention in the industry along with the emerging demand of Internet of Things applications and the maturity of supporting technologies. With the objective of being able to generate, transmit, store, and analyze big data, these technologies include innovations in hardware (e.g. sensor instruments, memory technologies), network technologies (wireless communication protocols), software architectures (pipeline to process streaming data and/or unstructured data in the cloud), and advanced analytics (e.g. problem formulation, machine learning modeling).\nIn the context of predictive maintenance applications, this talk will focus on the big data analytics aspect of the above mentioned four innovation areas. We review the predictive maintenance problems from two perspectives: from the view of the traditional reliability-centered maintenance field, and from the view of the IoT applications. We\u2019ll emphasize bridging the data-driven approach with the problem-driven approach, by articulating which types of data are requested for different predictive maintenance applications. We strive to bring the attention of IoT industry leaders to the necessary data acquisition required before conducting effective predictive maintenance applications. A real-world example will be discussed by showing how a predictive maintenance problem is formulated into three related questions via different machine learning models.\nThe audience will learn hands-on experience about how to formulate a predictive maintenance problem into three different machine learning models (regression, binary classification, and multi-class classification) through a real-world example. This is illustrated by showing a step-by-step procedure of data input, data preprocessing, data labeling, and feature engineering to prepare the training/testing data from the raw data. Last, we illustrate how different types of learning models can be trained and compared with different algorithms."}, "big-data-conference-ny-2015/public/schedule/detail/43012": {"room": "3D 02/11", "title": "IoT with Spark Streaming: Practical lessons from real-world use cases", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43012", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/202833", "/big-data-conference-ny-2015/public/schedule/speaker/203730"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Over the past year, Spark Streaming has emerged as the leading platform to implement IoT and similar real-time use cases. There are successful implementations across a diverse spectrum of industries: consumer internet and mobile, to healthcare to traditional manufacturing.\nWe will start with a brief introduction to Spark Streaming\u2019s micro-batch architecture for real-time stream processing. However, the primary focus of the talk will be on end-to-end architectures and use cases. We will give a walkthrough, and live demo, of an example use case that includes processing and alerting on-time series data (such as sensor data); all the way from ingestion of the time series data streams with Kafka, processing in Spark Streaming to identify egregious conditions, and sending alerts via Kafka events.\nAlerting and visualization often go together. After all, when something goes wrong, the investigation entails visualizing relevant events and metrics. We will extend our architecture by showing how the time series output of Spark Streaming can be written to HBase or OpenTSDB, so that it can be served to a front end for visualization.\nIn addition to the above use use case, we will highlight some of the high-level operators and libraries available in Spark Streaming that make it easy to implement IoT use cases:\n\nSliding windows to identify faulty sensors, trending items, correlating data from disparate streams\nStateful operators to maintain user session information for personalization\nMllib for easy machine learning on streaming data.\n\nWe will share some pro tips for:\n\nPerformance tuning: checkpointing of stateful data, parallelization of data receivers, recommended data serialization formats and settings, and memory tuning\n\u201cExactly Once Processing\u201d semantics and how to achieve it.\n\nLast, we will describe how to monitor your long-running streaming applications, and highlight some recent and upcoming improvements in monitoring."}, "big-data-conference-ny-2015/public/schedule/detail/44510": {"room": "The High Line/Meatpacking District", "title": "Data After Dark: High Line Hop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44510", "topics": "Events", "speaker_urls": [], "timing": "8:00pm\u201310:30pm Wednesday, 09/30/2015", "abstract": "Note: This event is open to Strata + Hadoop World attendees only \u2014bring your conference badge to get in. Expo Plus pass holders do not have access to this event.\nHome of The High Line, the new Whitney Museum, and countless shops & restaurants, the Meatpacking District has something for everyone, including an incredible nightlife. Mix with your fellow attendees at seven distinct venues, within a few blocks of one another in this vibrant neighborhood.\n\n\n\n\n\n\nData After Dark Sponsored By:\n\n\n\n\n \n\n\n\n\n\n\n\u00a0"}, "big-data-conference-ny-2015/public/schedule/detail/45983": {"room": "3D 03/10", "title": "Session with Paul Shannon", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45983", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204244"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "Paul Shannon, VP of technology, 7digital Group Plc."}, "big-data-conference-ny-2015/public/schedule/detail/45986": {"room": "1 E15", "title": "Faster time to insight using Spark, Tachyon and Zeppelin", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45986", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/163506"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "All of us involved in Big Data are working to decrease time to insights.  Today, we build Spark on Yarn clusters with Hadoop ecosystem components, and there are clear benefits to this implementation.  However, there are other use cases that may benefit from a more streamlined stack.  At the same time, developers and data scientists need to reduce development time through collaboration and closed loop feedback.\nIn this talk, we will discuss how a streamlined Spark stack including Tachyon and Zeppelin, can solve both the need for speed and reduced development time.\nThis session is sponsored by Rackspace"}, "big-data-conference-ny-2015/public/schedule/detail/42977": {"room": "3D 04/09", "title": "Leveraging asset reputation systems to detect and prevent fraud and abuse at LinkedIn", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42977", "topics": "Security & Governance", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203561"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "LinkedIn\u2019s Security Data Science group creates models to detect and prevent abusive and fraudulent behavior on the LinkedIn site. Examples include models for login, registration, scraping, and spam. Data common to all of these models is available, including the IP address and the browser type. These models use scores, or reputations, describing the likelihood of bad behavior coming from the IP, browser type, etc. as input. For example, in our registration model, we want to know if the IP being used to register is likely to be used to create fake accounts.\nWe consider two kinds of reputations: instantaneous reputation, a simple online reputation measured by counters or ratios of counters of recent events (within the past 24 hours); and long-term reputation, that can use more data from a larger range of sources and a longer time scale by being calculated offline. I will discuss the advantages and drawbacks of these two kinds of reputations.\nAs a use case, I will describe in detail the offline IP reputation system. I will discuss the features that go into calculating the IP reputation score and how we combine signals from logged-in and logged-out users. I will then describe how we can predict the IP reputation for future events given the past IP history. I will briefly compare our IP reputation system to other reputation systems we have built, including those for browser types and email domains. Additionally, I will compare it to the instantaneous IP reputations we use in various online models."}, "big-data-conference-ny-2015/public/schedule/detail/45893": {"room": "Javits North", "title": "The rise of the citizen data scientist", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45893", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/123816"], "timing": "9:20am\u20139:25am Thursday, 10/01/2015", "abstract": "Every company is looking for new ways to become more data-driven, yet the traditional BI and analytics tools of the last decade have made it difficult for users to work directly with their data. But with the latest innovations in big data discovery platforms, a new role has emerged: the citizen data scientist. Gartner predicts that this role will grow five times faster than its highly trained counterparts.\nIn this keynote, Ben will share Platfora\u2019s research behind the importance of this emerging role and will dive deeper into the needs of the citizen data scientist so that companies can become truly data-driven.\nThis keynote is sponsored by Platfora"}, "big-data-conference-ny-2015/public/schedule/detail/43571": {"room": "1 E12/ 1 E13", "title": "Migrating workloads from data warehouses to Hadoop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43571", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/143668"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Workloads are migrated to Hadoop for various reasons, ranging from cost saving to its promise of scalability and performance. While SQL capability is now available in Hadoop, the subtle difference in functionality, performance characteristics, and cluster management could make the migration challenging. Compounded with a fast growing data set and real-time response time requirement, it can be overwhelming.\nIn this talk, attendees will learn a systematic approach that addresses these challenges. We have broken down the migration process into steps: identify candidate workload for migration, initial cluster sizing, schema/query design, benchmarking, and production readiness. In each step, we have identified a few key insights to keep in mind in the Hadoop world.\nWe will start with a few common misconceptions, followed by an in-depth discussion of our approach and insights. We will finish the talk with some potential improvements of the whole migration process."}, "big-data-conference-ny-2015/public/schedule/detail/43570": {"room": "1 E8 / 1 E9", "title": "Scaling Python analytics on Impala", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43570", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/193580"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "Data scientists and engineers who create data analytics applications use a wide variety of tools such as Python, R, and Spark. While some frameworks such as Spark or Oracle\u2019s ORE offer scalability, many data science applications are prototyped using Python\u2019s pandas, scikit-learn, or R\u2019s analogous packages, which are comparatively scale-limited. While users reach for Python or R because they offer a familiar and feature-rich environment, the inability to run and validate prototypes at production scales is a major pain point.\nWe are developing a new Python framework, with a familiar user interface for users of pandas and other tools in the Python data science stack. It leverages Impala (a high performance relational query engine for Hadoop) under the hood for execution at scale.\nIn the talk, we will give an overview of existing tools for scalable analytics in Python, R, and other analytics and data science tools, and demonstrate how the new work relates to the rest of the ecosystem. We\u2019ll discuss opportunities for future growth in the project (and others related to it), and ways that the community can get involved to help drive forward scalable open source data analytics."}, "big-data-conference-ny-2015/public/schedule/detail/43245": {"room": "3D 03/10", "title": "Value in the details - understanding data through visual exploration", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43245", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/123601", "/big-data-conference-ny-2015/public/schedule/speaker/182292"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "Analysts and data scientists work with large amounts of data, but the common approach, which dates back 20 years, is to roll up all the data into summary tables or charts, resulting in loss of detail. In contrast, direct visual exploratory analysis of massive amounts of raw data can yield insights that are otherwise overlooked.\nIn financial markets, most participants simply track the price of a security rather than all the underlying activity of buy and sell orders. By plotting all the data, various patterns appear, such as macro-level pricing strategies down to specifics such as consolidation around particular price points. The approach is broadly applicable \u2014 for instance, in social media data over time, macro-patterns such as periodicity may appear and micro-patterns such as popularity are revealed.  Looking beyond the immediate use of exploratory analysis, the approach can be effective for monitoring to detect new, emerging patterns; or in fraud-type applications, to see anomalies.\nThis presentation will highlight high-density visualizations that directly plot hundreds of millions of data points for applications such as market opportunity, periodicity analysis, and anomaly identification."}, "big-data-conference-ny-2015/public/schedule/detail/43575": {"room": "1 E18 / 1 E19", "title": "Real-time analytics with Solr", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43575", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204311"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Search engines have come a long way, from relative obscurity as niche products only used to address full-text search problems, to general purpose full fledged members of the NoSQL gang that developers turn to first to help solve a great variety of problems.\nThis talk will cover how search and Solr have become a critical part of the Hadoop stack, and have also emerged as one of the highest-performing solutions for analytics over big data.\nWe\u2019ll also cover new analytics capabilities in Apache Solr that marry full-text search, faceted search, statistics, and grouping, joining into a powerful engine for powering next-generation big data analytics applications."}, "big-data-conference-ny-2015/public/schedule/detail/43411": {"room": "3D 03/10", "title": "From profiling to analysis: Designing visualization tools for purpose", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43411", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157546", "/big-data-conference-ny-2015/public/schedule/speaker/204025"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "The big data era has lead to the proliferation of new technologies that enable a wider set of users to more effectively analyze and consume data to improve operations and decision making. In large part, this has been brought about by the advancement and popularity of data visualization.\nWe\u2019ve moved beyond traditional dashboard visualizations as the sole method of consuming data, and instead are now incorporating data visualizations into nearly every aspect of the analysis process. With advances in data visualization technology, designers and developers have more flexibility in how visualizations are leveraged within data products. But without a well-defined strategy for the end purpose of the visualization, these products can be ineffective.\nIn this session, Jeffrey Heer of Trifacta and Jock Mackinlay of Tableau will discuss the process for designing data visualizations for distinct stages of the analysis process. The discussion will explore how the personas and considerations of designing data visualizations, to enable fast exploration and profiling of data in native formats, differ greatly from the inputs for designing data visualizations for performing exploratory analysis on clean, structured data downstream. Jeffrey and Jock will incorporate examples from their work in research and academia, as well as examples from their respective industry experience at Trifacta and Tableau."}, "big-data-conference-ny-2015/public/schedule/detail/43530": {"room": "3D 04/09", "title": "How we amplify privilege with supervised machine learning", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43530", "topics": "Law, Ethics, & Open Data", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203745"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "This talk will use the example of sentiment analysis to show that supervised machine learning has the potential to amplify the voices of the most privileged people in society.\nA sentiment analysis algorithm is considered \u2018table stakes\u2019 for any serious text analytics platform in social media, finance, or security. In order for the problem to be tractable and the results to be interpretable, these algorithms reduce the \u2018sentiment\u2019 of a text to a one-dimensional classification (very positive, fairly negative, etc.). As an example of supervised machine learning, I\u2019ll review briefly how these algorithms are trained. I\u2019ll explain this process qualitatively so you develop an intuition for what is going on, but I\u2019ll also show Python code that will give you practical techniques you can apply to your own data.\nThis one-dimensional, supervised approach means that sentiment analysis algorithms fail to measure what they claim to measure, but they don\u2019t measure nothing. Rather they learn to spot unsubtle expressions of extreme emotion. In fact, the words a simple algorithm learns that are the most predictive of sentiment tend to be used by a particularly privileged group of authors: men.\nFrom this specific example, I will develop the ways in which a supervised machine-learning algorithm can embed biases that enhance privilege or are otherwise harmful: from training data, to figures of merit, to feature selection.\nThese issues are morally and legally important to everyone who is in the business of making inferences about people from data."}, "big-data-conference-ny-2015/public/schedule/detail/43121": {"room": "3D 03/10", "title": "Data science for Wall Street", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43121", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/157554", "/big-data-conference-ny-2015/public/schedule/speaker/193680", "/big-data-conference-ny-2015/public/schedule/speaker/156694"], "timing": "9:00am\u201312:30pm Tuesday, 09/29/2015", "abstract": "Description\nOther industries are catching on to what Wall Street has known for years \u2013 the collection of data and application of analytic methods can provide enormous value to enterprises.\nIn this tutorial, attendees will get a taste of how large scale data science techniques and technologies developed for the consumer internet can be applied in the world of finance. Attendees will enrich stock tick data with Wikipedia page view traffic data as well as the text of pages. We will guide an exploration of the relationship between the traffic on Wikipedia pages to the movement of stock prices.\nIn this tutorial attendees will learn how to:\n\nClean and transform data sets in Spark\nJoin together varying data sets (text and time series) by defining conformed dimensions\nUse MLLib and other statistical libraries to build and evaluate models. Examples of model types to be covered include anomaly detection, time series forecasting, and/or textual analysis\nUse Hue to run Spark jobs, and visualize the results."}, "big-data-conference-ny-2015/public/schedule/detail/43124": {"room": "3D 04/09", "title": "Fixing Chicago\u2019s crime data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43124", "topics": "Law, Ethics, & Open Data", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/202714", "/big-data-conference-ny-2015/public/schedule/speaker/155586"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Open government is an incredibly popular topic today. From the appointment to office of the nation\u2019s first chief data scientist, to cities like New York, San Francisco, and Chicago signing executive orders to open up city data to the public, more government data is available to us than ever before. Because of that, one would be right to think we\u2019re in an era of unprecedented transparency.\nYet in late February 2015, investigative journalists revealed that the Chicago police department had been operating \u201cblack sites\u201d around the city \u2014 essentially, places where Americans were detained, and then disappeared off the record. None of this data made it onto the city\u2019s open data networks. This egregious, illegal behavior was not uncovered through open data, but through traditional journalistic methods.\nThis, and other stories like this, fundamentally calls into question the data integrity of open government initiatives. How can we still use this information to derive insights into our government? What can we do to identify omissions in the data? And how can we improve the integrity of open government data through traditional data analysis?"}, "big-data-conference-ny-2015/public/schedule/detail/43484": {"room": "1 E20 / 1 E21", "title": "First-ever scalable, distributed deep learning architecture using Spark and Tachyon", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43484", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/169826", "/big-data-conference-ny-2015/public/schedule/speaker/204061"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Deep learning algorithms have been widely used in many real-world applications, including computer vision, machine translation, and fraud detection. Unfortunately, deep learning only works best when the model is big and trained on large-scale datasets. Meanwhile, distributed computing platforms like Spark are designed to handle big data, and have been used extensively. By having deep learning available on Spark, businesses can fully take advantage of deep learning capabilities on their datasets using their existing Spark infrastructure.\nIn this talk, we present a scalable implementation of predictive deep learning algorithms on Spark, including feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). This, to our best knowledge, is the first successful implementation of CNNs and RNNs on Spark. To support big model training, we use Tachyon as common storage layers between the Spark workers. With its in-memory distributed execution model, Tachyon provides a scalable approach even when the model is too big to be handled on a single machine. Our solution also exploits graphical processing units (GPUs) for matrix computation whenever they are available on worker nodes, further improving execution time.\nThe attendees will learn about deep learning models, the architecture of the system, and how to train and run deep learning models on Spark with Tachyon."}, "big-data-conference-ny-2015/public/schedule/detail/42564": {"room": "1 E20 / 1 E21", "title": "Spark on Mesos", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42564", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/77526"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Apache Spark is an open-source computation platform for big data that supports both batch-mode (offline) data analysis, just like MapReduce, but also processing of event streams, embedded SQL queries, and other extensions.\nWhile Spark is most often discussed as a replacement for MapReduce in Hadoop clusters, Spark is actually agnostic to the underlying infrastructure for clustering, so alternative deployments are possible.\nMesos offers a superset of resource management and scheduling services compared to YARN, making it a viable alternative. The advantages of Mesos include greater flexibility for non-Hadoop, clustered applications, and more fine-grained resource management. The disadvantages of Mesos include the ecosystem of other tools that require Hadoop, which you might need to use.\nWe\u2019ll use several example applications to discuss pragmatic details for Spark on Mesos, including streaming, batch-mode, and interactive application deployment tuning, and integration with databases and distributed file systems. We\u2019ll contrast Mesos vs. YARN performance characteristics, and we\u2019ll describe the Myriad project that integrates YARN with Mesos for a hybrid solution. Finally, we\u2019ll make recommendations on when to use Spark on Mesos, when to use Hadoop instead, and how to be successful in either case."}, "big-data-conference-ny-2015/public/schedule/detail/44169": {"room": "1B 04", "title": "Designing and building big data applications (Day 3)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44169", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/160550"], "timing": "9:00am\u20135:00pm Thursday, 10/01/2015", "abstract": "Take your knowledge to the next level and solve real-world problems with training for Hadoop and the enterprise data hub\nCloudera University\u2019s three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).\nYou will work through the entire process of designing and building solutions, including ingesting data, determining the appropriate file format for storage, processing the stored data, and presenting the results to the end-user in an easy-to-digest form. Go beyond MapReduce to use additional elements of the EDH and develop converged applications that are highly relevant to the business.\nHands-on Hadoop \nThrough instructor-led discussion and interactive, hands-on exercises, participants will navigate the Hadoop ecosystem, learning topics such as:\n\nCreating a data set with Kite SDK\nWriting user-defined functions for Hive and Impala\nTransforming data with Morphlines\nIndexing data with Cloudera Search\nBuilding a Search UI with Hue\n\nAudience and prerequisites\nThis course is best suited to developers, engineers, and architects who want to use Hadoop and related tools to solve real-world problems. Participants should have already attended Cloudera Developer Training for Apache Hadoop or have equivalent practical experience.\nGood knowledge of Java and basic familiarity with Linux are required. Experience with SQL is helpful.\nDeveloper certification\nUpon completion of the course, attendees are encouraged to continue their study and register for a Cloudera certification exam. Certification is a great differentiator; it helps establish you as a leader in the field, providing employers and customers with tangible evidence of your skills and expertise."}, "big-data-conference-ny-2015/public/schedule/detail/44168": {"room": "1B 04", "title": "Designing and building big data applications (Day 2)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44168", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/160550"], "timing": "9:00am\u20135:00pm Wednesday, 09/30/2015", "abstract": "Take your knowledge to the next level and solve real-world problems with training for Hadoop and the enterprise data hub\nCloudera University\u2019s three-day course for designing and building big data applications prepares you to analyze and solve real-world problems using Apache Hadoop and associated tools in the enterprise data hub (EDH).\nYou will work through the entire process of designing and building solutions, including ingesting data, determining the appropriate file format for storage, processing the stored data, and presenting the results to the end-user in an easy-to-digest form. Go beyond MapReduce to use additional elements of the EDH and develop converged applications that are highly relevant to the business.\nHands-on Hadoop \nThrough instructor-led discussion and interactive, hands-on exercises, participants will navigate the Hadoop ecosystem, learning topics such as:\n\nCreating a data set with Kite SDK\nWriting user-defined functions for Hive and Impala\nTransforming data with Morphlines\nIndexing data with Cloudera Search\nBuilding a Search UI with Hue\n\nAudience and prerequisites\nThis course is best suited to developers, engineers, and architects who want to use Hadoop and related tools to solve real-world problems. Participants should have already attended Cloudera Developer Training for Apache Hadoop or have equivalent practical experience.\nGood knowledge of Java and basic familiarity with Linux are required. Experience with SQL is helpful.\nDeveloper certification\nUpon completion of the course, attendees are encouraged to continue their study and register for a Cloudera certification exam. Certification is a great differentiator; it helps establish you as a leader in the field, providing employers and customers with tangible evidence of your skills and expertise."}, "big-data-conference-ny-2015/public/schedule/detail/46048": {"room": "1 E15", "title": "Roll your own big data analytics in the cloud without reinventing the wheel", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46048", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/135162"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "Big data analytics has been proven to improve profitability and spur innovation for the enterprise. However, the real-world deployment of big data analytics solutions at scale is still slow.\nIntel, teamed up with ecosystem partners, has introduced an open source analytic platform-as-a-service software stack for data scientists and app developers to build and deploy domain-specific advanced analytics applications at cloud scale. The platform is designed to accelerate the development of cloud-native applications driven by big data analytics, simplify their deployment on-premises and in public clouds, and deliver hardware-enhanced performance and security for analytic workloads.\nJoin us and discover use cases in various industries and learn how you can build your own solutions with this open source trusted analytic platform.\nThis session is sponsored by Intel"}, "big-data-conference-ny-2015/public/schedule/detail/45916": {"room": "1 E12/ 1 E13", "title": "Data and music: How India\u2019s music streaming service uses big data to address a 1 billion user market", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45916", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/221608", "/big-data-conference-ny-2015/public/schedule/speaker/217838"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Saavn believes that data is key to transform the music industry. Drawing insights from data helps drive not only our product but also makes music streaming more targeted and personalized. This talk will focus on how we at Saavn are using various big data technologies to become the one stop destination for all kinds of music needs.\nWe will touch upon specific roles Hadoop, Kafka/Storm, and Neo4j have played in our infrastructure. We will also speak about how we have leveraged big data to measure user analytics, to make real time user cohorts and to build a unique recommendation platform.\nAt Saavn, both real time and batch processing systems work in tandem to build a hybrid, scalable and extendible platform. The aforementioned data infrastructure is powering Saavn to drive engagement, retention and ad targeting. This talk will cover:\n\nHadoop\u2019s role in measuring long term user analytics\nKafka/Storm\u2019s role in driving user growth and building real time cohorts\nDesign of a recommendation engine focussed on Indian music"}, "big-data-conference-ny-2015/public/schedule/detail/46162": {"room": "South Concourse", "title": "Ice Cream Social", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46162", "topics": "Events", "speaker_urls": [], "timing": "5:15pm\u20136:15pm Thursday, 10/01/2015", "abstract": "Join attendees, speakers, and exhibitors as we end the conference on a sweet note with some ice cream in the South Concourse."}, "big-data-conference-ny-2015/public/schedule/detail/42961": {"room": "1 E10 / 1 E11", "title": "Goldman Sachs data lake", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42961", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203669"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "Goldman Sachs is a leading global investment banking, securities, and investment management firm that provides a wide range of financial services. Goldman executes hundreds of millions of financial transactions per day, across nearly every market in the world. In this presentation, we will describe how we manage this scale of data as an enterprise asset.\nThe Business Problem\nLarge organizations, such as Goldman Sachs, create very large amounts of data.  Traditionally, most of this data is managed in functional silos, making it difficult and expensive to discover, query, and analyze data across these silos.  Overlapping and sometimes redundant repositories, differences in meaning of data, and lack of transparency on ownership present challenges, inefficiencies, and inconsistencies.  In recent years it has become clear that data must be managed as a corporate strategic asset. How do we know what data is available? How trustworthy is it?  How do you deal with different meanings resulting from traditional silo-based data management?\nIn this session we will share how Goldman Sachs is tackling this problem by developing an enterprise data lake platform to unify and manage data across the firm, enabling data to be discovered and used consistently and reliably for authorized users and use cases.\nWhat is the Goldman Sachs Data Lake?\nThe Data Lake is a data-centric ecosystem that will allow transactional, operational, and reference data to be:\n\nRegistered, ingested, validated, stored and archived in its native form\nSecured and entitled to authorized users\nModeled and made query-able using a unified query service\nCleansed, enriched, transformed, and analyzed through hosted compute engines\nManaged as a first class asset with transparency on ownership, lineage, and provenance\n\nThe GS Data Lake is being built to store ALL the data in the firm in one accessible place, where it can be rapidly analyzed for business purposes using a hosted query service.   This Data Lake will be used for near time OLAP, and GS is investigating using it for OLTP/streaming.\nWho will use the Goldman Sachs Data Lake?\nThere are three main actors in the data lake.\n\nProducers \u2013 responsible to register and publish their data to the data lake and ensure it meets data validation standards and SLA\nRefiners \u2013 will cleanse, enrich, and transform the data and re-publish the curated version to the data lake\nConsumers \u2013 will browse available data and run reports, queries, and analytics\n\nWhat technology are we using?\nGS is building this infrastructure using open source components such as Hadoop, Spark, and Hive as well as commercial offerings and custom-developed software.\nIn this talk we will describe the technical architecture of the solution in detail.  We will cover the different services provided by the Data Lake that enable the management of data and metadata through a lifecycle of data being published, refined, enriched, and ultimately consumed.  We will also cover some of the design patterns that help us scale the platform, namely using SPARK for the ingestion pipeline and the separation of storage from compute."}, "big-data-conference-ny-2015/public/schedule/detail/42498": {"room": "1 E18 / 1 E19", "title": "Big data at Netflix: Faster and easier", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42498", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/138686"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "The Netflix Data Platform is a constantly evolving, large-scale infrastructure running in the (AWS) cloud. This talk will dive into what we\u2019re up to and why. We are especially focused on performance and ease of use. We\u2019ve upgraded to Hadoop 2, have partnered with the community developing Pig on Tez, have adopted the Parquet file format, and fully integrated Presto into our stack. We are exploring Spark for streaming, machine learning, and analytic use cases.\nWe continue to add to our big data, open source suite, with our latest contribution Inviso (which provides easy searching and visibility into Hadoop execution and performance). We are also heads-down developing a cohesive framework for easy platform interaction (via our big data API and big data portal). We\u2019ll talk through these technologies and how they are benefiting the Netflix business. We\u2019ll also dive into how we do things differently at Netflix (vs. most other companies), notably the motivations behind our architecture/ approach and the benefits that we (and hopefully you can) achieve."}, "big-data-conference-ny-2015/public/schedule/detail/45875": {"room": "1 E14", "title": "Machine learning in big data \u2013 look forward or be left behind", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45875", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217406"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Applying machine learning to big data is something many strive for and few achieve \u2013 yet.\nCreating models to predict customer response or to segment customer data into set categories are \u201cpredictable\u201d use cases.  Taking data, discovering what it can tell you, and creating a model and a use for it sound simple enough.  It\u2019s a start, but not enough to impact sustainable revenue or cost advantage for your enterprise.\nThis session will cover the mission-critical questions related to model choice, viability horizon, practical design alternatives, learning from on-the-fence model factors, and opportunities for automating access to changing data and netting-out error and noise.\nBill Porto, senior analytics engineer at RedPoint Global, will discuss why continual, adaptive optimization is the key to maintaining a leadership position in satisfying customer demand.  He\u2019ll explain in detail the applicability of machine learning tools with pros/cons for each approach, and discuss how these processes should and can be optimized to predict, segment, and ultimately drive more predictable outcomes from business decisions.\nApproaches for populating and tuning your models will also be explored. Through real-world examples and customer use cases, you will learn how to apply predictive modeling and optimization to harness the full power and potential of your data.\nThis session is sponsored by RedPoint Global, Inc."}, "big-data-conference-ny-2015/public/schedule/detail/45850": {"room": "1 E15", "title": "Do you know where your data is?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45850", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/190289"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Enterprises find it far too costly and time-consuming to find the data needed to support their analyses. These data assets quickly become fragmented across silos, leaving most organizations without a useful way to discover and organize it \u2013 limiting any real return on their big data investments.\nTamr Catalog solves this by creating a vendor-agnostic inventory of all enterprise metadata in a collaborative, platform-neutral web app.\nThrough a combination of machine learning and human guidance, the metadata can be organized logically, according to what it represents rather than just where it\u2019s stored \u2013 making it easier for people to find what they need to answer critical business questions. Tamr\u2019s goal is to make metadata and the people associated with it accessible and actionable by everyone, not just IT people \u2013 even as metadata volume and variety scale.\nFor example, say you\u2019re a marketing analyst looking to gain a 360-degree view of your customers \u2013 but your enterprise has customer data spread across multiple CRM, financial, and service record databases. Tamr Catalog would enable you to easily find all the data and attributes related to \u2018customers\u2019 without having to resort to one-off emails to individual data managers or spreadsheets of complicated queries.\nWith Tamr Catalog, now anyone can easily find and use the data they need to answer mission-critical business questions \u2013 not just the data that\u2019s most familiar, the closest or the most convenient.\nThis session is sponsored by Tamr Inc."}, "big-data-conference-ny-2015/public/schedule/detail/42968": {"room": "3D 04/09", "title": "Data democratization versus data governance", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42968", "topics": "Security & Governance", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/138584"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Combining data in Hadoop for the purpose of data discovery often runs into barriers from the security group because of legal or corporate policies. Many data governance efforts attempt to replicate existing policies, or relegate a big data pilot to just one small portion of the available data, to address these concerns.\nYet the power in big data systems is often the ability for an enterprise to discover useful data for analysis; and democratizing access to that data often translates into a larger ROI of these systems. Organizations need to think differently about how to secure data while still giving broad access within big data systems.\nThis talk will discuss the challenges with implementing data governance in big data systems, and a design pattern for addressing those challenges within an organization, through a recent case study."}, "big-data-conference-ny-2015/public/schedule/detail/45854": {"room": "1 E6 / 1 E7", "title": "Big data modeling and analytic patterns \u2013 beyond schema on read", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45854", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217331"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Hadoop and NoSQL systems allow for new ways of modeling data, notably arrays of data (e.g., sessionization) and JSON-style complex structures. They also allow for a variety of physical optimizations, some new (bucketing, range scans), and others not (partitions, columnar storage, indexes).  The range of access patterns continues to expand for these systems, from batch view creation real-time analytic queries, to ad hoc analysis.\nIn this session, we look at patterns and practices for modeling and organizing data in big data systems. We examine anti-patterns like hotspotting and typical causes of inefficiency, along with patterns and best practices like sessionization, aggregation, denormalized views, and slowly changing dimensions. We also present an approach to agile data modeling, incrementally parsing and shredding columns; and discuss when to add to tables, when to create logical views, and when to materialize views.\nWe present examples of problems like keeping current snapshots of changing databases in sync while keeping change records, and joining transactions to user profiles and structures to allow for both real-time status and historical analysis of related (joined) streams. We look at important access patterns (key lookup, ad hoc query, aggregate access, model training) and how they impact physical models.\nThis session is sponsored by Teradata"}, "big-data-conference-ny-2015/public/schedule/detail/44574": {"room": "Hudson River Park", "title": "Data Dash", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44574", "topics": "Events", "speaker_urls": [], "timing": "6:30am\u20137:30am Wednesday, 09/30/2015", "abstract": "Calling all Hadoop enthusiasts!\n\n\n\n\n\nWHAT\nJoin Cloudera and O\u2019Reilly Media for the Data Dash run / walk, held in conjunction with Strata + Hadoop World in New York. Register free for the Data Dash\nGIVING\nCloudera and O\u2019Reilly have partnered with Make-A-Wish\u00ae Metro New York as the beneficiary of this year\u2019s Data Dash New York.\nWe ask all Data Dash registrants to consider making a donation (suggested $25) using the following link: http://site.wish.org/goto/datadash\n\n\n\n100% will benefit Make-A-Wish\u00ae\nWHERE\nWe will meet Hudson River Park Pier 84, 555 12th Ave New York, NY 10036 at 6:30 a.m. The start marker is an 8\u2019 checkered flag.\nWHY\nMeet fellow Hadoop enthusiasts, find a new pal to run with and enjoy the fresh air!  Finish at the same location around 7:30 and receive a swag bag including a $5.00 coffee gift card \u2014 it is early, after all!\nROUTE\nTBD\nWe look forward to seeing you there!"}, "big-data-conference-ny-2015/public/schedule/detail/45906": {"room": "3D 06/07", "title": "Putting Modern BI to Work: Innovative Use Cases", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45906", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217626"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "With the modernization of BI, companies across many industries are quickly moving beyond  traditional IT-dependent BI tools and static dashboards toward fast-cycle insights and actions to drive better decisions. In this session, you will learn why pioneering companies like Merck and others are embarking on a mission to understand the \u201cnow\u201d of their businesses, what they are doing with their internal and external data to drive continuous insights, and how their businesses benefit from these insights. Come experience how Spark-based, modern BI solutions help businesses reach insights faster by accelerating the data-to-insights pipeline.\nThis session is sponsored by ClearStory Data"}, "big-data-conference-ny-2015/public/schedule/detail/42723": {"room": "1 E18 / 1 E19", "title": "Data liberation and data integration with Kafka", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42723", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/177568"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "Apache Kafka is a popular open source message broker for high-throughput real-time event data, such as user activity logs or IoT sensor data. It originated at LinkedIn, where it reliably handles around a trillion messages per day.\nWhat is less widely known: Kafka is also well suited for extracting data from existing databases, and making it available for analysis or for building data products. Unlike slow batch-oriented ETL, Kafka can make database data available to consumers in real time, while also allowing efficient archiving to HDFS, for use in Spark, Hadoop, or data warehouses.\nWhen data science and product teams can process operational data in real time, and combine it with user activity logs or sensor data, it is a potent mixture. Having all the data centrally available in a stream data platform is an exciting enabler for data-driven innovation.\nIn this talk, we will discuss what a Kafka-based stream data platform looks like, and how it is useful:\n\nExamples of the kinds of problems you can solve with Kafka\nExtracting real-time data feeds from databases, and sending them to Kafka\nUsing Avro for schema management and future-proofing your data\nDesigning your data pipelines to be resilient, but also flexible and amenable to change."}, "big-data-conference-ny-2015/public/schedule/detail/42780": {"room": "Javits North", "title": "Data vs Creativity: The last battleground?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42780", "abstract": "With self-driving cars and delivery drones becoming a reality, it\u2019s clear that data is on a relentless march to conquer industries one-by-one. But some industries have been largely successful in fending off the attack: despite shiny case studies of data use that act as decoy flares, in creative businesses like music, books, and television the most fundamental decisions are still made using gut instinct (or \u2018skills and judgment\u2019). Are creative businesses the last battleground for data-driven decision making? Drawing lessons from successes and failures in the music industry, book publishing, and TV, David Boyle will argue for a negotiated settlement in the war between data and creative, and show how long-term and mutually beneficial peace can work.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/137683"], "timing": "9:55am\u201310:05am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43542": {"room": "1 E20 / 1 E21", "title": "What's new in Spark Streaming - a technical overview", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43542", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/152590"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "As the adoption of Spark Streaming in the industry is increasing, so is the community\u2019s demand for more features. Since the beginning of this year, we have made significant improvements in performance, usability, and semantic guarantees. In particular, some of these features are:\n\nNew Kafka integration for exactly-once guarantees\nImproved Kinesis integration for stronger guarantees\nAddition of more sources to the Python API\nSignificantly improved UI for greater monitoring and debuggability.\n\nIn this talk, I am going to discuss these improvements as well as the plethora of features we plan to add in the near future."}, "big-data-conference-ny-2015/public/schedule/detail/46154": {"room": "1 E14", "title": "SAP HANA Vora to query Big Data with greater ease", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46154", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/219758"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "SAP continues to work to improve the speed and ease of querying data held in Hadoop and the ease of combining this data with operational data held in the enterprise. Three years after introducing its first integration of Hadoop in SAP HANA, working with technologies such as the Apache Spark framework, SAP is now delivering a new experience to solve the day-to-day challenges of querying Big Data by enabling OLAP-style analytics and interactive SQL on Hadoop data. Come learn how SAP HANA Vora helps business users and data scientists gain contextual insight from data across the enterprise, including data lakes. Join us to learn about how SAP HANA Vora  can be used as a stand-alone or in concert with SAP HANA platform to extend enterprise-grade analytics to Hadoop clusters and provide enriched, interactive analytics on Hadoop.\nThis session is sponsored by SAP"}, "big-data-conference-ny-2015/public/schedule/detail/42729": {"room": "1 E10 / 1 E11", "title": "How women are conquering the S&P 500", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42729", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/202698"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "According to Credit Suisse\u2019s Gender 3000 report, at the end of 2013, women accounted for 12.9% of top management in 3000 companies across 40 countries. Additionally, since 2009, companies with women comprising 25-50% of their management team returned 22-29% more than those without women.\n\nIf companies with women in management outperform so dramatically, what would happen if you invested in women-led companies?*\n\nKaren Rubin has spent the last nine months exploring this question. In doing so, she developed an investment algorithm that invests in the women-led companies of the Fortune 1000. Based on a simulation run from 2002-2014, this investment algorithm would have returned 340%, or 217% more than the S&P500.\nIn this talk, Karen will walk through the process she went through to develop and validate the strategy. She will cover how the algorithm decides to buy and sell stock, how the backtest works, and how she has validated the results of the simulation."}, "big-data-conference-ny-2015/public/schedule/detail/45352": {"room": "3D 05/08", "title": "Ask me anything: Developing a modern enterprise data strategy", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45352", "topics": "Ask Me Anything", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/105756", "/big-data-conference-ny-2015/public/schedule/speaker/27740", "/big-data-conference-ny-2015/public/schedule/speaker/172046"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Big data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technical solutions? Fundamentally, data should serve the strategic imperatives of a business\u2014those key strategic aspirations that define the future vision for an organization. A data strategy should guide your organization in two key areas\u2014which actions your business should take to get started with data, and where to start to realize the most value."}, "big-data-conference-ny-2015/public/schedule/detail/43435": {"room": "3D 05/08", "title": "The glue: Building the connectors and tools to manage big data warehouses", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43435", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203462", "/big-data-conference-ny-2015/public/schedule/speaker/174489"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "For most companies, data analysis means collecting the data, building a data pipeline to clean and transform the data into a usable form, and only then looking for insights. Without good tools to automate the data pipeline, data flow management can become a tedious and brittle process.\nIn this talk we highlight some useful tools that we built in-house:\n\nScheduling of nightly jobs and anomaly detection to catch errors in the data or in the data transformation code\nA backfill tool to retroactively update historical data to reflect new changes in the code\nA dependency management tool to capture dependency restrictions in the data pipeline and schedule jobs to run in the optimal order\nA data versioning tool that \u201cremembers\u201d which query generated the data. Without this, there is no way to tell apart data generated from two different versions of the code that may have very different logic, leading to faulty conclusions"}, "big-data-conference-ny-2015/public/schedule/detail/44509": {"room": "3E", "title": "Opening Reception", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44509", "topics": "Events", "speaker_urls": [], "timing": "5:00pm\u20136:30pm Tuesday, 09/29/2015", "abstract": "Join us at the Opening Reception for a drink or two. Network with other attendees while visiting our companies innovating in the data space. This event is open to all sponsors, exhibitors, and attendees."}, "big-data-conference-ny-2015/public/schedule/detail/46002": {"room": "1 E14", "title": "Big Data Analytics in the Cloud", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46002", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/138899"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "At Microsoft, we work with thousands of enterprises every day, and leverage exabytes of data to run our own businesses such as Windows, Bing, and XBox.  In this talk, we\u2019ll showcase key customer use cases that show how we have overcome common obstacles in big data adoption such as the high learning curve, cost of implementation, challenges in tuning infrastructure, and providing enterprise grade security.  Come learn how you can leverage the rich analytic services in Azure to transform your data easily at any scale with the tools you already know. This talk will introduce new capabilities to process data at any scale using familiar tools.  You will also learn how to compose tools such as Spark and R or do scale-out querying on demand, at massive scale with no hardware to deploy, software to tune and configure, or infrastructure to manage.\nThis session is sponsored by Microsoft"}, "big-data-conference-ny-2015/public/schedule/detail/45329": {"room": "3D 05/08", "title": "Ask me anything: Apache Spark", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45329", "topics": "Ask Me Anything", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/152591", "/big-data-conference-ny-2015/public/schedule/speaker/147848"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Join the Spark team for an informal question and answer session. Spark committers from Databricks will be on hand to field a wide range of detailed questions. Even if you don\u2019t have a specific question, join in to hear what others are asking."}, "big-data-conference-ny-2015/public/schedule/detail/43666": {"room": "1 E10 / 1 E11", "title": "Death of the click: How big data is killing your favorite metrics", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43666", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/103739"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "Description\nData driving decisioning is not an invention of the recent past. We have a waste collection of metrics, KPIs, and other statistics that help decision makers to be more data driven. While the recent technological advances have increased our ability to see the world through the objective lenses of data, one unintended side effect of exactly this technology is the diminishing value of many metrics we cherish.\nConsider the clickthrough rate (CTR) in advertising. With the rise of Doubleclick, CTR has been a core metric of an advertiser\u2019s ability to identify a relevant audience: more clicks equals more interest equals more qualified potential consumers. While this might have been true in the past, the sad reality is that measures like CTR have lost their meaning.\nThe issue is not as much adversarial attempts of gaming, but the fact that even highly correlated proxies like CTR lose their power once optimization can draw from more granular information. Given much more detailed information, modern optimization approaches can find signals in the noise, and identify users with poor vision or bad fine motor skills who accidentally (but not randomly) click on ads, rather than interested potential consumers. This fundamental issue extends beyond clicks and calls for a close examination of many of our favorite KPIs."}, "big-data-conference-ny-2015/public/schedule/detail/45371": {"room": "Javits North", "title": "The Next Generation", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45371", "abstract": "Mike Olson, CSO and Chairman, Cloudera", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/5259"], "timing": "8:55am\u20139:10am Wednesday, 09/30/2015"}, "big-data-conference-ny-2015/public/schedule/detail/44623": {"room": "Hall B", "title": "Cultivate: Leading Through Culture", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44623", "topics": "Cultivate", "speaker_urls": [], "timing": "9:00am\u20135:00pm Monday, 09/28/2015", "abstract": "\u201cCommand and control\u201d no longer works. Systems, speed, and culture do.\nWe\u2019re at the cusp of a new network age. The companies defining it are fast, flat, and flexible. They devour data and focus obsessively on their customers. \u201cAnalyze and adapt\u201d is their Standing Operating Procedure. At Cultivate, experts from these leading companies will tell you how they do it\u2014and how you can, too.\nJoin us for Cultivate, a two-day event taking place during Strata + Hadoop World New York.\nYour business is facing perpetual change\u2014every business is. And that means everyone responsible for its success\u2014from team leaders up to CXOs\u2014must shift to a new way of managing. Here\u2019s what you\u2019re up against in today\u2019s business environment:\n\nProduct development cycles\u2013for both online and physical products\u2013are continually contracting, as durable goods with software at the core can be modified nearly as easily as a website.\n\n\nEvery company needs to navigate a deluge of data\u2014not to confirm the opinion of the highest-paid person in the room, but to spark intelligent discussion of strategies and options among employees, regardless of their position.\n\n\nYour customers no longer simply want things to work; they want products that please and delight them. Creating a great user experience is no longer just \u201cnice;\u201d it\u2019s essential.\n\nSimply put, organizations need to embrace continual change, leverage copious data, and adapt to a marketplace in which customers\u2019 needs and desires truly come first. Those that can will survive; those that can\u2019t face failure.\nThe kind of leadership needed for these nimble, responsive organizations isn\u2019t taught in traditional management programs. It\u2019s been worked out in practice by the engineers, designers, programmers, and data scientists who are living it. They have created companies that activate leadership at every level, from small, task-driven working groups to the board room.\nThis is core of Cultivate: growing talent for leadership within organizations, not from managers assigned to lead engineers or designers, but by identifying and encouraging designers and engineers with the ability to lead.\nA crucial element of this new business model is an organization\u2019s culture. At Cultivate, we\u2019ll explore the values, practices, and structures that enable organizations to respond with agility to changes in their products and the marketplace.\nIf you need to learn about growing teams within your organization; training the leadership it needs now and in the future; and creating processes that work (rather than work against you), you need to be at Cultivate. Learn from the leaders who have been through this before; meet others who are also struggling to build their businesses; and work with us to cultivate the leadership and organizational understanding we\u2019ll need for the next century."}, "big-data-conference-ny-2015/public/schedule/detail/45884": {"room": "1 E6 / 1 E7", "title": "The economics and psychology of Big Data data centers and information management", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45884", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/184704", "/big-data-conference-ny-2015/public/schedule/speaker/165644"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "As Big Data becomes a pervasive force in the enterprise, many of our fundamental ideas around how to optimize compute, storage, network and resource management are being stretched.\nJoin Cisco Consulting Systems Engineer Robert Novak and Paxata\u2019s co-founder and VP of Products Nenshad Bardoliwalla for a discussion about the power of unification across the software and hardware layers and how this is completely up-ending the price-to-performance ratio of enterprise big data computing.\nIn this thought-provoking session, we will examine the new architecture that represents the hyper-convergence of software and hardware for Big Data along multiple dimensions:\n\nHow to design big data software for true elasticity for interactive and batch performance combined with dynamic centralized hardware management through service profiles\nHow to combine a state of the art computer, storage, and networking backbone with an enterprise information fabric which optimizes for big data\nWhy the promise of drastically lowering the cost-to-compute big data won\u2019t be fulfilled using commodity hardware and open source technologies without high-value vendor extensions and integration.\n\nThis session is sponsored by Cisco"}, "big-data-conference-ny-2015/public/schedule/detail/42896": {"room": "1 E12/ 1 E13", "title": "Transitioning from reactive to proactive: Etsy's data platform team", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42896", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/160824"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "In September 2014, I became the manager of the team then known as Etsydoop. A team of four, we had just started to get on top of all our urgent problems, and I got to be part of the exciting job of deciding what to do with our new-found breathing room.\nTogether we agreed that we wanted to do less maintenance work, more building. We wanted to be involved in the planning process of projects, so we could help teams use the right tool for their data jobs, not just whatever they\u2019d heard of. We wanted to change our reputation from a team that always said No to being a team that you\u2019d want to approach for help.\nI\u2019m going to tell stories about:\n\nHow we do outreach to the Engineering org\nCreating dashboards that answer common questions\nHow we changed the way we work with Operations and Data Engineering\nSetting up processes for evaluating and comparing new tools\nWorking with teams early in their planning process to find out their needs\nInviting teams to speak to us about their data needs."}, "big-data-conference-ny-2015/public/schedule/detail/44632": {"room": "3D 02/11", "title": "Twitter Heron: Stream processing at scale", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44632", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/208670"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "Storm has long served as the main platform for real-time analytics at Twitter. However, as the scale of data being processed in real time at Twitter has increased, along with an increase in the diversity and the number of use cases, many limitations of Storm have become apparent. We needed a system that scaled better, had better debug-ability, better performance, and was easier to manage \u2013 all while working in a shared cluster infrastructure.\nWe considered various alternatives to meet these needs, and in the end concluded that we needed to build a new real-time stream data processing system. This talk will present the design and implementation of a new system, called Heron, that is now the de facto stream data processing engine inside Twitter. Share our experiences in running Heron in production."}, "big-data-conference-ny-2015/public/schedule/detail/43036": {"room": "1 E12/ 1 E13", "title": "The data-driven future of biotechnology", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43036", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/106962"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "Synthetic biology is an exploding new industry. Technology has progressed so that we can turn microbes into manufacturing facilities, converting renewable resources such as sugar into complex chemicals for a variety of applications. However, cellular biology is still incredibly complex, making it difficult to engineer microbes that produce the chemicals we need with reasonable efficiency.\nZymergen\u2019s software infrastructure provides microbiologists with a powerful platform for parallel genome design, and couples this with workflow systems that guide our factory\u2019s production. This fuels our lab\u2019s capability to coordinate the many steps required in constructing thousands of genomes in parallel: a quantum leap in our ability to quickly construct and test microbial genomes. This work is performed in a state-of-the-art, high-throughput biotech wet lab consisting entirely of robotic systems.\nOur data collection framework feeds assay data into machine learning-guided QC and result analyses that direct factory operations and flow into the next generation of genome design.\nNoSQL technology enables us to generate, record, and analyze genomic possibilities at ever-increasing scale, while advances in automation makes high-throughput genome editing and testing feasible. To build microbes that produce chemicals at industrial scale, we must log every operation and component used in the lab, while machine learning guides future genomic exploration. To power this we must provide a feature engineering platform that can track diverse phenotype (trait) information associated with genomic designs, as well as many other fine-grained characteristics of our assembly processes.\nIn marrying robot automation with this data-driven genome design workflow, we have fundamentally changed how we design genetic experiments: not as testing specific changes to a genome, but as specifying whole classes of genomic changes, which are applied systematically, aggregated, and optimized.\nIn this talk we will describe some of the challenges and promises of synthetic biology, introduce the basics of genome engineering, and describe how Hadoop ecosystem tools can fundamentally alter how we approach scientific experimentation. In particular, we will present details of the design of our system and describe how we are solving this complex problem using open source software for cloud computing, complex data storage, search, workflow execution, and bioinformatics."}, "big-data-conference-ny-2015/public/schedule/detail/45360": {"room": "Javits North", "title": "Privacy protection and reproducible research", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45360", "abstract": "It is easy to make \u201cfalse discoveries\u201d when analyzing big data.  It is harder to draw causal conclusions that are reliable and reproducible, especially when private or proprietary information is involved.  Recent mathematical ideas, like differential privacy, offer new ways of reaching robust conclusions while provably protecting personal information.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/125941"], "timing": "9:00am\u20139:10am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/45811": {"room": "Javits North", "title": "The Race to Modernize BI: What It Is and Why So Urgent?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45811", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/216644"], "timing": "9:35am\u20139:40am Wednesday, 09/30/2015", "abstract": "Traditional BI is getting an overhaul at a rapid pace. \u201cBI 2.0\u201d was predicted to hit the mainstream by 2017, but business leaders view the ability to see \u201cwhat\u2019s happening now\u201d and make better decisions as an urgent need to differentiate and win. 89 percent of business leaders view data analysis to be as transformative as the Internet. Three factors are fueling its early arrival: 1) Rapid innovation through Spark, machine learning, cloud technology, and other advances; 2) A data landscape that\u2019s growing larger, more diverse and complex; 3) Accelerating business needs that are driving decision times down and the impact of data insights up, directly for the mainstream business user. This keynote unveils why rapid modernization is taking place , what it looks like and the urgent business use cases driving it. The modernization of BI moves businesses beyond IT-dependency, traditional ETL and static dashboards toward interactive answers that use up-to-the-minute data so companies can compete, win, and thrive."}, "big-data-conference-ny-2015/public/schedule/detail/42957": {"room": "1 E16 / 1 E17", "title": "Native erasure coding support inside HDFS", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42957", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/199375", "/big-data-conference-ny-2015/public/schedule/speaker/203661"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "The current HDFS replication mechanism is expensive: the default triplication scheme has 200% overhead in storage space and other resources (e.g., NameNode memory usage). Erasure coding (EC) can greatly reduce the storage overhead without sacrificing data reliability. The HDFS-EC project (HDFS-7285) aims to build native EC support inside HDFS.\nBy treating EC as \u201cfirst class citizen\u201d instead of an external layer (as in the HDFS-RAID project), HDFS-EC brings about several significant benefits. First, it enables flexible and fine-grained EC policies. For large files, EC can be applied on the existing contiguous block layout, preserving data locality and facilitating efficient conversion to and from replication. Small files can also enjoy the benefits of EC by using the striping layout introduced as part of HDFS-EC. The striping layout also enables the client to work with multiple data nodes in parallel, greatly enhancing the aggregate throughput.\nPreliminary analysis of several production clusters shows that HDFS-EC can reduce the storage overhead from 200% to 50% on average. To allow the adjustment between storage overhead and data reliability, HDFS-EC supports configurable and pluggable erasure codec algorithms and schemas through a unified framework, where different native libraries can be employed to implement a concrete erasure coder. This is critical to alleviate the performance impact involved by EC on both the client and data node. Benchmark tests indicate that by using the Intel ISA-L library, we can eliminate the CPU bottleneck and achieve 1~3x higher performance compared against other implementations (and >20X speedup compared to an original HDFS-RAID implementation)."}, "big-data-conference-ny-2015/public/schedule/detail/45849": {"room": "3D 06/07", "title": "Simplify Big Data with Platform, Discovery and Data Preparation from the Cloud", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45849", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/195264", "/big-data-conference-ny-2015/public/schedule/speaker/219002"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "Most organizations are aware of Big Data\u2019s raw power and scale by now, but few have figured out how to extract value from their Big Data investments in an agile and repeatable fashion.  Oracle sees a tremendous opportunity to provide thought leadership in the space, by combining significant R&D investments in Big Data technologies and the Oracle Cloud.  This session will highlight two exciting new products: Oracle Big Data Discovery Cloud Service and Oracle Big Data Preparation Cloud Service.\nOracle Big Data Discovery Cloud Service is the industry\u2019s first platform that supports all the activities needed for analysts and data scientists to tackle Big Data Variety and produce high-value insights in a fraction of the usual time.  Through a series of live demos, this session will detail the deep integration of BDDCS with Hadoop and Spark, and highlight the rich set of integrated UIs that guide users to understand unfamiliar data, transform and enrich dirty data, and create compelling visualizations and dashboards.\nOracle Big Data Preparation Cloud Service is the industry\u2019s only data preparation platform to combine Big Data, Spark, and Apache Natural Language Processing to dramatically improve the intuitiveness and ease of use for preparing and enriching semi-structured data. Come to this session to find out how Oracle is pushing forward state of the art technology all while making it easier for non-programmers to participate in the benefits of a truly modern data foundation.\nThe intersection of cloud and big data on the information management industry might become the \u2018killer app\u2019 for how we collect, store, move, and analyze information. Cloud and big data are driving business analytics to new breakthroughs in discovery, real-time streaming, predictive capabilities, and self-service modes of operation. Join us in this session as we help bridge the gap between old and new by explaining how to take advantage of this new world of big data in the simplest possible way, from the Cloud.\nThis sessions is sponsored by Oracle"}, "big-data-conference-ny-2015/public/schedule/detail/45375": {"room": "Javits North", "title": "Startup Showcase", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45375", "topics": "Events", "speaker_urls": [], "timing": "6:30pm\u20138:00pm Tuesday, 09/29/2015", "abstract": "Sponsored by:\nAs a part of NYC DataWeek, Startup Showcase is a free event open to the public. Register here to attend. (Strata + HW attendees do not need to register separately for Startup Showcase).\n\n\n\n\n\nA new generation of businesses is betting that data-driven products can give them the edge. From efficient marketplaces to frictionless transactions, from better customer interaction to prescient predictions, from platforms that can mine vast troves of data instantly to catalogs of information available for sale, the startup ecosystem loves big data.\nStartups that put data first aren\u2019t just able to adjust faster and find their product and market better. They\u2019re also reconsidering aging industries and ailing infrastructure, upending incumbents and breaking down barriers to entry.\nStartup Showcase returns to Strata + Hadoop World in New York. It\u2019s a chance for founders to hone their pitch, meet investors, and bounce big idea off the industry\u2019s movers and shakers.\nStartup Showcase Finalist Companies\n\nAlgorithmia\nArcadia Data\nBlueTalon\nData Visor\nFuzzy.io\nImmuta\n\n\nPHEMI\nPodium Data\nPopily\nSense\nTimbr.io\nTuva Labs"}, "big-data-conference-ny-2015/public/schedule/detail/42710": {"room": "3D 02/11", "title": "What does your smart device know about you?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42710", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/145736"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "Devices that make up the Internet of Things (IoT) collect a monumental amount of data about their owners.  In most cases, the data they gather benefits the owner of the device and performs some useful purpose for them.  However, when viewed in aggregate, the data gathered can reveal an enormous amount of information about the devices\u2019 owner that can be very invasive if this information were to fall into the wrong hands.\nOver the course of several months, Charles Givre did an experiment in which he collected data from several IoT devices including a Nest Thermostat, the Automatic Car dongle, the Wink hub, and a few others in order to determine what could be learned about the owner of the devices.  Givre approached this experiment like a law enforcement or intelligence investigation, beginning with a bit of seed knowledge about the target, and built a profile about the target using the data that was available via these devices\u2019 APIs and the data they transmit over the internet.\nThis presentation is not about how to bypass the devices\u2019 security features, hack them, or how to mess with people by randomly turning off their A/C; but rather focuses on the consequences of IoT devices collecting and storing data."}, "big-data-conference-ny-2015/public/schedule/detail/43223": {"room": "3D 05/08", "title": "Multi-tenant, multi-cluster, and multi-container Apache HBase deployment", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43223", "topics": "Production Ready Hadoop", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/103890", "/big-data-conference-ny-2015/public/schedule/speaker/203924"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "With the number of production Apache HBase clusters increasing, there is greater demand for running multiple applications on single clusters, for data reliability and availability, and for developers to better test their applications. We\u2019ll lay out how these new demands can be addressed using multi-tenant, multi-cluster, or multi-container deployments.\nA multi-cluster approach is a viable option when single-cluster fault tolerance is insufficient. We will discuss several deployments and strategies where availability-sensitive applications benefit from geographically-distributed clusters.\nMulti-tenancy becomes vital as the number of users and use cases for Apache Hadoop and Apache HBase continue to grow. The ability to handle multiple workloads and multiple frameworks on less hardware improves cost efficiency. We\u2019ll present current solutions and new features, such as request scheduling, that can help overcome the isolation challenges with these deployments.\nMulti-container deployments of HBase on a single host using Docker, a container-based virtualization platform, have driven Cloudera\u2019s efforts to improve the quality of HBase releases. We will discuss how a \u201cdistributed\u201d HBase cluster can be quickly deployed, and how it can help test HBase applications with less hardware while reducing complexity."}, "big-data-conference-ny-2015/public/schedule/detail/43186": {"room": "1 E20 / 1 E21", "title": "Next-generation genomics analysis with Apache Spark", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43186", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/197575"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "The genome is \u201cthe blueprint of life,\u201d a repeating string composed of four letters, whose order and configuration lay the plan for each individual\u2019s growth and development. Genomics is the study of the structure, function, and evolution of genomes at a variety of scales: from the single cells of a cancer tumor to the genomes of an entire population of individuals. Scientists use \u201csequencers\u201d to look at the molecular structure of the genome much the same way that astronomers use telescopes to examine composition of stars, and what they see with these molecular telescopes holds the potential to find new drugs, diagnose patients, uncover the genealogy of entire populations, and discover the genetic bases for human disease.\nGenomics is also in the middle of a massive technological revolution; over the past decade, the sequencers used by scientists have improved in cost, quality, and speed at exponential rates. Fifteen years ago, it took billions of dollars and years of work for an international consortium of researchers to produce a single human genome; today a single sequencing center can sequence a human genome in a single day for almost $1000. Thousands of human genomes have been sequenced, and projects to sequence hundreds of thousands or millions of genomes are already underway.\nEven as the experimental machinery of genomics has advanced, however, its computational support \u2014 the tools and methods that convert raw data into clinical findings and research discoveries \u2014 has not kept pace. Genomics software today runs much the way it did ten years ago: discrete tools, scripting for workflow, files instead of databases, file formats in place of data models, and little-to-no parallelism.\nSpark is an ideal platform for organizing large genomics analysis pipelines and workflows. Its compatibility with the Hadoop platform makes it easy to deploy and support within existing bioinformatics IT infrastructures, and its support for languages such as R, Python, and SQL ease the learning curve for practicing bioinformaticians. Widespread use of Spark for genomics, however, will require adapting and rewriting many of the common methods, tools, and algorithms that are in regular use today.\nThis talk will present ADAM, an open-source library for bioinformatics analysis, written for Spark and hosted by the AMPLab.  We will discuss both the places where Spark\u2019s ability to parallelize an analysis pipeline is a natural fit for genomics methods, as well as some methods that have proven more difficult to adapt. We will also cover ADAM\u2019s use of technologies like Avro, for schema specification, and Parquet, for compressed file formats, in conjunction with its Spark-based workflows."}, "big-data-conference-ny-2015/public/schedule/detail/43341": {"room": "1 E16 / 1 E17", "title": "Transaction processing with Apache Hive, HBase, and Phoenix", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43341", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/163846"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "One of the promises of Hadoop is that a group of users can store all their data together, for shared use and analysis. A pattern frequently seen today is Apache HBase for fast updates and low latency data with Apache Phoenix on top for SQL; while Apache Hive is the de-facto SQL standard for data warehousing on Hadoop, used for both batch and interactive analytic queries.\nThis separation of data into tools based on the intended use leads to duplication of data when it must be used in both situations. It causes users to spend resources moving data back and forth between tools. And users must learn multiple SQL dialects, remember which data is where, and understand when to use which tool.\nThere is work going on in the Hive, HBase, and Apache Phoenix communities to significantly improve the integration of these tools so that users can have one dialect of SQL, one O/JDBC connection point, and one set of tables to store their data in, regardless of whether it is intended for transactional or analytic use. This work takes advantage of changes in Hive to incorporate HBase as a storage layer for Hive tables, and Phoenix operators to execute queries against data stored in HBase. This talk will cover this work and how it relates to other work happening in the Hive, HBase, and Phoenix communities."}, "big-data-conference-ny-2015/public/schedule/detail/43346": {"room": "1 E16 / 1 E17", "title": "Understanding Hadoop security internals", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43346", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203846"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "First-generation Hadoop security design was implemented a few years ago and became the core of Hadoop 1.0 security. Elements of the original design have been adopted and extended by other projects in the Hadoop stack. For example, HBase authentication token is a Hadoop job credential issued by HBase and can be used to access HBase from Hadoop jobs. It is similar in design to Hadoop block token but used in the fashion of Hadoop delegation token. Understanding the original Hadoop security design is essential for evaluating or adding such extensions, e.g., when designing your own security tokens to be used with Hadoop jobs.\nTo that end, this talk will cover:\n\nScope and assumptions of the original design\nRationale behind our design choices\nHadoop credential delegation model\nLimitations of delegation tokens used in Hadoop"}, "big-data-conference-ny-2015/public/schedule/detail/43181": {"room": "1 E12/ 1 E13", "title": "Leverage data analytics to reduce human space mission risks", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43181", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203783", "/big-data-conference-ny-2015/public/schedule/speaker/133673"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Testing of all the systems involved with manned as well as unmanned human space missions is a serious endeavor.  One of the major projects is the Orion Multi-Purpose Crew Vehicle, which is designed for long-duration, human-rated deep space exploration. Orion will transport humans to interplanetary destinations beyond low Earth orbit, such as asteroids, the moon, and eventually Mars, and return them safely back to Earth.\nWith human lives on the line, along with millions of dollars of highly technical equipment, testing of all the systems and subsystems for the Orion Space systems is long and exhaustive.  Simulating the space environment on the ground and testing the system is a lengthy and expensive effort \u2013 requiring perfect results for each subsystem and its subsystems. These tests generate hundreds of megabytes per second of telemetry data that in very short time becomes petabytes of data that needs to managed, analyzed, and leveraged to validate healthy functioning of all the systems.\nIn simulating the real mission environment, test telemetry data is streamed to the testing system. This telemetry data contains mission-critical health information about equipment, and the test\u2019s status. Knowing whether or not the test is progressing correctly can advise the test conductors to make decisions about the continuance or modification of a test scenario \u2013 a test that may take weeks to accomplish.\nOur work with the Orion Space capsule takes this streaming test data and saves it to a Hadoop-based cluster supporting high data rate ingest. Advanced analytics can be run on the streaming data to check for expected or indeterminate patterns.   This method of data analytics for system testing in an online environment opens up new opportunities for the test conductors to significantly reduce the risk of missing critical test parameters. It also creates a highly cost-effective and productive test environment."}, "big-data-conference-ny-2015/public/schedule/detail/43119": {"room": "3D 02/11", "title": "When it absolutely, positively, has to be there: Reliability guarantees in Kafka", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43119", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/126882", "/big-data-conference-ny-2015/public/schedule/speaker/187008"], "timing": "11:20am\u201312:00pm Wednesday, 09/30/2015", "abstract": "In the financial industry, losing data is unacceptable. Financial firms are adopting Kafka for their critical applications. Kafka provides the low latency, high throughput, high availability, and scale that these applications require. But can it also provide complete reliability? As a system architect, when asked \u201cCan you guarantee that we will always get every transaction,\u201d you want to be able to say \u201cYes\u201d with total confidence.\nIn this session, we will go over everything that happens to a message \u2013 from producer to consumer, and pinpoint all the places where data can be lost \u2013 if you are not careful. You will learn how developers and operation teams can work together to build a bulletproof data pipeline with Kafka. And if you need proof that you built a reliable system \u2013 we\u2019ll show you how you can build the system to prove this too."}, "big-data-conference-ny-2015/public/schedule/detail/45338": {"room": "3D 04/09", "title": "Protecting the humanity in data II: personalized crisis counseling / messiness of interpretation", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45338", "topics": "Law, Ethics, & Open Data", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/148636", "/big-data-conference-ny-2015/public/schedule/speaker/164227", "/big-data-conference-ny-2015/public/schedule/speaker/63069"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "DataKind founder and executive Jake Porway hosts two sessions back-to-back that explore ethical questions in data science for good projects.\nIn this second session, Bob Filbin, chief data scientist at Crisis Text Line, will present on their approach to personalized crisis counseling and Danah Boyd, founder of Data & Society Research Institute, will present on the challenges of interpreting results.\nPersonalized crisis counseling \u2013 Bob Filbin\nCrisis Text Line is a free, 24/7 text line available nationwide that connects anyone in crisis to crisis counselors. Having handled over 5 million text messages to date, Crisis Text Line recently worked with DataKind to more quickly identify repeat texters, and route them to long-term care so their counselors could focus on critical cases requiring rapid intervention.\nThe messiness of interpretation and decision-making \u2013 Danah Boyd\nThe data is clean; the algorithm is mathematically sound. But that is not enough when data is being used to make meaning and decisions. In this talk, Danah will drill into the assumptions involved in interpretation, and the social and cultural challenges that data analysts must grapple with when doing their work.\nDon\u2019t miss the first half of this speaking series \u2013 join us for both!"}, "big-data-conference-ny-2015/public/schedule/detail/45337": {"room": "3D 04/09", "title": "Protecting the humanity in data I: Ethics of algorithms / ethics of data activism / targeting services without excluding the needy", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45337", "topics": "Law, Ethics, & Open Data", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/148636", "/big-data-conference-ny-2015/public/schedule/speaker/213533", "/big-data-conference-ny-2015/public/schedule/speaker/217563", "/big-data-conference-ny-2015/public/schedule/speaker/213534"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "DataKind founder and executive Jake Porway hosts two sessions back-to-back that explore ethical questions in data science projects.\nIn this first session, Cathy O\u2019Neil, author of Weapons of Math Destruction, will present on the ethics of algorithms; Vlad Dubovskiy, Data Scientist at DonorsChoose.org, will present on the ethics of data activism and Kamalesh Rao, DataKind volunteer, will present on his recent project with social enterprise Simpa Networks targeting services without excluding the needy.\nThe ethics of algorithms \u2013 Cathy O\u2019Neil\nMathematical modeling has become a pervasive and destructive force in society\u2014in finance, education, medicine, politics, and the workplace. How are current models exacerbating inequality and endangering democracy and how we might rein them in?\nThe ethics of data activism \u2013 Vlad Dubovskiy\nData for good can hurt a social mission. How does one expose data-backed underfunding of schools in low income communities without being in the business of public shaming, alienating public figures and hurting a broader cause of education reform?\nTargeting services without excluding the needy \u2013 Kamalesh Rao\nSimpa Networks is a technology company with a bold mission: to make modern energy simple, affordable, and accessible for everyone.  In a project financially underwritten by MasterCard, DataKind volunteers are using Simpa Networks\u2019 historical data on customer payment behavior, and to predict which new applicants are likely to be a good fit for their model and able to successfully unlock the system. More customers successfully making payments means more families and households with reliable access to clean electricity they wouldn\u2019t have had otherwise.\nStay put for our second session and don\u2019t miss Jake\u2019s talk on the main stage on October 1!"}, "big-data-conference-ny-2015/public/schedule/detail/43550": {"room": "1 E16 / 1 E17", "title": "OLTP on Hadoop: Reviewing the first Hadoop-based TPC-C benchmarks", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43550", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/148190", "/big-data-conference-ny-2015/public/schedule/speaker/153764"], "timing": "5:25pm\u20136:05pm Wednesday, 09/30/2015", "abstract": "Benchmarks represent a way for computing systems and databases to show how they stack up on a common workload. Often, benchmarking becomes a \u201cspace race\u201d where vendors work to push the envelope of their joint systems with high-powered hardware and optimized databases. For companies looking to identify solutions based on benchmarks, high performance dreams are often tempered by high performance prices.\nThe rise of Hadoop, NoSQL, and other distributed computing platforms that rely on commodity hardware has led to a cultural shift away from \u201cbig iron\u201d to systems that can scale out by adding lower cost commodity servers. While Hadoop has been able to do many things in the enterprise when it comes to big data, analytics, and applications, one hurdle it had not cleared was the TPC-C benchmark\u2026until now.\nIn this session, we\u2019ll review how a transactional Hadoop database was able to complete the TPC-C benchmarks through the use of HBase and SQL. We\u2019ll also discuss the deep empirical analysis that we performed on the TPC-C benchmark to understand how it translates to Hadoop. Finally, we\u2019ll share our results for the first time with the public, and show how transactional Hadoop can change the status quo for running operational applications.\nThe goal of this session is to help expand the footprint of Hadoop in the enterprise by educating attendees about the possibilities for Hadoop to replace traditional databases for OTLP and OLAP workloads."}, "big-data-conference-ny-2015/public/schedule/detail/43553": {"room": "1 E10 / 1 E11", "title": "How Hadoop is powering Walmart\u2019s data-driven business", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43553", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/204188"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Under Jeremy King\u2019s watch, Walmart has been using big data to power the best online shopping experience available today. Every week, up to 250 million people visit a Walmart property online and King is making it Walmart\u2019s mission to keep those people coming back with personalized shopping experiences.\nThe transformation started in 2012 when Walmart eCommerce moved from a small Hadoop cluster to a big one (250 modes), and has since used Hadoop to consolidate 10 different websites into one website. The big data capabilities in the new Hadoop cluster provide a central repository that can go back years. Walmart\u2019s access to big data is indefinite, and powers various personalized shopping experiences for millions of Walmart\u2019s customers, making their lives easier."}, "big-data-conference-ny-2015/public/schedule/detail/44261": {"room": "3A & 3B", "title": "Lunch / Wednesday BoF Tables", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44261", "topics": "Events", "speaker_urls": [], "timing": "12:00pm\u20131:15pm Wednesday, 09/30/2015", "abstract": "Note: Expo Plus pass holders do not have access to Lunch or Lunchtime BoFs.\nBirds of a Feather (BoF) discussions are a great way to informally network with people in similar industries or interested in the same topics.\nBoFs will happen during lunch on Wednesday, September 30 and Thursday, October 1.\n\n\n\n\n\nThis year\u2019s Industry Birds of a Feather discussion topics include:\n\nAdvertising & Marketing\nEnergy\nFinance\nGovernment & Policy\nHealthcare\nMedia & Entertainment\nRetail & eCommerce\nTelecommunications"}, "big-data-conference-ny-2015/public/schedule/detail/43555": {"room": "1 E16 / 1 E17", "title": "What does it mean to virtualize the Hadoop distributed file system?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43555", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/178134"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "There are at least two meanings of the phrase \u201cvirtualized HDFS.\u201d One is the creation of an HDFS file system within a cluster of virtual machines; the second is the abstraction of the HDFS protocol in order to implement a \u201cvirtual\u201d HDFS file system and permit any storage device to provide data to Hadoop applications.\nThis session will investigate both of these meanings of virtualized HDFS. It will draw from experiences with multiple projects (including Apache Tachyon, MemHDFS, CEPH object store, and others) to describe existing implementations of the first and propose a high-speed implementation of the second."}, "big-data-conference-ny-2015/public/schedule/detail/45332": {"room": "3D 05/08", "title": "Ask me anything: Hadoop operations for production systems", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45332", "topics": "Ask Me Anything", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/163508", "/big-data-conference-ny-2015/public/schedule/speaker/190141", "/big-data-conference-ny-2015/public/schedule/speaker/107366", "/big-data-conference-ny-2015/public/schedule/speaker/151508"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "Join the instructors of the all-day tutorial \u201cApache Hadoop operations for production systems,\u201d as they field a wide range of detailed questions. Even if you don\u2019t have a specific question, join in to hear what others are asking."}, "big-data-conference-ny-2015/public/schedule/detail/45330": {"room": "3D 05/08", "title": "Ask me anything: Hadoop application architectures", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45330", "topics": "Ask Me Anything", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/126882", "/big-data-conference-ny-2015/public/schedule/speaker/134675", "/big-data-conference-ny-2015/public/schedule/speaker/169835", "/big-data-conference-ny-2015/public/schedule/speaker/147273"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "Join the authors of Hadoop Application Architectures for an open Q/A session on considerations and recommendations for architecture and design of applications using Hadoop. Talk to us about your use-case and its big data architecture, or just come to listen in."}, "big-data-conference-ny-2015/public/schedule/detail/45838": {"room": "Javits North", "title": "Sponsored keynote", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45838", "abstract": "Details to come\u2026", "speaker_urls": [], "timing": "9:45am\u20139:50am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43024": {"room": "3D 04/09", "title": "Many streams lead to Kafka  - An event data workshop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43024", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203749", "/big-data-conference-ny-2015/public/schedule/speaker/214487"], "timing": "9:00am\u201312:30pm Tuesday, 09/29/2015", "abstract": "Description\nSee how Kafka can help you harness the value of stream data in your organization!  During this three-hour tutorial we\u2019ll discuss what Kafka is, and its emerging critical role in the modern data management and distribution pipeline.\nWe\u2019ll also discuss key architectural concepts and developer APIs. The tutorial includes hands-on labs where you\u2019ll build an application that can publish data to Kafka, and subscribe to receive data from Kafka.\nHere\u2019s the high-level agenda for the tutorial:\n\nIntroduction to what Kafka is, its capabilities, and major components\nTypes of data appropriate for Kafka\nProducers, consumers, and brokers and their roles in a Kafka cluster\nDeveloper APIs in various languages for publication/subscription to Kafka Topics\nCommon patterns for application development with Kafka\n\nThis tutorial is ideal for application developers, extraction-transformation-load (ETL) developers, or data scientists who need to interact with Kafka clusters as a source of, or destination for, stream data."}, "big-data-conference-ny-2015/public/schedule/detail/43026": {"room": "1 E8 / 1 E9", "title": "From anomalies to alerts: Identifying anomalies and rank ordering them to create alerts for data scientists to investigate", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43026", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/124005"], "timing": "11:20am\u201312:00pm Thursday, 10/01/2015", "abstract": "In this talk, we discuss some lessons learned from building three operational systems for detecting anomalies and creating alerts that can be examined in detail by data scientists.\nThe first case study involves identifying anomalies and other patterns of interest in disease surveillance data.  The second case study involves looking for anomalies that may indicate data quality problems in a large-scale cloud computing platform that processes genomic data.  The third case involves looking for anomalous events that are important enough for network analysts to investigate in detail from the daily data that a large network produces, including the network flow data and log files.\nFrom these three case studies, we extract six techniques that have consistently proved useful and discuss how best these techniques can be used in practice.\n\nUsing Monte Carlo methods to estimate the statistical significance of anomalies\nUsing change detection models to identify anomalies more quickly\nUsing segmented models to manage very large datasets\nCollapsing cascades of anomalies to reduce the number of alerts to investigate\nUsing visual analytics to reduce the number of alerts to investigate\nUsing semi-supervised learning to deal with the lack of labeled data"}, "big-data-conference-ny-2015/public/schedule/detail/45853": {"room": "1 E14", "title": "Eventual consistent systems a.k.a mostly inconsistent systems versus strongly consistent systems in big data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45853", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/153419"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "This talk explores the actual behavior of eventual consistent systems and argues that they should, in fact, be called mostly inconsistent systems. It presents a paxos algorithm based strongly consistent system as an alternative. Amazon\u2019s eventually consistent storage system S3 is explored. The presentation highlights the various fixes that Amazon had to make to S3 in order to make it work with Hadoop workflows. Also, it delves into the various inconsistent alternatives offered by Cassandra. As an alternative to such inconsistent systems, paxos-based strongly consistent solutions for Hadoop Storage and HBase are presented.\nThis session is sponsored by WANdisco"}, "big-data-conference-ny-2015/public/schedule/detail/44766": {"room": "3E", "title": "Booth Crawl", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44766", "topics": "Events", "speaker_urls": [], "timing": "6:05pm\u20137:05pm Wednesday, 09/30/2015", "abstract": "Quench your thirst with vendor-hosted libations and snacks while you check out all the exhibitors in the Expo Hall. It\u2019s also a great time to meet and mingle with fellow attendees and Strata + Hadoop World speakers and authors.\nThe Booth Crawl is happening immediately after the afternoon sessions on Wednesday."}, "big-data-conference-ny-2015/public/schedule/detail/43228": {"room": "3D 03/10", "title": "Knowledge and the geospatial mixing pot", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43228", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/135670"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "The geospatial world has recently been transformed. You no longer need a remote sensing specialist to access real-time imagery from space. Up-to-the minute Twitter data can be harvested with a couple of clicks. And beautiful maps can be created by anyone with a basic understanding of design. So how do maps fit into a future where everything is geospatial and everyone can access it?\nWe have built an open source tool and a platform that is wiring together geospatial data on the web. Now, 200,000 users are building the next generation of mapping and geospatial applications, by easily tapping into data sources that were historically difficult to use. Journalists are leveraging images taken from satellites to tell breaking news stories, and environmentalists are tracking deforestation from a world away.\nHere we will talk about how we have reimagined the way people work with geospatial data to make it easy and powerful for those with little to no programming capability, but still flexible enough for application development.\nWe will discuss how CartoDB has helped change who creates and consumes maps on the web. We will discuss how we have exposed high-resolution up-to-the-hour satellite data for anyone to access. And we will talk about how we see the future of web-mapping changing, as many of the tools that were once difficult are commoditized, and as more and more data becomes connected on the cloud."}, "big-data-conference-ny-2015/public/schedule/detail/43782": {"room": "3D 01/12", "title": "Spark Development Bootcamp (Day 2)", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43782", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/215137"], "timing": "9:00am\u20135:00pm Wednesday, 09/30/2015", "abstract": "Description\nOverview\nThis three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.\nSpark is a unified framework for big data analytics. Spark provides one integrated API for use by developers, data scientists, and analysts to perform diverse tasks that would have previously required separate processing engines such as batch analytics, stream processing and statistical modeling. Spark supports a wide range of popular languages including Python, R, Scala, SQL, and Java. Spark can read from diverse data sources and scale to thousands of of nodes.\nIn this class, you will learn how to build and manage Spark applications using Spark\u2019s core programming APIs and its standard Libraries. Hands-on exercises will be done in Scala. Course materials emphasize design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.\nThose who attend the training will have opportunities during the tutorial to meet and have discussions with members of the Spark development community, including Q&A sessions and discussions about real-world use cases. You will receive a free Databricks account for the duration of training.\nCourse Learning Objectives\nAfter taking this class you will be able to:\n\nBuild a data pipeline using Spark DataFrames and Spark SQL\nUnderstand Spark concepts, architecture, and applications\nExecute SQL queries on large scale data using Spark\nExplore and visualize your data by entering and running code in Notebooks\nTrain, and use an ML model on real data with Spark\u2019s Machine Learning library MLlib\nTune Spark job performance and troubleshoot errors using logs and administration UIs\nFind answers to common questions using Spark documentation and discussion forums\nWrite and monitor a Spark Streaming job to analyze data with sub-second latency\nUnderstand common use-cases and business applications of Spark\nRecognize all of the topics tested by the Spark Developer Certification and know what further work is required to prepare to take and pass the exam\n\nPrerequisites\nStudents, please arrive to class with:\n\nA basic understanding of software development\nSome experience coding in Python, Java, SQL, or Scala\nA modern operating system (Windows, OS X, Linux), browser (Internet Explorer not supported)\n\nOutline of topics covered in class\n\nHistory of Big Data & Apache Spark\n    \u2013 Introduction to the Spark Shell and the training environment\n    \u2013 Just enough Scala for Spark\n    \u2013 Introduction to Spark DataFrames and Spark SQL\n    \u2013 Introduction to RDDs\n       \u2013 Lazy Evaluation\n        \u2013 Transformations and Actions\n        \u2013 Caching\n        \u2013 Using the Spark UIs\n\n\nData Sources: reading from Parquet, S3, Cassandra, HDFS, and your local file system\nSpark\u2019s Architecture\nProgramming with Accumulators and Broadcast Variables\nDebugging and tuning Spark jobs using Spark\u2019s admin UIs\nMemory & Persistence\nAdvanced programming with RDDs (understanding the shuffle phase, partitioning, etc.)\nVisualization: matplotlib, gg_plot, dashboards, exploration and visualization in notebooks\nIntroduction to Spark Streaming\nIntroduction to MLlib and GraphX"}, "big-data-conference-ny-2015/public/schedule/detail/44158": {"room": "1B 03", "title": "Practical Data Science on Hadoop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44158", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/190721", "/big-data-conference-ny-2015/public/schedule/speaker/155534", "/big-data-conference-ny-2015/public/schedule/speaker/198568", "/big-data-conference-ny-2015/public/schedule/speaker/45559", "/big-data-conference-ny-2015/public/schedule/speaker/220734"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nIn this three-day course, you will:\n\nLearn how to use machine learning, text analysis, and real-time analytics to solve frequently \nencountered, high-value business problems.\nUnderstand data science methodology and end-to-end work flow of problem solution including \ndata preparation, model building and validation, and model deployment.\nUse Apache Spark and other tools for analytics.\n\nAgenda\nDay 1\n\nFundamental data science methodology\nOverview of selected machine learning methods\nHands-on labs with Spark MLlib and SystemML libraries\nDescriptive statistics\nFeature transformations\nSupervised and unsupervised methods\nDiagnostics\n\nDay 2\n\nText analytics concepts\nText analytics development, testing, and deployment\nContinuous analytics (streaming)\nHands-on labs on text analytics and streaming\n\nDay 3\n\nRecommendation engines with hands-on lab\nUsing Apache Spark with IBM SPSS Modeler\nWhat\u2019s coming in data science\nSpark and hardware accelerators\nMachine learning pipelines with hands-on lab\nProductization with Spark\n\nTarget Audience\nData scientists, business analysts.\nSome knowledge of R and/or Python is preferable but not required.\nAdditional Information\nHands-on lab environment will be provided by IBM."}, "big-data-conference-ny-2015/public/schedule/detail/45851": {"room": "1 E15", "title": "Building your first big data application on AWS", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45851", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217299"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "Want to get ramped up on how to use Amazon\u2019s big data web services and launch your first big data application on AWS? Join us on our journey as we build a big data application in real time using Amazon Elastic MapReduce (Amazon EMR) and other Amazon big data web services. We will review architecture design patterns for big data, and give you access to a take-home lab so you can rebuild and customize the application yourself.\nThis session is sponsored by Amazon Web Services"}, "big-data-conference-ny-2015/public/schedule/detail/44153": {"room": "1 E12/ 1 E13", "title": "PyData at Strata", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44153", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/130245", "/big-data-conference-ny-2015/public/schedule/speaker/130244", "/big-data-conference-ny-2015/public/schedule/speaker/190248", "/big-data-conference-ny-2015/public/schedule/speaker/34833", "/big-data-conference-ny-2015/public/schedule/speaker/169776", "/big-data-conference-ny-2015/public/schedule/speaker/219609", "/big-data-conference-ny-2015/public/schedule/speaker/180034", "/big-data-conference-ny-2015/public/schedule/speaker/133178", "/big-data-conference-ny-2015/public/schedule/speaker/175621", "/big-data-conference-ny-2015/public/schedule/speaker/219610", "/big-data-conference-ny-2015/public/schedule/speaker/196705", "/big-data-conference-ny-2015/public/schedule/speaker/219611", "/big-data-conference-ny-2015/public/schedule/speaker/219612", "/big-data-conference-ny-2015/public/schedule/speaker/220716", "/big-data-conference-ny-2015/public/schedule/speaker/155761", "/big-data-conference-ny-2015/public/schedule/speaker/189812", "/big-data-conference-ny-2015/public/schedule/speaker/221377", "/big-data-conference-ny-2015/public/schedule/speaker/221577", "/big-data-conference-ny-2015/public/schedule/speaker/221380", "/big-data-conference-ny-2015/public/schedule/speaker/221405"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Python has become an increasingly important part of the data engineer and analytic tool landscape. Pydata at Strata provides in-depth coverage of the tools and techniques gaining traction with the data audience, including IPython Notebook, NumPy/matplotlib for visualization, SciPy, scikit-learn, and how to scale Python performance, including how to handle large, distributed data sets. Come see how the leading lights in the Python data community are making Python ever more useful to data analysts and data engineers.\nSCHEDULE:\n9:00 \u2013 9:45\n\nHow to Build a Company on Open Source\nTravis Oliphant & Peter Wang\n\n9:45 \u2013 10:30\n\nHow to Build Publishing & On-Demand Learning Environments with IPython\nKyle Kelley & Andrew Odewahn\n\n10:30am \u2013 11:00am\nMORNING BREAK\n11:00am \u2013 12:30pm\nTrack 1 (room 1 E14):\n\nHow to Use Pandas for Data Analysis\nJeff Reback\n\nTrack 2 (room 1 E15):\n\nHow to Create Beautiful Visualizations with Bokeh\nSarah Bird & Bryan Van de van\n\n12:30pm \u2013 1:30pm\nLUNCH\n1:30pm \u2013 2:15pm\nTrack 1 (room 1 E14):\n\nHow to Build Big Data Workflows\nAndy Terrel & Ben Zaitlen\n\nTrack 2 (room 1 E15):\n\nHow to Solve Problems in Geophysics with Python\nPaige Bailey\n\n2:15pm \u2013 3:00pm\nTrack 1 (room 1 E14):\n\nHow to Leverage the Blaze Ecosystem\nMatthew Rocklin & Phil Cloud\n\nTrack 2 (room 1 E15):\n\nHow to Think About Python\nJames Powell\n\n3:30pm \u2013 4:00pm\nAFTERNOON BREAK\n3:30pm \u2013 4:15pm\nTrack 1 (room 1 E14):\n\nHow to Use Scikit-Learn for Machine Learning\nAndreas M\u00fcller\n\nTrack 2 (room 1 E15):\n\nHow to Use Python for Predictive Modeling\nOwen Zhang & Peter Prettenhofer\n\n4:15pm \u2013 5:00pm\nTrack 1 (room 1 E14):\n\nIntroduction to Publication Quality Plotting with Matplotlib\nDamon McDougall & Michael Droettboom\n\nTrack 2 (room 1 E15):\n\nInteractive Computing in the Jupyter Notebook \u2013 Present and Future\nJason Grout & Chris Colbert"}, "big-data-conference-ny-2015/public/schedule/detail/44154": {"room": "1 E16 / 1 E17", "title": "R Day", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/44154", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/148939", "/big-data-conference-ny-2015/public/schedule/speaker/217841", "/big-data-conference-ny-2015/public/schedule/speaker/217840", "/big-data-conference-ny-2015/public/schedule/speaker/218429"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "TUTORIAL PREREQUISITES\n\nPlease bring a laptop and power cord to class \u2013 each class will be centered around hands-on exercises.\n\n\nBefore class, please install both R and the RStudio IDE, and ensure that your computer can connect to the internet. We will use the following R packages in class, so you should install them ahead of time:\n\n\ntidyr, devtools, dplyr, ggplot2, scales, rmarkdown, knitr, DT, shiny, DBI, and RSQLite as well as the reportsWS package which must be installed with the command:\ndevtools::install_github(\u201crstudio/reportsWS\u201d)\n\n\nYou can find instructions on how to install R, the RStudio IDE, and the above packages at http://r4ds.had.co.nz/intro.html\n\nDescription\nFrom advanced visualization, collaboration, and reproducibility to data manipulation, R Day at Strata covers a raft of current topics that analysts and R users need to pay attention to. The R Day tutorials come from leading luminaries and R committers, the folks keeping the R ecosystem apace of the challenges facing analysts and others who work with data.\nSchedule\nR Quickstart: Wrangle, transform, and visualize data\nInstructor: Garrett Grolemund\n90 minutes\nThis 90 minute quickstart will teach you the most used\u2013and most powerful\u2013parts of the R language. You will learn the best ways to perform the core tasks of data science:\n\nHow to wrangle your data (with the tidyr package)\nHow to transform your data (with the dplyr package)\nHow to visualize your data (with the ggplot2 package)\nThese fast and intuitive packages will provide a solid foundation for everything else you do in R.\n\nWork with Big Data in R\nInstructor: Nathan Stephens\n90 minutes\nR is the go to language for data exploration and development, but what role can R play in production with big data? In this class, RStudio\u2019s solution engineer demonstrates a pragmatic approach for pairing R with big data. You will learn to use R\u2019s familiar dplyr syntax to query big data stored on a server based data store, like Amazon Redshift or Google BigQuery. We will also discuss how to generalize the process to other big data stores, and how to best leverage R within a big data pipeline.\nReproducible Reports with Big Data\nInstructor: Yihui Xie\n90 minutes\nThis tutorial will teach you a time-saving workflow that has become the new standard for reproducible research. The R Markdown package makes it easy to document both your code and your results in the same file. With an R Markdown file and the click of a button, you can\nre-execute your analysis with the most up-to-date code and data to create new results, and/or\ngenerate a polished report in a variety of formats (html, pdf, Word, etc.) to share your results\nThis class will demonstrate some best practices that further increase the efficiency of reproducible research with R Markdown.\nInteractive Shiny Applications built on Big Data\nInstructor: Garrett Grolemund\n90 minutes\nR\u2019s Shiny package lets you move beyond static reports to easily build interactive applications powered by R. Run your Shiny apps locally, or share them over a server with clients, customers, and colleagues. Your visitors will be in the driver seat. They can explore data, monitor dashboards, run R analyses, or do anything else you prepare for them all without knowing any R code. If a picture is worth 1000 words, a Shiny app is worth a million. In this tutorial, you will learn the basics of creating Shiny apps, as well as the best practices for using big data with your apps."}, "big-data-conference-ny-2015/public/schedule/detail/46273": {"room": "Javits North", "title": "Closing Remarks", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46273", "abstract": "Program Chairs Roger Magoulas, Doug Cutting, and Alistair Croll, close out the Strata + Hadoop World keynotes.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/5107", "/big-data-conference-ny-2015/public/schedule/speaker/103766", "/big-data-conference-ny-2015/public/schedule/speaker/17816"], "timing": "10:40am\u201310:45am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/43358": {"room": "1 E16 / 1 E17", "title": "HDFS operations made easy: Guide to the improved, full service HDFS File Browser", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43358", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203891"], "timing": "2:05pm\u20132:45pm Thursday, 10/01/2015", "abstract": "In the 2.4.0 release, a new and improved HTML5-based native Web UI was introduced in Hadoop. The new WebUI used the WebHDFS API to talk to HDFS. However, it lacked write capabilities. Many customers requested this slick new UI to be improved to a full-service interface, including:\n1.\tAbility to create directories: mkdir\n2.\tAbility to change permissions: chmod\n3.\tAbility to change owner and group: chown, chgrp\n4.\tAbility to upload files\n5.\tAbility to delete files and directories\n6.\tSort based on columns\nThis presentation will cover the challenges to achieving these features and how these challenges were solved, such as how the WebHDFS API was modified.  If there is time, we will also cover how HttpFS was used to act as a single gateway for channeling all traffic.\nAlthough there are several frameworks that also provide this functionality, we believe that such basic features should be included natively in Hadoop, without requiring additional components. We also show how this may be more efficient than an additional server proxying all traffic.\nFuture upgrades will also be covered, such as functionality for multiple file/directory uploads and other features to further improve the experience of HDFS users."}, "big-data-conference-ny-2015/public/schedule/detail/42827": {"room": "1 E12/ 1 E13", "title": "Re-engineering legacy analytics solutions with big data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42827", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/177172"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "In this project, we rescued a few barely-usable solutions from the past, and made them viable again by exploiting the speed and performance of big data platform-based execution.\nIn our analytics solution park, a number of workflows are dedicated to the analysis of energy usage time series through the monitoring of smart meter IDs. One solution in particular predicts the amount of electrical energy usage for clusters of smart meter IDs in Ireland. The bottleneck in this whole solution, however, lies in the first ETL process, which takes up to two days to execute. Such a long execution time made it barely usable in production, and challenging for re-training.\nRecently, we decided to re-engineer this legacy solution and run it on a big data platform. We transformed all ETL processes into in-database ETL processing nodes. A complex and specific SQL query, implementing all necessary conversions, joins, and aggregations, was built and executed on Hadoop clusters. The (smaller) resulting data set was then pulled back into the analytics platform to build the time series prediction model. The execution of this re-engineered ETL process now takes less than half an hour and allows for more frequent model re-trainings."}, "big-data-conference-ny-2015/public/schedule/detail/43453": {"room": "1 E20 / 1 E21", "title": "Supercharging R with Spark for end-to-end data science", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43453", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/121048"], "timing": "1:15pm\u20131:55pm Wednesday, 09/30/2015", "abstract": "R is the favorite language of many data scientists. In addition to a language and runtime, R is a rich ecosystem of libraries for a wide range of use cases, from statistical inference to data visualization. However, handling large or distributed data with R is challenging. Hence R is used along with other frameworks and languages by most data scientists. In this mode, most of the friction is at the interface of R and the other systems. For example, when data is sampled by a big data platform, results need to be transferred to and imported in R as native data structures. In this talk we show an alternative, and complementary, approach to SparkR for integrating Spark and R.\nIn the SparkR model, data is loaded and manipulated in R processes on workers. We propose handling distributed data inside the JVM, and collecting results of analysis (summarization, sampling, or modeling) as R data frames in a single R process. This approach is more convenient when dealing with external data sources such as Cassandra, Hive, and Spark\u2019s own distributed data frames. We show two specific techniques to remove the data transfer friction: RODBC connections, and user space filesystems. We think this model complements SparkR\u2019s approach and improves the day-to-day workload of many data scientists who use R. Spark\u2019s interactive query processing, especially with in-memory datasets, closely matches the R interactive session model. When integrated together, Spark and R can provide state-of-the-art tools for the entire end-to-end data science pipeline. We will show how such a pipeline would work in a live demo at the end of the talk."}, "big-data-conference-ny-2015/public/schedule/detail/46153": {"room": "Javits North", "title": "What 0-50 million users in 7 days can teach us about big data", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46153", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/190656"], "timing": "9:20am\u20139:30am Wednesday, 09/30/2015", "abstract": "Join Microsoft\u2019s Joseph Sirosh for a behind-the-scenes sneak peek into the creation of the viral phenomenon How-Old.net. He\u2019ll cover how it got to 50 million users in 7 days, the unexpected big data challenges that came with it, and the surprising learnings they had about people and systems.\nThis keynote is sponsored by Microsoft"}, "big-data-conference-ny-2015/public/schedule/detail/42945": {"room": "3D 02/11", "title": "Streaming in the extreme", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42945", "topics": "IoT & Real-time", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/144388"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Have you ever heard of Kafka? Are you ready to start streaming all the events in your business? What happens to your streaming solution when you outgrow your single data center? What happens when you are at a company that is already running multiple data centers and you need to implement streaming across data centers? What about when you need to scale to a trillion events per day?\nI will discuss technologies like Kafka that can be used to accomplish real-time, lossless messaging that works in both single and multiple globally dispersed data centers. I will also describe how to handle the data coming in through these streams in batch processes as well as real-time processes.\nFinally, we will discuss why a streaming-only implementation can deliver a better experience with equivalent results to the Lambda Architecture."}, "big-data-conference-ny-2015/public/schedule/detail/42795": {"room": "1 E19/ 1 E 20/ 1 E21", "title": "Spark Camp: An introduction to Apache Spark with hands-on tutorials", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/42795", "topics": "Spark & Beyond", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/187782", "/big-data-conference-ny-2015/public/schedule/speaker/198581", "/big-data-conference-ny-2015/public/schedule/speaker/219840"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nSponsored by:\nSpark Camp, organized by the creators of the Apache Spark project at Databricks, will be a day-long hands-on introduction to the Spark platform, including Spark Core, Spark SQL, Spark Streaming, MLlib, GraphX, and more.\nWe will start with an overview of use cases and demonstrate writing simple Spark applications. We will cover each of the main components of the Spark stack via a series of technical talks targeted at developers who are new to Spark. Intermixed with the talks will be periods of hands-on lab work.\nEveryone who participates will receive a free temporary account on Databricks Cloud, for limited free usage of Amazon AWS to run Spark clusters. We will use a cloud-based notebook for the exercises."}, "big-data-conference-ny-2015/public/schedule/detail/43051": {"room": "1 E12/ 1 E13", "title": "A hierarchical data warehouse in Hadoop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43051", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203574"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "A data warehouse is used to organize enterprise data in a centralized location. The traditional approach of ETL and Dimension models are well established. However, with the advent of distributed computing and storage solutions like Hadoop, do old techniques still work effectively? We shall examine several data models that are suitable to build an enterprise-class system on Hadoop. In particular we will delve into hierarchical data models that have proven to be flexible and performant on Hadoop.\nA hierarchical database has several important properties to model a data warehouse on Hadoop, such as:\n1. Adding new data sources is easy: You can connect parts of the data sources to any node in a tree without impacting the rest of your system.\n2. It supports time series data so you can handle historical data and slow-changing data.\n3. A hierarchical model is denormalized (mostly) and is particularly suitable for storage and processing on Hadoop.\n4. Generating cube lattices from a hierarchical model is simple and efficient.\nIn this presentation we will walk through the process of creating a performant data warehouse on Hadoop with an example and share results from a real-world use case. The presentation will also highlight the architecture of an end-to-end system from source discovery, to building data warehouse models, to building large size cubes on Hadoop."}, "big-data-conference-ny-2015/public/schedule/detail/45904": {"room": "1 E6 / 1 E7", "title": "Enable secure data sharing and analytics in Hadoop with 5 key steps", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45904", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/217623"], "timing": "2:55pm\u20133:35pm Thursday, 10/01/2015", "abstract": "With big data analytics and Hadoop environments come myriad benefits\u2014but also new risks to enterprises. In the past, cyber-attackers had to search for high-value information across a wide range of systems. But now with centralized data, hackers can focus on a known, single target. Even when key information is secured in the organization, there is still a high risk that sensitive information can be re-identified by utilizing multiple data sets.\nBuilding a strategy and methodology that protects data is vital in securing these systems and enterprise assets. Learn how people protect big data in Hadoop, and understand how protecting the information is possible without removing the benefits, or paying a performance penalty.\nJoin us to understand the different security options available to you.  And come away with 5 key steps to protecting critical data.\nThis session is sponsored by HP Security Voltage"}, "big-data-conference-ny-2015/public/schedule/detail/43210": {"room": "1 E10 / 1 E11", "title": "What can Big Pharma teach us about Wall Street?  What can Wall Street teach us about Big Pharma?", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43210", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/184590", "/big-data-conference-ny-2015/public/schedule/speaker/203913", "/big-data-conference-ny-2015/public/schedule/speaker/203914"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "When Pfizer approved Viagra for commercial use in 1998, they enjoyed a handsome 38% increase in stock price the following year.  This spike reflects an intuitive notion: a winning record in the lengthy and costly drug passage process should be the key determiner of a pharmaceutical company\u2019s financial success.  But how do we distinguish between wonder drugs and duds, in both patient health and the bottom line?\nThe Freedom of Information Act requires the FDA to release drug passage data to the public.  This data includes basic information, such as type of drug and method of delivery, as well as chemical information such as molecular mass and bond types.  Studies suggest that several of these factors can determine a drug\u2019s success through the three-phase approval process.  The magnitude of a drug\u2019s success in the markets, however, remains largely an open question.\nOur talk explores the marriage of pharmaceutical and financial data.  In particular, Joe Klobusicky, Ali Habib, and Kate Volkova will investigate how a company\u2019s industrial features drive a drug\u2019s passage, as well as the inverse question of how drug passage determines financial growth.  While we will highlight several interesting examples of individual companies, our focus is predictive, and therefore aims to make forecasting statements over the pharmaceutical industry as a whole."}, "big-data-conference-ny-2015/public/schedule/detail/43217": {"room": "1 E18", "title": "Machine Learning 101", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43217", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/169590", "/big-data-conference-ny-2015/public/schedule/speaker/203918", "/big-data-conference-ny-2015/public/schedule/speaker/203919"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nThis hands-on tutorial provides a quick start to building intelligent business applications using machine learning. Learn about machine learning basics, feature engineering, recommender systems, and deep learning. We will build and deploy large-scale machine learning applications with Dato\u2019s Machine Learning platform: GraphLab Create, Dato Distributed, and Dato Predictive Services. The program will center around building two applications: a content-based recommender that tells you which talks you might be interested in at Strata, and an image search application built using deep learning.\nWe will walk you through all the steps of prototyping and production: data cleaning, feature engineering, model building and evaluation, and deployment.\nPlease check back here prior to the tutorial date for installation instructions.\nTopics:\n\nOverview of machine learning\nFeature engineering\nPersonalized recommenders\nContent-based analysis\nFactorization models\nBuilding StrataNow\u2014a personalized talk recommender for StrataConf\nDeep learning\nDeep learning models\nTransfer learning and deep features\nBuilding an image search application to find similar clothing\nDeploying machine learning models in production\nConstructing a real-time predictive service\nMonitoring and evaluation"}, "big-data-conference-ny-2015/public/schedule/detail/43781": {"room": "3D 01/12", "title": "Spark Development Bootcamp", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43781", "topics": "Training", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/215137"], "timing": "9:00am\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nOverview\nThis three-day curriculum features advanced lectures and hands-on technical exercises for Spark usage in data exploration, analysis, and building big data applications.\nSpark is a unified framework for big data analytics. Spark provides one integrated API for use by developers, data scientists, and analysts to perform diverse tasks that would have previously required separate processing engines such as batch analytics, stream processing and statistical modeling. Spark supports a wide range of popular languages including Python, R, Scala, SQL, and Java. Spark can read from diverse data sources and scale to thousands of of nodes.\nIn this class, you will learn how to build and manage Spark applications using Spark\u2019s core programming APIs and its standard Libraries. Hands-on exercises will be done in Scala. Course materials emphasize design patterns and best practices for leveraging Spark in the context of other popular, complementary frameworks for building and managing enterprise data workflows.\nThose who attend the training will have opportunities during the tutorial to meet and have discussions with members of the Spark development community, including Q&A sessions and discussions about real-world use cases. You will receive a free Databricks account for the duration of training.\nCourse Learning Objectives\nAfter taking this class you will be able to:\n\nBuild a data pipeline using Spark DataFrames and Spark SQL\nUnderstand Spark concepts, architecture, and applications\nExecute SQL queries on large scale data using Spark\nExplore and visualize your data by entering and running code in Notebooks\nTrain, and use an ML model on real data with Spark\u2019s Machine Learning library MLlib\nTune Spark job performance and troubleshoot errors using logs and administration UIs\nFind answers to common questions using Spark documentation and discussion forums\nWrite and monitor a Spark Streaming job to analyze data with sub-second latency\nUnderstand common use-cases and business applications of Spark\nRecognize all of the topics tested by the Spark Developer Certification and know what further work is required to prepare to take and pass the exam\n\nPrerequisites\nStudents, please arrive to class with:\n\nA basic understanding of software development\nSome experience coding in Python, Java, SQL, or Scala\nA modern operating system (Windows, OS X, Linux), browser (Internet Explorer not supported)\n\nOutline of topics covered in class\n\nHistory of Big Data & Apache Spark\n    \u2013 Introduction to the Spark Shell and the training environment\n    \u2013 Just enough Scala for Spark\n    \u2013 Introduction to Spark DataFrames and Spark SQL\n    \u2013 Introduction to RDDs\n       \u2013 Lazy Evaluation\n        \u2013 Transformations and Actions\n        \u2013 Caching\n        \u2013 Using the Spark UIs\n\n\nData Sources: reading from Parquet, S3, Cassandra, HDFS, and your local file system\nSpark\u2019s Architecture\nProgramming with Accumulators and Broadcast Variables\nDebugging and tuning Spark jobs using Spark\u2019s admin UIs\nMemory & Persistence\nAdvanced programming with RDDs (understanding the shuffle phase, partitioning, etc.)\nVisualization: matplotlib, gg_plot, dashboards, exploration and visualization in notebooks\nIntroduction to Spark Streaming\nIntroduction to MLlib and GraphX"}, "big-data-conference-ny-2015/public/schedule/detail/45826": {"room": "3D 03/10", "title": "Music Science: How data is changing what and what we listen to", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45826", "topics": "Design, User Experience, & Visualization", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/41072", "/big-data-conference-ny-2015/public/schedule/speaker/217087", "/big-data-conference-ny-2015/public/schedule/speaker/217349", "/big-data-conference-ny-2015/public/schedule/speaker/217351"], "timing": "4:35pm\u20135:15pm Wednesday, 09/30/2015", "abstract": "The convergence of digital music, streaming services, and big data have created a new discipline: Music Science. Digital downloads and monthly subscriptions have all but replaced the physical distribution of music, leaving labels scrambling and artists adapting to a new world.\nMusic science is about prediction\u2014what song you want next; what the next big hit will be; what that can\u2019t-ignore track is called; how to split, share, and remix songs across platforms. It\u2019s about business\u2014with billions of venture investment and industry-changing acquisitions happening every month. And most importantly\u2014it\u2019s a canary in the big data coal mine, because the tools and strategies that shape digital music will soon apply to every consumer service and every disrupted market.\nThis panel brings together founders and technologists who live on the cutting edge of music science. We\u2019ll look at the \u201cTuring problems\u201d of digital entertainment, as well as how providers strike a balance between human curation and machine optimization."}, "big-data-conference-ny-2015/public/schedule/detail/43292": {"room": "1 E10 / 1 E11", "title": "How a global entertainment company successfully built a data lake for continued digital dominance", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43292", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/162445"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "With a broad roster of new stars and legendary artists, this global record company has long been considered a technology innovator and progressive force in the music business. Ahead of the industry with its development of a dedicated digital strategy team, the company is recognized for adopting and harnessing digital technology for music creation and distribution.\nRecognizing that ongoing digital innovation is essential to this music giant\u2019s continued success, it is clear there is a need to replace legacy applications with a fully integrated framework based on open source, big data technologies. This means seamlessly integrating data from their more than 15+ record labels and a global publishing catalog containing more than 100 million+ copyrights held worldwide.\nTo transition their paradigm and embrace the benefits of a rapidly evolving technology ecosystem, the company and Caserta Concepts worked together to provide a comprehensive roadmap and implementation of a new data platform in the cloud.\nIn this presentation, you\u2019ll hear about the strategy, the process, the challenges, and the solution, including:\n\nThe process for assessing the current landscape, and how strategic recommendations that included re-architecting critical components of the platform to gain stability, performance and resiliency, were developed\nThe core components required to build a production-worthy data lake\nFramework to integrate data feeds from real-time and streaming sources such as Pandora, Spotify, iTunes, etc., each supplied in different formats and different time sequences\nThe task of systematically onboarding the more than 140+ unique data feeds\nThe strategy built with a laser focus on capacity models to ensure system scalability\nResolving data ingestion bottlenecks with a new, open and scalable framework that seamlessly accommodates existing and new data sources\nA complete overview of the data ecosystem core components and moving parts.\n\nThe discussion also covers emerging alternatives such as Spark, and how, where, when, and why these technologies are relevant in the new data lake architecture."}, "big-data-conference-ny-2015/public/schedule/detail/43290": {"room": "1 E8 / 1 E9", "title": "Preserving signal in customer journeys", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43290", "topics": "Data Science & Advanced Analytics", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/203966", "/big-data-conference-ny-2015/public/schedule/speaker/203967"], "timing": "2:05pm\u20132:25pm Wednesday, 09/30/2015", "abstract": "Customer journey analytics across multiple data channels has become a viable exercise in processing multi-channel data, which includes web clickstreams, IVR logs, and chat transcripts.  However, early systems for doing such analysis relied on some level of apriori aggregation; an example of this includes counting calls into an IVR system for a specific issue on an hourly or even daily basis.  This has been necessary because the underlying representations of the customer journey had constraints on representing:\n\nLarge numbers of paths\nThe intensity of activity (number of customers associated with the activity)\nThe time distance between steps along the journey\n\nIn this session we present GRASP, a directed graph data structure in which:\n\nNodes represent events or activities such as phone calls or web clicks\nEdges represent the path segments\n\nWe will also discuss queries, which are described for identifying:\n\nNodes between a starting node and a target node\nIntensity of traffic at nodes\nTime distances between nodes\nCustomers associated with paths of interest\n\nThe data structure also associates customer profile and attribute data sets with the journey. This is especially useful for associating text analytics data (i.e. from a chat transcript), which is available at an event level. In this instance, apriori aggregation methods would not be amenable to proper consideration of customer sentiment.  Note that this graph structure is different from traditional graph structures, which are used, for example, to map relationships between customers and products/services.\nGRASP has been implemented natively on Hadoop, and is currently in use for production systems that use both structured (event logs) and unstructured (text) data from customer care systems."}, "big-data-conference-ny-2015/public/schedule/detail/46170": {"room": "3D 06/07", "title": "Design patterns for real-time data analytics", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46170", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/164231"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "As businesses are realizing the power of Hadoop and large data analytics, many are demanding large-scale, real-time streaming data analytics. Apache Storm and Apache Spark are platforms that can process large amounts of data in real time. However, building applications on these platforms that can scale, reliably process data without any loss, satisfy functional needs, and at the same time meet the strict latency requirements, takes a lot of work to get it right.\nAfter implementing multiple large real-time data processing applications using these technologies in various business domains, we distilled commonly required solutions into generalized design patterns. These patterns are proven in the very large production deployments where they process millions of events per second, tens of billions of events per day, and tens of terabytes of data per day.\nThis talk covers these proven design patterns and every design pattern it covers \u2013 problem statement, applicability of design pattern, the pattern design, and sample code demonstrating the implementation. Attendees can take advantages of these patterns in building their applications and improve their productivity, quality of solution, as well as the success factor of their applications."}, "big-data-conference-ny-2015/public/schedule/detail/46136": {"room": "1 E18 / 1 E19", "title": "Considerations for building a cognitive application", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46136", "topics": "Data Innovations", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/219606"], "timing": "4:35pm\u20135:15pm Thursday, 10/01/2015", "abstract": "Recommendation engines are some of the first commercial examples of cognitive computing applications.  They were also the first big data products produced \u2013 think Amazon product recommendations, Google search results, or LinkedIn\u2019s \u201cPeople You May Know\u201d feature.  Recommendations narrow what could become a complex decision to just a few recommendations. Their underlying algorithms \u201clearn\u201d from the experience of the past.  They reduce big data observations to small data actions.  What if recommendations could help analysts sort through data in Hadoop?\nBuilding a query recommendation engine is slightly more complex than building a product, data, or people recommendation engine. Writing queries is a complex task, made up of multiple steps and those steps are not always predictably directed. In this session we\u2019ll share some of the technical challenges and learnings from building a cognitive application in daily use today, by analyst teams from eBay to Square. We\u2019ll cover:\nUnderstanding user context at the point of interaction\nWhat recommendations will a user benefit from? For query recommendations, do we want to suggest attributes, tables, filters, joins, or show data quality or other warnings on the data being queried? The recommendation depends on the context where the user is within a query. This requires us to deeply understand the user and their context.\nEnsuring quality & relevance\nRecommendations have to be very relevant to the user. Achieving this relevance requires that we understand the correlations among various data objects and that we leverage them well for the current user. For example, among the myriad choices available, what are the three most useful, accurate filters and joins to suggest for the specific query being written by an analyst, which efficiency or quality warnings need to be shown to a user, etc. All of these require us to accurately model the usage of the data being queried.\nDelivering performance and responsiveness\nRecommendations have to appear at the speed of typing so that a user doesn\u2019t notice lag as they type. Often, databases being queried may have several millions of objects including tables, attributes, predicates. Such sizes pose a challenge on the recommendation engine and lead to several interesting trade-offs around what information to push to a user\u2019s browser for very fast responses, and what information to retain on the server.\nWe\u2019ll use the Alation SmartSuggest feature as a working example of how to navigate these technical trade-offs, to build a cognitive application in use daily by one of the most discerning audiences for recommendations: analysts and data scientists themselves."}, "big-data-conference-ny-2015/public/schedule/detail/43294": {"room": "1 E12/ 1 E13", "title": "Continuous curation of event data for a customer event hub", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43294", "topics": "Hadoop Use Cases", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/178236"], "timing": "2:55pm\u20133:35pm Wednesday, 09/30/2015", "abstract": "Enterprises that interact with their customers through independent channels collect data about such interactions. This data is diverse, exists in different shapes and forms, is scattered all across the infrastructure or elsewhere, and is continuously being produced. Moreover, it changes in structure and semantics unexpectedly due to the independent nature of the interaction channels. An attempt to create a single customer view on top of all such data is a monumental task, due to the complexity of data collection and curation that is the necessary first step.\nUsing StreamSets continuous ingest, enterprises can build a customer event hub that allows consumption of data from all channels in a timely manner. Data thus collected is curated and sanitized on the fly to ensure compliance with PII requirements, as well as bringing it into a consumable form for analytics applications. This architecture has built-in safeguards against unexpected schema and semantic changes, and works with existing infrastructure that may already be in place, such as Flume, Kafka, Spark, etc."}, "big-data-conference-ny-2015/public/schedule/detail/45943": {"room": "1 E14", "title": "Delivering trusted data for analyst autonomy and operational agility with a unified big data fabric", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45943", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/162545", "/big-data-conference-ny-2015/public/schedule/speaker/217938"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "Organizations are rapidly adopting Hadoop as a more scalable and efficient persistence layer of all types of data. But organizations continue to struggle to efficiently ingest, cleanse, master, govern, and deliver trusted information out of Hadoop for analytical value.  Data either becomes distributed in siloed teams or becomes entirely inaccessible due to security and compliance constraints.\nWith more organizations looking to use big data platforms for sensitive and regulated data sets, new big data fabrics are required that promote operational agility and end-user analyst autonomy, while ensuring security and governance.  In this session, learn how leading customers have built a unified big data fabric on top of Hadoop, using technologies like Informatica to repeatably deliver trusted data assets to a large community of data consumers, for a multi-dimensional view of customers.\nThis session is sponsored by Informatica"}, "big-data-conference-ny-2015/public/schedule/detail/45823": {"room": "Javits North", "title": "Patterns from the future", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/45823", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/123141"], "timing": "9:25am\u20139:30am Thursday, 10/01/2015", "abstract": "Imagine the possibilities of having all of your data in one place \u2013 at a reasonable cost \u2013 with the computing potential to learn from relationships between data in all domains. Now imagine being able to react in a nimble and agile fashion, and use newly discovered relationships for good as quickly as possible. Advanced analytics and Hadoop are changing the way organizations approach big data. Join this keynote presentation to get tips from the future and hear about key patterns emerging from a wide cross section of corporate and institutional Hadoop journeys. Perhaps they\u2019ll inspire yours.\nThis keynote is sponsored by SAS"}, "big-data-conference-ny-2015/public/schedule/detail/46272": {"room": "Javits North", "title": "Keynote with Farrah Bostic", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46272", "abstract": "Farrah Bostic, Founder, The Difference Engine", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/169393"], "timing": "9:30am\u20139:45am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/46271": {"room": "Javits North", "title": "How Data Influences Work", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46271", "abstract": "Maria Konnikova, Science and Psychology Blogger for The New Yorker and Bestselling Author of Mastermind.", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/221494"], "timing": "10:20am\u201310:40am Thursday, 10/01/2015"}, "big-data-conference-ny-2015/public/schedule/detail/46178": {"room": "1 E6 / 1 E7", "title": "Case Study: How YP.com addresses real-world analytical challenges for SQL on Hadoop", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/46178", "topics": "Sponsored", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/144690", "/big-data-conference-ny-2015/public/schedule/speaker/189915"], "timing": "1:15pm\u20131:55pm Thursday, 10/01/2015", "abstract": "If you\u2019re struggling with determining which implementation of SQL on Hadoop can meet your analytics needs, you\u2019re not alone. Join us for a discussion on how YP.com, the third-largest company in the US for mobile advertising for search ads with a half million paid subscribers, uses HP Vertica for SQL on Hadoop to solve their organization\u2019s big data challenges. Bill Theisinger, Vice President of Engineering for YP.com will walk you through his journey and search for a solution to query growing volumes of customer search and click data stored in HDFS. Specifically, he\u2019ll take a technical look at how issues like latency, concurrency, ANSI SQL depth, and optimized access for Hadoop formats played in his company\u2019s evaluation of Hive and other offerings. Discover how companies are choosing the \u201cbest-tool-for-the-job\u201d using the HP Haven Big Data Platform, HP Project Minotaur Big Data Architecture, HP Enterprise Services, and Hadoop to their advantage.\nThis session is sponsored by HP"}, "big-data-conference-ny-2015/public/schedule/detail/43114": {"room": "1 E16 / 1 E17", "title": "Hadoop's storage gap: Resolving transactional access/analytic performance trade-offs", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43114", "topics": "Hadoop Internals & Development", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/75982"], "timing": "2:05pm\u20132:45pm Wednesday, 09/30/2015", "abstract": "Over the past several years, the Hadoop ecosystem has made great strides in its real-time access capabilities, narrowing the gap compared to traditional database technologies. With systems such as Impala and Spark, analysts can now run complex queries or jobs over large datasets within a matter of seconds. With systems such as Apache HBase and Apache Phoenix, applications can achieve millisecond-scale random access to arbitrarily-sized datasets.\nDespite these advances, some important gaps remain that prevent many applications from transitioning to Hadoop-based architectures. Users are often caught between a rock and a hard place: columnar formats such as Apache Parquet offer extremely fast scan rates for analytics, but little to no ability for real-time modification or row-by-row indexed access. Online systems such as HBase offer very fast random access, but scan rates that are too slow for large scale data warehousing workloads.\nThis session will investigate the trade-offs between real-time transactional access and fast analytic performance from the perspective of storage engine internals. We will discuss recent advances from academic literature and commercial systems, evaluate benchmark results from current generation Hadoop technologies, and propose potential ways ahead for the Hadoop ecosystem to conquer its newest set of challenges."}, "big-data-conference-ny-2015/public/schedule/detail/43116": {"room": "3D 06/07", "title": "Developing a modern enterprise data strategy", "url": "http://strataconf.com/big-data-conference-ny-2015/public/schedule/detail/43116", "topics": "Data-driven Business", "speaker_urls": ["/big-data-conference-ny-2015/public/schedule/speaker/172046", "/big-data-conference-ny-2015/public/schedule/speaker/1"], "timing": "1:30pm\u20135:00pm Tuesday, 09/29/2015", "abstract": "Description\nBig data and data science have great potential for accelerating business, but how do you reconcile the business opportunity with the sea of possible technical solutions? Fundamentally, data should serve the strategic imperatives of a business\u2014those key strategic aspirations that define the future vision for an organization. A data strategy should guide your organization in two key areas\u2014which actions your business should take to get started with data, and where to start to realize the most value.\nIn this tutorial, we will explain how we work to solve real business challenges with data, including the following topics:\n\nWhy have a data strategy?\nConnecting data with business\nDevising a data strategy\nThe data value chain\nNew technology potentials\nProject development style\nOrganizing to execute your strategy"}}